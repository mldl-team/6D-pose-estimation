{"cells":[{"cell_type":"markdown","metadata":{"id":"SMWItJ1CUp56"},"source":["# **Configure Google Colab Settings**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25615,"status":"ok","timestamp":1748562595796,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"St4iy9xVUvr8","outputId":"ed06dccf-a5af-408a-9fb4-b90d58ed0df7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Connect to personal Google Drive space\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1748562598252,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"6e80OV_qUcfQ","outputId":"f380c5cd-b5ed-47dd-d64a-677b4dcf92db"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["#Change directory\n","%cd /content/\n","#Create Paths\n","!mkdir /content/dataset/\n","!mkdir /content/dataset/linemod/\n","!mkdir /content/dataset/linemod/Linemod_preprocessed/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":151860,"status":"ok","timestamp":1748562752268,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"7kIVCfM8UwgR","outputId":"579d7955-3f04-4fb2-f231-e8da56a96d45"},"outputs":[{"output_type":"stream","name":"stdout","text":["📦 Extracting 62142 files...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Unzipping: 100%|██████████| 62142/62142 [02:27<00:00, 421.01file/s]  "]},{"output_type":"stream","name":"stdout","text":["\n","✅ Extraction complete.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import zipfile\n","import os\n","from tqdm import tqdm\n","\n","zip_path = \"/content/drive/MyDrive/Linemod_preprocessed.zip\"\n","extract_to = \"/content/dataset/linemod/\"\n","\n","# Create the output directory if it doesn't exist\n","os.makedirs(extract_to, exist_ok=True)\n","\n","# Open the zip file\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    # Retrieve the list of files in the archive\n","    file_list = zip_ref.infolist()\n","\n","    print(f\"📦 Extracting {len(file_list)} files...\\n\")\n","    for file in tqdm(file_list, desc=\"Unzipping\", unit=\"file\"):\n","        # Extract each file to the target directory\n","        zip_ref.extract(file, extract_to)\n","\n","print(\"\\n✅ Extraction complete.\")"]},{"cell_type":"markdown","metadata":{"id":"Rio2kCCbU0rl"},"source":["# **Configure Github Settings**"]},{"cell_type":"markdown","metadata":{"id":"zFNfdbV7BQgc"},"source":["### Install Git"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":35078,"status":"ok","timestamp":1748569010748,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"WskY0S65VGM9","outputId":"7633e9e7-c58c-4c5d-bf70-c8417ab0dee0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.12).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.12).\n","Calculating upgrade... Done\n","The following packages have been kept back:\n","  libcudnn9-cuda-12 libcudnn9-dev-cuda-12 libnccl-dev libnccl2\n","The following packages will be upgraded:\n","  base-files binutils binutils-common binutils-x86-64-linux-gnu\n","  cuda-toolkit-12-config-common cuda-toolkit-config-common e2fsprogs\n","  libbinutils libc-bin libcap2 libctf-nobfd0 libctf0 libext2fs2 libgnutls30\n","  libldap-2.5-0 libpam-modules libpam-modules-bin libpam-runtime libpam0g\n","  libperl5.34 libseccomp2 libss2 libtasn1-6 libudev1 linux-libc-dev logsave\n","  openssl perl perl-base perl-modules-5.34 python3-pkg-resources\n","31 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n","Need to get 19.3 MB of archives.\n","After this operation, 289 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 base-files amd64 12ubuntu4.7 [61.9 kB]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-config-common 12.9.37-1 [16.5 kB]\n","Get:3 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libperl5.34 amd64 5.34.0-3ubuntu1.4 [4,820 kB]\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-config-common 12.9.37-1 [16.5 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl amd64 5.34.0-3ubuntu1.4 [232 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-base amd64 5.34.0-3ubuntu1.4 [1,759 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-modules-5.34 all 5.34.0-3ubuntu1.4 [2,977 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-bin amd64 2.35-0ubuntu3.10 [706 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam0g amd64 1.4.0-11ubuntu2.5 [59.8 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-modules-bin amd64 1.4.0-11ubuntu2.5 [37.4 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-modules amd64 1.4.0-11ubuntu2.5 [280 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logsave amd64 1.46.5-2ubuntu1.2 [10.1 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libext2fs2 amd64 1.46.5-2ubuntu1.2 [208 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 e2fsprogs amd64 1.46.5-2ubuntu1.2 [590 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcap2 amd64 1:2.44-1ubuntu0.22.04.2 [18.3 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-runtime all 1.4.0-11ubuntu2.5 [40.2 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtasn1-6 amd64 4.18.0-4ubuntu0.1 [43.5 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgnutls30 amd64 3.7.3-4ubuntu1.6 [969 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libseccomp2 amd64 2.5.3-2ubuntu3~22.04.1 [47.4 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libss2 amd64 1.46.5-2ubuntu1.2 [12.3 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssl amd64 3.0.2-0ubuntu1.19 [1,186 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf0 amd64 2.38-4ubuntu2.8 [103 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf-nobfd0 amd64 2.38-4ubuntu2.8 [108 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.38-4ubuntu2.8 [2,324 kB]\n","Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libbinutils amd64 2.38-4ubuntu2.8 [661 kB]\n","Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils amd64 2.38-4ubuntu2.8 [3,196 B]\n","Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-common amd64 2.38-4ubuntu2.8 [223 kB]\n","Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libldap-2.5-0 amd64 2.5.19+dfsg-0ubuntu0.22.04.1 [184 kB]\n","Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 linux-libc-dev amd64 5.15.0-140.150 [1,313 kB]\n","Fetched 19.3 MB in 9s (2,188 kB/s)\n","Extracting templates from packages: 100%\n","Preconfiguring packages ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../base-files_12ubuntu4.7_amd64.deb ...\n","Unpacking base-files (12ubuntu4.7) over (12ubuntu4.6) ...\n","Setting up base-files (12ubuntu4.7) ...\n","Installing new version of config file /etc/issue ...\n","Installing new version of config file /etc/issue.net ...\n","Installing new version of config file /etc/lsb-release ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libperl5.34_5.34.0-3ubuntu1.4_amd64.deb ...\n","Unpacking libperl5.34:amd64 (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Preparing to unpack .../perl_5.34.0-3ubuntu1.4_amd64.deb ...\n","Unpacking perl (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Preparing to unpack .../perl-base_5.34.0-3ubuntu1.4_amd64.deb ...\n","Unpacking perl-base (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Setting up perl-base (5.34.0-3ubuntu1.4) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../perl-modules-5.34_5.34.0-3ubuntu1.4_all.deb ...\n","Unpacking perl-modules-5.34 (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Preparing to unpack .../libc-bin_2.35-0ubuntu3.10_amd64.deb ...\n","Unpacking libc-bin (2.35-0ubuntu3.10) over (2.35-0ubuntu3.8) ...\n","Setting up libc-bin (2.35-0ubuntu3.10) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam0g_1.4.0-11ubuntu2.5_amd64.deb ...\n","Unpacking libpam0g:amd64 (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam0g:amd64 (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam-modules-bin_1.4.0-11ubuntu2.5_amd64.deb ...\n","Unpacking libpam-modules-bin (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam-modules-bin (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam-modules_1.4.0-11ubuntu2.5_amd64.deb ...\n","Unpacking libpam-modules:amd64 (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam-modules:amd64 (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../logsave_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking logsave (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Preparing to unpack .../libext2fs2_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking libext2fs2:amd64 (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Setting up libext2fs2:amd64 (1.46.5-2ubuntu1.2) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../e2fsprogs_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking e2fsprogs (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Preparing to unpack .../libcap2_1%3a2.44-1ubuntu0.22.04.2_amd64.deb ...\n","Unpacking libcap2:amd64 (1:2.44-1ubuntu0.22.04.2) over (1:2.44-1ubuntu0.22.04.1) ...\n","Setting up libcap2:amd64 (1:2.44-1ubuntu0.22.04.2) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam-runtime_1.4.0-11ubuntu2.5_all.deb ...\n","Unpacking libpam-runtime (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam-runtime (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libudev1_249.11-0ubuntu3.15_amd64.deb ...\n","Unpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\n","Setting up libudev1:amd64 (249.11-0ubuntu3.15) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libtasn1-6_4.18.0-4ubuntu0.1_amd64.deb ...\n","Unpacking libtasn1-6:amd64 (4.18.0-4ubuntu0.1) over (4.18.0-4build1) ...\n","Setting up libtasn1-6:amd64 (4.18.0-4ubuntu0.1) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libgnutls30_3.7.3-4ubuntu1.6_amd64.deb ...\n","Unpacking libgnutls30:amd64 (3.7.3-4ubuntu1.6) over (3.7.3-4ubuntu1.5) ...\n","Setting up libgnutls30:amd64 (3.7.3-4ubuntu1.6) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libseccomp2_2.5.3-2ubuntu3~22.04.1_amd64.deb ...\n","Unpacking libseccomp2:amd64 (2.5.3-2ubuntu3~22.04.1) over (2.5.3-2ubuntu2) ...\n","Setting up libseccomp2:amd64 (2.5.3-2ubuntu3~22.04.1) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../00-libss2_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking libss2:amd64 (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Preparing to unpack .../01-openssl_3.0.2-0ubuntu1.19_amd64.deb ...\n","Unpacking openssl (3.0.2-0ubuntu1.19) over (3.0.2-0ubuntu1.16) ...\n","Preparing to unpack .../02-libctf0_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking libctf0:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../03-libctf-nobfd0_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking libctf-nobfd0:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../04-binutils-x86-64-linux-gnu_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking binutils-x86-64-linux-gnu (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../05-libbinutils_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking libbinutils:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../06-binutils_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking binutils (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../07-binutils-common_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking binutils-common:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../08-cuda-toolkit-12-config-common_12.9.37-1_all.deb ...\n","Unpacking cuda-toolkit-12-config-common (12.9.37-1) over (12.5.82-1) ...\n","Preparing to unpack .../09-cuda-toolkit-config-common_12.9.37-1_all.deb ...\n","Unpacking cuda-toolkit-config-common (12.9.37-1) over (12.5.82-1) ...\n","Preparing to unpack .../10-libldap-2.5-0_2.5.19+dfsg-0ubuntu0.22.04.1_amd64.deb ...\n","Unpacking libldap-2.5-0:amd64 (2.5.19+dfsg-0ubuntu0.22.04.1) over (2.5.17+dfsg-0ubuntu0.22.04.1) ...\n","Preparing to unpack .../11-linux-libc-dev_5.15.0-140.150_amd64.deb ...\n","Unpacking linux-libc-dev:amd64 (5.15.0-140.150) over (5.15.0-113.123) ...\n","Preparing to unpack .../12-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n","Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.2) ...\n","Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n","Setting up cuda-toolkit-config-common (12.9.37-1) ...\n","Setting up binutils-common:amd64 (2.38-4ubuntu2.8) ...\n","Setting up linux-libc-dev:amd64 (5.15.0-140.150) ...\n","Setting up libctf-nobfd0:amd64 (2.38-4ubuntu2.8) ...\n","Setting up perl-modules-5.34 (5.34.0-3ubuntu1.4) ...\n","Setting up libldap-2.5-0:amd64 (2.5.19+dfsg-0ubuntu0.22.04.1) ...\n","Setting up libss2:amd64 (1.46.5-2ubuntu1.2) ...\n","Setting up logsave (1.46.5-2ubuntu1.2) ...\n","Setting up libbinutils:amd64 (2.38-4ubuntu2.8) ...\n","Setting up openssl (3.0.2-0ubuntu1.19) ...\n","Setting up cuda-toolkit-12-config-common (12.9.37-1) ...\n","Setting up libctf0:amd64 (2.38-4ubuntu2.8) ...\n","Setting up libperl5.34:amd64 (5.34.0-3ubuntu1.4) ...\n","Setting up e2fsprogs (1.46.5-2ubuntu1.2) ...\n","Setting up perl (5.34.0-3ubuntu1.4) ...\n","Setting up binutils-x86-64-linux-gnu (2.38-4ubuntu2.8) ...\n","Setting up binutils (2.38-4ubuntu2.8) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.10) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n"]}],"source":["#Install/Upgrade Git\n","!apt-get install git\n","!apt upgrade git"]},{"cell_type":"markdown","metadata":{"id":"BIXMgN0jVTDp"},"source":["### Clone project from Github\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8048,"status":"ok","timestamp":1748569035517,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"GlWFu5IEiZHW","outputId":"c90bc30b-0b9c-4ab9-accf-7131e536f5b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into '6D-pose-estimation'...\n","remote: Enumerating objects: 152, done.\n","remote: Counting objects: 100% (33/33), done.\n","remote: Compressing objects: 100% (23/23), done.\n","remote: Total 152 (delta 5), reused 30 (delta 4), pack-reused 119 (from 2)\n","Receiving objects: 100% (152/152), 121.78 MiB | 20.73 MiB/s, done.\n","Resolving deltas: 100% (25/25), done.\n"]}],"source":["%cd /content\n","!git clone https://github.com/mldl-team/6D-pose-estimation.git"]},{"cell_type":"markdown","metadata":{"id":"IRP4-CsXVbZL"},"source":["### Copy modified code into the cloned project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1748446198485,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"MLzANWzrVLv_","outputId":"f300cdf9-a58d-4c99-fc90-2cd490cb98d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/6D-pose-estimation\n"]}],"source":["#Copy the file into cloned project\n","%cd /content/6D-pose-estimation\n","!cp -r \"/content/drive/MyDrive/Backups/7 - Backup/6D-Pose-Estimation.ipynb\" \"/content/6D-pose-estimation\""]},{"cell_type":"markdown","metadata":{"id":"cjH76VX3BTow"},"source":["### Login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2MnT90rh5SV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748445590225,"user_tz":-120,"elapsed":1318,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"5e5e3d35-48c6-4015-e1eb-b09da58a1b92"},"outputs":[{"output_type":"stream","name":"stdout","text":["From https://github.com/mldl-team/6D-pose-estimation\n"," * branch            sina       -> FETCH_HEAD\n","error: The following untracked working tree files would be overwritten by checkout:\n","\t6D-Pose-Estimation.ipynb\n","Please move or remove them before you switch branches.\n","Aborting\n"]}],"source":["#Config Git Before Push\n","!git config --global user.email \"sina.ghiabi1@gmail.com\"\n","!git config --global user.name \"sina-ghiabi\"\n","\n","#Authentication\n","!git remote set-url origin https://sina-ghiabi:<SSH Key>@github.com/mldl-team/6D-pose-estimation.git\n","!git fetch origin sina\n","!git checkout sina"]},{"cell_type":"markdown","source":["### Push"],"metadata":{"id":"FYynj1jMhvzv"}},{"cell_type":"code","source":["!cd /content/6D-pose-estimation\n","!git add 6D-Pose-Estimation.ipynb\n","!git commit -m \"Add 6D Pose Estimation notebook\"\n","!git push origin sina"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oa81mstghGVS","executionInfo":{"status":"ok","timestamp":1748446285504,"user_tz":-120,"elapsed":1716,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"e05b4ab0-2f54-4fc5-d7eb-1527a9491aaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[sina 33d4288] Add 6D Pose Estimation notebook\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite 6D-Pose-Estimation.ipynb (95%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 8 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 33.23 KiB | 1.95 MiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\n","To https://github.com/mldl-team/6D-pose-estimation.git\n","   a87cfb1..33d4288  sina -> sina\n"]}]},{"cell_type":"markdown","metadata":{"id":"hDNLbiT4Vnkb"},"source":["### Push the modified project into Github"]},{"cell_type":"code","source":["# Step 1: Navigate to the project directory\n","%cd /content/6D-pose-estimation\n","\n","# Step 2: Remove internal Git folder from rcvpose (if it exists)\n","!rm -rf RCVPose/rcvpose/.git\n","\n","# Step 3: Remove submodule from Git index (not the actual files)\n","!git rm --cached -r RCVPose/rcvpose || echo \"rcvpose was not tracked or already clean\"\n","\n","# Step 4: Re-add the folder as a normal directory\n","!git add .\n","\n","# Step 5: Commit with a clear message\n","!git commit -m \"RCVPose - Sixth Version (re-added rcvpose as regular folder)\"\n","\n","# Step 6: Push to the 'sina' branch\n","!git push origin sina"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"6S3sD5j3fe7m","executionInfo":{"status":"ok","timestamp":1747998162516,"user_tz":-120,"elapsed":1152,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"1968d4ac-ec87-4788-85e2-585f1f52a10b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/6D-pose-estimation\n","rm 'RCVPose/rcvpose/.gitignore'\n","rm 'RCVPose/rcvpose/3DRadius_lm.py'\n","rm 'RCVPose/rcvpose/3DRadius_ycb.py'\n","rm 'RCVPose/rcvpose/AccumulatorSpace.py'\n","rm 'RCVPose/rcvpose/LICENSE'\n","rm 'RCVPose/rcvpose/README.md'\n","rm 'RCVPose/rcvpose/data_loader.py'\n","rm 'RCVPose/rcvpose/doc/teaser_code.gif'\n","rm 'RCVPose/rcvpose/main.py'\n","rm 'RCVPose/rcvpose/models/fcnresnet.py'\n","rm 'RCVPose/rcvpose/rcvpose.yml'\n","rm 'RCVPose/rcvpose/rmap_dataset.py'\n","rm 'RCVPose/rcvpose/train.py'\n","rm 'RCVPose/rcvpose/util/horn.py'\n","rm 'RCVPose/rcvpose/utils.py'\n","On branch sina\n","Your branch is up to date with 'origin/sina'.\n","\n","nothing to commit, working tree clean\n","Everything up-to-date\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jtvb6CJ_VwP0"},"source":["# **Configure Wandb Settings**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2316,"status":"ok","timestamp":1747778368264,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"mI6oDV8iV4AY","outputId":"6acba1f3-5ae1-4386-d271-93c1448ebfc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n","Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"]}],"source":["#Wandb installation\n","!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9327,"status":"ok","timestamp":1747778382449,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"I5ZYYFDTV4q2","outputId":"d4507ed9-b436-43dd-c01f-7604c088b2d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msina-ghiabi\u001b[0m (\u001b[33merythm-mldl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}],"source":["!wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":6237,"status":"ok","timestamp":1747778400109,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"h-piQyhBV6le","outputId":"4b8b84ad-a284-4ae6-dc76-af4f4caddc9b"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msina-ghiabi\u001b[0m (\u001b[33merythm-mldl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250520_215956-3i6cbp0i</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">warm-pond-15</a></strong> to <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">https://wandb.ai/erythm-mldl/6D</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▇▄█▅▇</td></tr><tr><td>loss</td><td>█▆▃▄▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.84825</td></tr><tr><td>loss</td><td>0.12104</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">warm-pond-15</strong> at: <a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i</a><br> View project at: <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">https://wandb.ai/erythm-mldl/6D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20250520_215956-3i6cbp0i/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import random\n","\n","import wandb\n","\n","# Start a new wandb run to track this script.\n","run = wandb.init(\n","    # Set the wandb entity where your project will be logged (generally your team name).\n","    entity=\"erythm-mldl\",\n","    # Set the wandb project where this run will be logged.\n","    project=\"6D\",\n","    # Track hyperparameters and run metadata.\n","    config={\n","        \"learning_rate\": 0.02,\n","        \"architecture\": \"CNN\",\n","        \"dataset\": \"CIFAR-100\",\n","        \"epochs\": 10,\n","    },\n",")\n","\n","# Simulate training.\n","epochs = 10\n","offset = random.random() / 5\n","for epoch in range(2, epochs):\n","    acc = 1 - 2**-epoch - random.random() / epoch - offset\n","    loss = 2**-epoch + random.random() / epoch + offset\n","\n","    # Log metrics to wandb.\n","    run.log({\"acc\": acc, \"loss\": loss})\n","\n","# Finish the run and upload any remaining data.\n","run.finish()"]},{"cell_type":"markdown","metadata":{"id":"ja4g4pbfWAOK"},"source":["# **Configure Python**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2956,"status":"ok","timestamp":1748562928044,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"tf4W2glvWGIg","outputId":"1bd0c55c-ed09-4242-dfcb-d8bef78c9d04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n","Collecting jedi>=0.16 (from ipython)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jedi\n","Successfully installed jedi-0.19.2\n"]}],"source":["#Install Python\n","!pip install ipython\n","\n","#Usage of the library is to display input or output images\n","from IPython.display import Image, display"]},{"cell_type":"markdown","metadata":{"id":"VpvHIshEiF5O"},"source":["# **Configure PyTorch**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":71736,"status":"ok","timestamp":1748563002849,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"GL955jklWLg6","outputId":"f9f02d3e-4607-40f8-e7d5-f2b5e058f826"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]},{"output_type":"execute_result","data":{"text/plain":["'2.6.0+cu124'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["#Install PyTorch\n","!pip install torch\n","!pip install torch torchvision opencv-python matplotlib tqdm pyyaml\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","\n","#Usage of the library is the primary neural network computation framework.\n","import torch\n","torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"eI7EMHExgdOs"},"source":["# **Configure YOLO**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3824,"status":"ok","timestamp":1748346779889,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"mkcXvJosWJG7","outputId":"334e3a3f-1efd-4684-9e96-969818a898f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.145-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.145-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.145 ultralytics-thop-2.0.14\n","/bin/bash: -c: line 2: syntax error: unexpected end of file\n","Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["#Usage of the library is To run Object Detection & Image Classification with YOLO\n","!pip install ultralytics\n","\n","#Check availability of ultralytics\n","!ultralytics.checks()\n","\n","import ultralytics\n","\n","from ultralytics import YOLO"]},{"cell_type":"markdown","metadata":{"id":"cxJlZbFTlShT"},"source":["# **Configure OpenCV**"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2539,"status":"ok","timestamp":1748563014320,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"dCEtKW0eTrST","outputId":"599cc089-fbc8-490e-f892-e2aea081a0e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"]}],"source":["#Usage of the library is for real‐time image and video processing\n","!pip install opencv-python\n","\n","import cv2"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":125,"status":"ok","timestamp":1748563017083,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"zvAaOEWyWNu4","outputId":"69d26a75-5ac2-4062-b939-3223787f6560"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu May 29 23:56:57 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["#Check if we have an access to nvidia\n","#If \"/bin/bash: line 1: nvidia-smi: command not found\" appeared, change Runtime to GPU\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"IiEXNi0d1krR"},"source":["# **Configure Open3D**\n","### **Open3D is used for 3D geometry processing in Pose Estimation, SciPy for scientific computations**"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":27586,"status":"ok","timestamp":1748563046849,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"6V3FOZ9H0pF8","outputId":"a0244f82-d0d5-42e8-d33f-e091ffa9daf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting open3d\n","  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Collecting dash>=2.6.0 (from open3d)\n","  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n","Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n","Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n","Collecting configargparse (from open3d)\n","  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n","Collecting ipywidgets>=8.0.4 (from open3d)\n","  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n","Collecting addict (from open3d)\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.2.1)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n","Collecting pyquaternion (from open3d)\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n","Collecting flask>=3.0.0 (from open3d)\n","  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n","Collecting werkzeug>=3.0.0 (from open3d)\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.13.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n","Collecting retrying (from dash>=2.6.0->open3d)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.2.1)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n","Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n","Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n","  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.25.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.4.26)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n","Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n","Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Installing collected packages: addict, widgetsnbextension, werkzeug, retrying, pyquaternion, configargparse, comm, flask, ipywidgets, dash, open3d\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 3.1.1\n","    Uninstalling Flask-3.1.1:\n","      Successfully uninstalled Flask-3.1.1\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7.1 dash-3.0.4 flask-3.0.3 ipywidgets-8.1.7 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.14\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.0.6)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.5.21)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}],"source":["!pip install open3d scipy numpy\n","!pip install opencv-python tensorboard pyyaml scipy scikit-image\n","!pip install tensorboardX\n","\n","import open3d as o3d\n","from scipy.spatial import distance_matrix\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"hcrD4hz007gZ"},"source":["### **Manage file paths with os Library**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"OiDNB_0YWPhe","executionInfo":{"status":"ok","timestamp":1748563051572,"user_tz":-120,"elapsed":22,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}}},"outputs":[],"source":["import os\n","import glob\n","\n","#Examples\n","#img_path = os.path.join('data', 'images', img_name)\n","#images = glob.glob('data/images/*.jpg')\n","#labels = glob.glob('data/labels/*.txt')"]},{"cell_type":"markdown","metadata":{"id":"WZxMTz5eKiig"},"source":["### **Progress Bar**"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2313,"status":"ok","timestamp":1748563056675,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"OvswuY3cKfa_","outputId":"cb49b207-b0af-49c2-c0e1-053d137df962"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"]}],"source":["!pip install tqdm\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"BYVEIJsVY16K"},"source":["# **Object Detection**\n"]},{"cell_type":"markdown","metadata":{"id":"hXrj5tdH6FsL"},"source":["### **Import all the Tools**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":933,"status":"ok","timestamp":1747403978128,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"JqLkUNE_ZGLC","outputId":"9b398b85-7dc6-4f4a-c76d-b3a68973ea57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 21.5M/21.5M [00:00<00:00, 140MB/s] \n"]},{"data":{"text/plain":["YOLO(\n","  (model): DetectionModel(\n","    (model): Sequential(\n","      (0): Conv(\n","        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (1): Conv(\n","        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (2): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (3): Conv(\n","        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (4): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0-1): 2 x Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (5): Conv(\n","        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (6): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0-1): 2 x Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (7): Conv(\n","        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (8): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (9): SPPF(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n","      )\n","      (10): Upsample(scale_factor=2.0, mode='nearest')\n","      (11): Concat()\n","      (12): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (13): Upsample(scale_factor=2.0, mode='nearest')\n","      (14): Concat()\n","      (15): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (16): Conv(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (17): Concat()\n","      (18): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (19): Conv(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (20): Concat()\n","      (21): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (22): Detect(\n","        (cv2): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (cv3): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (dfl): DFL(\n","          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","  )\n",")"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","\n","#Choose YOLO Model\n","model = YOLO('yolov8s.pt')\n","#Choose Computing Device - CUDA: Compute Unified Device Architecture\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","#Transfer Model to Computing Device\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"00BexPmO6J9N"},"source":["### **Make Dataset Ready**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":188095,"status":"ok","timestamp":1747412607059,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"LX6Kpu2tdb9r","outputId":"5b28b097-5d56-47a3-9541-de8961c1ce99"},"outputs":[{"name":"stdout","output_type":"stream","text":["🔍 Class distribution:\n","Class  1: 1236 samples\n","Class  2: 1214 samples\n","Class  4: 1201 samples\n","Class  5: 1196 samples\n","Class  6: 1179 samples\n","Class  8: 1188 samples\n","Class  9: 1254 samples\n","Class 10: 1253 samples\n","Class 11: 1220 samples\n","Class 12: 1237 samples\n","Class 13: 1152 samples\n","Class 14: 1227 samples\n","Class 15: 1243 samples\n","\n","⚙️ Processing and splitting images...\n","    Train Count  Validation Count\n","0           988               248\n","1           971               243\n","2           960               241\n","3           956               240\n","4           943               236\n","5           950               238\n","6          1003               251\n","7          1002               251\n","8           976               244\n","9           989               248\n","10          921               231\n","11          981               246\n","12          994               249\n","✅ YOLO dataset generated, split, and labeled.\n"]}],"source":["import os\n","import yaml\n","import random\n","import shutil\n","from PIL import Image\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","\n","# ================== CONFIGURATION ======================\n","!mkdir /content/dataset/linemod/Linemod_ready\n","root_dir = '/content/dataset/linemod/Linemod_preprocessed/data'\n","output_base = '/content/dataset/linemod/Linemod_ready'  # Base output folder\n","img_out_train = os.path.join(output_base, 'images/train')\n","img_out_val = os.path.join(output_base, 'images/val')\n","label_out_train = os.path.join(output_base, 'labels/train')\n","label_out_val = os.path.join(output_base, 'labels/val')\n","\n","# Create directories if they don't exist\n","for path in [img_out_train, img_out_val, label_out_train, label_out_val]:\n","    os.makedirs(path, exist_ok=True)\n","\n","# Define valid class IDs and their YOLO-mapped indices (0-based)\n","existing_classes = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n","#Create {cls_id: idx}: {0:1} - {1:2} - {2:4} - {3:5} ...\n","class_map = {cls_id: idx for idx, cls_id in enumerate(existing_classes)}\n","\n","# ================ STEP 1: GATHER SAMPLES ===================\n","samples_by_class = defaultdict(list)\n","for cls in existing_classes:\n","    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n","    rgb_folder = os.path.join(folder_path, 'rgb')\n","    gt_file = os.path.join(folder_path, 'gt.yml')\n","\n","    if not os.path.exists(gt_file):\n","        print(f\"⚠️ Class {cls:02d} missing → skipped.\")\n","        continue\n","\n","    with open(gt_file, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    for img_id in gt_data:\n","        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n","        if os.path.exists(img_path):\n","            samples_by_class[cls].append(img_id)\n","\n","# Display class distribution\n","print(\"🔍 Class distribution:\")\n","for cls, samples in samples_by_class.items():\n","    print(f\"Class {cls:>2}: {len(samples)} samples\")\n","\n","# =============== STEP 2: LABEL AND IMAGE PROCESSING =================\n","def save_yolo_labels(sample_list, mode, cls, gt_data, rgb_folder):\n","    label_dir = label_out_train if mode == 'train' else label_out_val\n","    img_dir = img_out_train if mode == 'train' else img_out_val\n","    label_id = class_map[cls]\n","\n","    for img_id in sample_list:\n","        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n","        img = Image.open(img_path)\n","        w, h = img.size\n","\n","        bbox = gt_data[img_id][0]['obj_bb']\n","        x, y, bw, bh = bbox\n","        x_center = (x + bw / 2) / w\n","        y_center = (y + bh / 2) / h\n","        norm_bw = bw / w\n","        norm_bh = bh / h\n","\n","        # Save label\n","        label_file = f\"{cls:02d}_{int(img_id):04d}.txt\"\n","        with open(os.path.join(label_dir, label_file), 'w') as f:\n","            f.write(f\"{label_id} {x_center:.6f} {y_center:.6f} {norm_bw:.6f} {norm_bh:.6f}\\n\")\n","\n","        # Save image\n","        img_out_path = os.path.join(img_dir, f\"{cls:02d}_{int(img_id):04d}.png\")\n","        shutil.copy(img_path, img_out_path)\n","\n","# Split, save labels and copy images\n","print(\"\\n⚙️ Processing and splitting images...\")\n","for cls, samples in samples_by_class.items():\n","    random.shuffle(samples)\n","    split_idx = int(0.8 * len(samples))\n","    train_samples = samples[:split_idx]\n","    val_samples = samples[split_idx:]\n","\n","    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n","    rgb_folder = os.path.join(folder_path, 'rgb')\n","    gt_file = os.path.join(folder_path, 'gt.yml')\n","\n","    with open(gt_file, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    save_yolo_labels(train_samples, 'train', cls, gt_data, rgb_folder)\n","    save_yolo_labels(val_samples, 'val', cls, gt_data, rgb_folder)\n","\n","# ============= STEP 3: LABEL DISTRIBUTION STATS ===============\n","def count_labels(label_dir):\n","    counter = Counter()\n","    for filename in os.listdir(label_dir):\n","        if filename.endswith('.txt'):\n","            with open(os.path.join(label_dir, filename), 'r') as f:\n","                for line in f:\n","                    class_id = int(line.strip().split()[0])\n","                    counter[class_id] += 1\n","    return counter\n","\n","train_counts = count_labels(label_out_train)\n","val_counts = count_labels(label_out_val)\n","\n","import pandas as pd\n","df_stats = pd.DataFrame({\n","    \"Train Count\": pd.Series(train_counts),\n","    \"Validation Count\": pd.Series(val_counts)\n","}).fillna(0).astype(int)\n","\n","print(df_stats)\n","\n","print(\"✅ YOLO dataset generated, split, and labeled.\")"]},{"cell_type":"markdown","metadata":{"id":"Y5UQxMsZ6RT2"},"source":[]},{"cell_type":"markdown","metadata":{"id":"raqAJg_s6Y8D"},"source":["### **Train the Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWdLRQ-qyHUQ"},"outputs":[],"source":["model.train(\n","    data='/content/project/6D-pose-estimation/configs/linemod_final.yaml',\n","    epochs=15,\n","    imgsz=640,\n","    batch=8,\n","    device=0,\n","    patience=5,\n","    weight_decay=0.0005\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXhj_4zwy4Gt"},"outputs":[],"source":["!cp -r runs/detect/train ~/6d/yolo_logs/"]},{"cell_type":"markdown","metadata":{"id":"ZACuf_m66sGQ"},"source":["### **Save Trained Model Metrics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YD2UGsFky-pW"},"outputs":[],"source":["from ultralytics import YOLO\n","\n","model = YOLO(\"model.pt\")\n","metrics = model.val(data=\"linemod_final.yaml\", plots=True, save=True)"]},{"cell_type":"markdown","metadata":{"id":"Z05pObtQAvQb"},"source":["# **Pose Estimation**"]},{"cell_type":"markdown","metadata":{"id":"NLw9Lzbazk4D"},"source":["### **Prepare File & Folders for RCVPose**"]},{"cell_type":"markdown","metadata":{"id":"DingMrNW0bPZ"},"source":["### 1 - Move Objects' Models From models Folder To Class Folders & Rename Them To mesh.ply"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":56,"status":"ok","timestamp":1748563071887,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"0n9I3pMgxT0u","outputId":"b0161b6c-2fd8-4d8f-994e-9c1e51d6cb46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Copying obj_11.ply → /content/dataset/linemod/Linemod_preprocessed/data/11/mesh.ply\n","Copying obj_01.ply → /content/dataset/linemod/Linemod_preprocessed/data/01/mesh.ply\n","Copying obj_09.ply → /content/dataset/linemod/Linemod_preprocessed/data/09/mesh.ply\n","Copying obj_02.ply → /content/dataset/linemod/Linemod_preprocessed/data/02/mesh.ply\n","Copying obj_12.ply → /content/dataset/linemod/Linemod_preprocessed/data/12/mesh.ply\n","Copying obj_14.ply → /content/dataset/linemod/Linemod_preprocessed/data/14/mesh.ply\n","Copying obj_08.ply → /content/dataset/linemod/Linemod_preprocessed/data/08/mesh.ply\n","Copying obj_06.ply → /content/dataset/linemod/Linemod_preprocessed/data/06/mesh.ply\n","Folder 07 does not exist → skipped\n","Folder 03 does not exist → skipped\n","Copying obj_10.ply → /content/dataset/linemod/Linemod_preprocessed/data/10/mesh.ply\n","Copying obj_13.ply → /content/dataset/linemod/Linemod_preprocessed/data/13/mesh.ply\n","Copying obj_04.ply → /content/dataset/linemod/Linemod_preprocessed/data/04/mesh.ply\n","Copying obj_05.ply → /content/dataset/linemod/Linemod_preprocessed/data/05/mesh.ply\n","Copying obj_15.ply → /content/dataset/linemod/Linemod_preprocessed/data/15/mesh.ply\n","\n","✅ Model files have been moved and renamed to mesh.ply.\n"]}],"source":["import os\n","import shutil\n","\n","# ==== CONFIGURATION ====\n","# Base directory for preprocessed model data\n","data_path = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","# Directory containing original model files\n","models_path = \"/content/dataset/linemod/Linemod_preprocessed/models\"\n","\n","for filename in os.listdir(models_path):\n","    # Only process files matching the pattern obj_<id>.ply\n","    if not filename.startswith(\"obj_\") or not filename.endswith(\".ply\"):\n","        continue\n","\n","    # Extract the model identifier (e.g., \"01\")\n","    model_id = filename.split(\"_\")[1].split(\".\")[0]  # example: \"01\"\n","    src_file = os.path.join(models_path, filename)\n","    dst_folder = os.path.join(data_path, model_id)\n","\n","    # Skip if the destination folder does not exist\n","    if not os.path.isdir(dst_folder):\n","        print(f\"Folder {model_id} does not exist → skipped\")\n","        continue\n","\n","    # Copy and rename the model file to mesh.ply\n","    dst_file = os.path.join(dst_folder, \"mesh.ply\")\n","    print(f\"Copying {filename} → {dst_file}\")\n","    shutil.copy2(src_file, dst_file)\n","\n","print(\"\\n✅ Model files have been moved and renamed to mesh.ply.\")\n"]},{"cell_type":"markdown","metadata":{"id":"vunah5M8cYW-"},"source":["### 2 - Create Outside9.npy From Objects' Models\n","A NumPy file containing a [9, 3] array of 3D keypoint coordinates sampled from the object mesh using Farthest-Point Sampling. These five points are chosen to maximally span the surface of the model. During data preprocessing, each keypoint is used to generate a per-frame radial distance map: for every pixel with valid depth, its Euclidean distance in 3D space to each keypoint is computed and stored. These distance maps serve as ground-truth supervision when training a network to predict 3D distance fields from RGB-D inputs.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":490,"status":"ok","timestamp":1748563074723,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"g0L1ky8r2svp","outputId":"75b75f59-c242-4e13-b81d-c999c7442c3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["🎯 Processing class 01\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/01/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/01/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 02\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/02/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/02/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 04\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/04/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/04/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 05\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/05/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/05/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 06\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/06/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/06/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 08\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/08/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/08/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 09\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/09/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/09/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 10\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/10/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/10/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 11\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/11/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/11/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 12\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/12/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/12/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 13\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/13/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/13/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 14\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/14/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/14/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 15\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/15/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/15/Outside9.npy loaded successfully with shape (9, 3)\n","\n"]}],"source":["import os\n","import numpy as np\n","import open3d as o3d\n","\n","def fps(points: np.ndarray, k: int, seed: int = 0) -> np.ndarray:\n","    \"\"\"\n","    Farthest-Point Sampling:\n","      - points: (N,3) array of XYZ samples\n","      - k: number of keypoints to pick\n","    Returns an array of shape (k,3).\n","    \"\"\"\n","    np.random.seed(seed)\n","    N = points.shape[0]\n","    centroids = np.zeros((k,), dtype=np.int32)\n","    distances = np.full((N,), np.inf)\n","    farthest = np.random.randint(0, N)\n","    for i in range(k):\n","        centroids[i] = farthest\n","        centroid = points[farthest]\n","        dist = np.sum((points - centroid)**2, axis=1)\n","        distances = np.minimum(distances, dist)\n","        farthest = np.argmax(distances)\n","    return points[centroids]\n","\n","# --- Define base path and class list ---\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","for cls in classes:\n","    seq_dir = os.path.join(base_dir, cls)\n","    mesh_path = os.path.join(seq_dir, \"mesh.ply\")\n","\n","    if not os.path.isfile(mesh_path):\n","        print(f\"⚠️  Skipping {cls}: mesh.ply not found at {mesh_path}\")\n","        continue\n","\n","    print(f\"🎯 Processing class {cls}\")\n","\n","    # Load point cloud\n","    pcd = o3d.io.read_point_cloud(mesh_path)\n","    pts = np.asarray(pcd.points)\n","\n","    if pts.shape[0] < 9:\n","        print(f\"❌ Not enough points in {mesh_path} (only {pts.shape[0]}), skipping.\")\n","        continue\n","\n","    # Perform Farthest-Point Sampling\n","    keypoints = fps(pts, k=9, seed=42)\n","\n","    # Save output\n","    out_path = os.path.join(seq_dir, \"Outside9.npy\")\n","    np.save(out_path, keypoints)\n","    print(f\"✅ Saved {out_path} (shape {keypoints.shape})\")\n","\n","    # Test load to confirm file is valid\n","    try:\n","        test = np.load(out_path)\n","        print(f\"📦 Verified file: {out_path} loaded successfully with shape {test.shape}\\n\")\n","    except Exception as e:\n","        print(f\"❌ Failed to load saved file: {e}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"AXbUbafiiMFD"},"source":["### It is only Possible to see the content of Outside9.npy using code below"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94,"status":"ok","timestamp":1748563078016,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"rrrof0ashbQ9","outputId":"cb046d70-0ee5-4260-f117-7cba0bbe3ba9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📂 Class 01 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/01/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -44.2630\n"," ├─ Max  : 41.4566\n"," ├─ Mean : -3.6767\n"," ├─ Std  : 23.3938\n"," └─ Sample Data:\n","[[-20.3664  19.3442  -7.6361]\n"," [ 35.6255 -16.9712 -44.263 ]\n"," [  7.5111  -4.4303  41.4566]\n"," [ -1.604  -33.9459  -5.4867]\n"," [-26.6571 -13.9838 -43.0416]\n"," [ 10.2731  20.1762 -40.7712]\n"," [ 13.9023  17.8718   9.8713]\n"," [-20.7606  -9.8099  16.1704]\n"," [ 30.0282 -21.3168 -10.4571]]\n","\n","📂 Class 02 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/02/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -107.0920\n"," ├─ Max  : 107.3880\n"," ├─ Mean : 0.5087\n"," ├─ Std  : 54.8957\n"," └─ Sample Data:\n","[[  12.3025  -17.8971   43.9721]\n"," [  90.5719   -1.9537 -107.092 ]\n"," [ -70.8535   22.383   -92.6023]\n"," [  53.2234   58.4881  -23.509 ]\n"," [   7.5829  -29.9273  -56.5234]\n"," [ -92.0255   -4.1247   -3.2999]\n"," [  66.5464    0.135   107.388 ]\n"," [  86.4643  -24.3331  -25.008 ]\n"," [ -11.3591   30.5359   -5.3499]]\n","\n","📂 Class 04 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/04/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -67.9932\n"," ├─ Max  : 67.0034\n"," ├─ Mean : -11.1607\n"," ├─ Std  : 39.5965\n"," └─ Sample Data:\n","[[-60.7914  62.0638 -19.3466]\n"," [ 67.0034 -10.3907  -2.8697]\n"," [-54.8219 -67.9932  19.5177]\n"," [-10.7869  13.9897  49.5203]\n"," [-19.0693 -15.506  -47.6997]\n"," [ 17.4433  49.3472 -20.1936]\n"," [-66.316   -0.8159   7.7016]\n"," [  4.4803 -41.5652  16.038 ]\n"," [-56.6145 -67.7992 -45.8652]]\n","\n","📂 Class 05 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/05/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -93.7359\n"," ├─ Max  : 96.3970\n"," ├─ Mean : -3.6729\n"," ├─ Std  : 47.5687\n"," └─ Sample Data:\n","[[ -8.0273  55.7049 -79.5385]\n"," [  1.2705   6.302   96.397 ]\n"," [ -6.0933 -70.6847 -20.2746]\n"," [ 47.7824  17.2083   1.3454]\n"," [-48.5596  10.6845  13.091 ]\n"," [ 35.0964 -27.4034 -93.7359]\n"," [  2.8314  88.323   -1.3848]\n"," [-42.4456 -17.0875 -69.874 ]\n"," [ 45.2718  25.511  -60.8797]]\n","\n","📂 Class 06 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/06/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -61.5181\n"," ├─ Max  : 55.2023\n"," ├─ Mean : -5.8993\n"," ├─ Std  : 34.8858\n"," └─ Sample Data:\n","[[ -9.8392  50.4696  31.5456]\n"," [ 21.0611 -61.5181 -54.2087]\n"," [-27.4429  20.7841 -56.9346]\n"," [  1.3448 -54.5936  25.5383]\n"," [ 18.5875   3.4214  -9.0781]\n"," [-32.6254 -37.7601 -54.0536]\n"," [ 27.5001  27.813  -57.959 ]\n"," [ 12.0627   7.1267  55.2023]\n"," [-26.0997   4.744   15.631 ]]\n","\n","📂 Class 08 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/08/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -102.5330\n"," ├─ Max  : 114.7380\n"," ├─ Mean : -1.7252\n"," ├─ Std  : 57.1376\n"," └─ Sample Data:\n","[[ -85.1721   27.1747   55.3571]\n"," [  41.817   -21.577  -102.533 ]\n"," [ 114.738    -3.9728   89.5442]\n"," [  18.887    -2.2339   10.7861]\n"," [ -64.6352    9.4542  -96.3116]\n"," [  12.7168   -1.2401  101.553 ]\n"," [ -52.2799   -5.5599  -20.5459]\n"," [ -37.3041  -31.165    53.1621]\n"," [   1.973    36.0232  -95.2365]]\n","\n","📂 Class 09 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/09/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -46.6694\n"," ├─ Max  : 46.7280\n"," ├─ Mean : -4.6406\n"," ├─ Std  : 27.9066\n"," └─ Sample Data:\n","[[ -2.0994  11.7574   0.0198]\n"," [ 46.728  -13.5908 -40.1141]\n"," [-35.4824 -23.6476 -38.1239]\n"," [ 44.0857  -6.8009  20.9951]\n"," [ 21.6419  31.9062 -40.411 ]\n"," [-30.3699  26.0363 -38.8646]\n"," [  7.4866 -33.6071 -20.1031]\n"," [  1.8624  -9.9197  40.7914]\n"," [-46.6694   3.9089  -2.7115]]\n","\n","📂 Class 10 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/10/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -73.4333\n"," ├─ Max  : 63.0991\n"," ├─ Mean : -0.9485\n"," ├─ Std  : 36.2279\n"," └─ Sample Data:\n","[[ 39.1349  11.601  -23.9408]\n"," [-73.4333 -30.9207  -0.2043]\n"," [-29.2001  45.2791  29.0616]\n"," [  4.209  -42.7445  30.9551]\n"," [-56.7928  29.7865 -33.7575]\n"," [ 63.0991 -46.6746  -1.2703]\n"," [-14.9766 -26.8483 -32.3674]\n"," [ 44.4272  40.0844  33.1058]\n"," [ -5.5014  42.452  -20.1714]]\n","\n","📂 Class 11 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/11/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -84.7123\n"," ├─ Max  : 85.9639\n"," ├─ Mean : -6.8638\n"," ├─ Std  : 34.8670\n"," └─ Sample Data:\n","[[ 15.494  -15.955   -7.3843]\n"," [  0.3111   5.3904  85.9639]\n"," [-12.4399  31.7775 -84.7123]\n"," [  7.3067 -37.177  -76.2814]\n"," [-11.4441  34.2681 -24.9297]\n"," [-10.3177  14.479   31.7019]\n"," [ 16.6566   7.0834 -54.5396]\n"," [-15.0613 -32.8293 -36.4945]\n"," [-11.6849 -26.7442  22.2407]]\n","\n","📂 Class 12 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/12/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -46.0149\n"," ├─ Max  : 49.0182\n"," ├─ Mean : -1.7436\n"," ├─ Std  : 34.0428\n"," └─ Sample Data:\n","[[  1.2851 -26.6466  13.895 ]\n"," [ 44.2851  48.1457 -37.705 ]\n"," [-43.2355  29.6682 -42.005 ]\n"," [ 42.1351  36.1182  40.9671]\n"," [-42.4672 -41.2818 -39.855 ]\n"," [ 48.7106 -21.9318 -37.705 ]\n"," [-15.9149  49.0182  13.9386]\n"," [-46.0149  -4.7318  -0.097 ]\n"," [  5.5851  10.3182 -31.5558]]\n","\n","📂 Class 13 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/13/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -127.0280\n"," ├─ Max  : 127.4950\n"," ├─ Mean : -7.1318\n"," ├─ Std  : 59.5631\n"," └─ Sample Data:\n","[[ -11.3506  -37.9226  -11.2949]\n"," [ 127.495    -0.2632  -67.9833]\n"," [-127.028    44.9615  -37.3693]\n"," [-107.902    -1.4071   69.7612]\n"," [  80.0632    0.7081   38.3604]\n"," [  28.8828   49.0412  -65.602 ]\n"," [ -99.7896  -47.943   -50.6508]\n"," [ -42.0713   43.7073  -13.5953]\n"," [ -21.6835    5.7878   62.5302]]\n","\n","📂 Class 14 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/14/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -101.7510\n"," ├─ Max  : 101.2500\n"," ├─ Mean : -6.5086\n"," ├─ Std  : 62.7174\n"," └─ Sample Data:\n","[[  43.2177   52.4112   54.0458]\n"," [ -99.9506  -36.8233 -101.751 ]\n"," [  32.046    34.081   -98.0899]\n"," [ -70.1905  -24.798    76.0068]\n"," [  82.3728  -56.4652   23.6938]\n"," [  -5.7534  -11.7776  -12.261 ]\n"," [ -63.4212   52.9445  -85.1847]\n"," [ 101.25     -3.7718   92.8678]\n"," [ -11.3597  -43.5055  -95.5663]]\n","\n","📂 Class 15 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/15/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -87.9908\n"," ├─ Max  : 91.5867\n"," ├─ Mean : -14.2489\n"," ├─ Std  : 49.5074\n"," └─ Sample Data:\n","[[-33.3298 -51.9169 -81.7832]\n"," [-10.1223 -27.1229  91.5867]\n"," [ -3.8379  72.9849 -24.3855]\n"," [ 46.3641   7.2564 -79.3277]\n"," [ 10.0339 -60.7272   3.6516]\n"," [-37.3104  31.5644 -87.9908]\n"," [-39.8922   0.1733 -13.9607]\n"," [ 36.755  -63.5682 -71.5871]\n"," [ 19.6582  64.2833 -82.1688]]\n","\n","✅ All classes processed.\n"]}],"source":["import numpy as np\n","import os\n","\n","# Ensure NumPy will print the full array\n","np.set_printoptions(precision=4, suppress=True, threshold=np.inf)\n","\n","base_path = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","\n","# List of classes (01–15, skipping 03 and 07)\n","classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","for cls in classes:\n","    file_path = os.path.join(base_path, cls, \"Outside9.npy\")\n","    if not os.path.isfile(file_path):\n","        print(f\"⚠️  File not found for class {cls}: {file_path}\")\n","        continue\n","\n","    data = np.load(file_path)\n","    flat = data.flatten()\n","    # Compute some summary statistics\n","    shape   = data.shape\n","    dtype   = data.dtype\n","    minimum = flat.min()\n","    maximum = flat.max()\n","    mean    = flat.mean()\n","    std     = flat.std()\n","\n","    # Nicely formatted output\n","    print(f\"\\n📂 Class {cls} — Outside9.npy\")\n","    print(f\" ├─ Path : {file_path}\")\n","    print(f\" ├─ Shape: {shape}\")\n","    print(f\" ├─ DType: {dtype}\")\n","    print(f\" ├─ Min  : {minimum:.4f}\")\n","    print(f\" ├─ Max  : {maximum:.4f}\")\n","    print(f\" ├─ Mean : {mean:.4f}\")\n","    print(f\" ├─ Std  : {std:.4f}\")\n","    # Print the full matrix\n","    print(\" └─ Sample Data:\")\n","    print(data)\n","\n","print(\"\\n✅ All classes processed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"PDddF5Sy-XbT"},"source":["### **3 - Generate Pose For Each Picture Using gt.yml**\n","\n","Purpose: This script generates pose files (Rotation & Translation matrices) for each image in the dataset.\n","\n","How it works:\n","1. Reads the depth image and removes the background using the mask.\n","2. Converts the depth into a point cloud (scene point cloud).\n","3. Loads the 3D model (`mesh.ply`) and samples it into a point cloud.\n","4. Uses ICP to align the model point cloud with the scene point cloud.\n","5. Saves the resulting [R|t] matrix as a `.npy` file named `poseXXXXXX.npy`.\n","\n","Output:\n","Each frame will have a file like `pose000123.npy` in the `pose/` folder.\n","It contains a 3×4 RT matrix with:\n","- R: rotation (3×3)\n","- t: translation (3×1)\n","\n","Use in RCVPose:\n","These pose files are used to:\n","- Project 3D keypoints onto 2D image space\n","- Generate 3D radius maps\n","- Serve as ground truth during training\n","\n","Requirements:\n","- `.dpt` depth images\n","- `.png` binary masks\n","- 3D model in `.ply` format\n","- Camera intrinsics `K`\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25982,"status":"ok","timestamp":1748563110254,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"Pj1gZdE7-9j4","outputId":"103a7060-4220-4088-e881-2c8d9b5f8d9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 01...\n"]},{"output_type":"stream","name":"stderr","text":["Class 01: 100%|██████████| 1236/1236 [00:00<00:00, 10671.39image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 02...\n"]},{"output_type":"stream","name":"stderr","text":["Class 02:  99%|█████████▉| 1201/1214 [00:00<00:00, 6688.11image/s]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Multiple objects in image 000000, using object 2.\n","⚠️ Multiple objects in image 000001, using object 2.\n","⚠️ Multiple objects in image 000002, using object 2.\n","⚠️ Multiple objects in image 000003, using object 2.\n","⚠️ Multiple objects in image 000004, using object 2.\n","⚠️ Multiple objects in image 000005, using object 2.\n","⚠️ Multiple objects in image 000006, using object 2.\n","⚠️ Multiple objects in image 000007, using object 2.\n","⚠️ Multiple objects in image 000008, using object 2.\n","⚠️ Multiple objects in image 000009, using object 2.\n","⚠️ Multiple objects in image 000010, using object 2.\n","⚠️ Multiple objects in image 000011, using object 2.\n","⚠️ Multiple objects in image 000012, using object 2.\n","⚠️ Multiple objects in image 000013, using object 2.\n","⚠️ Multiple objects in image 000014, using object 2.\n","⚠️ Multiple objects in image 000015, using object 2.\n","⚠️ Multiple objects in image 000016, using object 2.\n","⚠️ Multiple objects in image 000017, using object 2.\n","⚠️ Multiple objects in image 000018, using object 2.\n","⚠️ Multiple objects in image 000019, using object 2.\n","⚠️ Multiple objects in image 000020, using object 2.\n","⚠️ Multiple objects in image 000021, using object 2.\n","⚠️ Multiple objects in image 000022, using object 2.\n","⚠️ Multiple objects in image 000023, using object 2.\n","⚠️ Multiple objects in image 000024, using object 2.\n","⚠️ Multiple objects in image 000025, using object 2.\n","⚠️ Multiple objects in image 000026, using object 2.\n","⚠️ Multiple objects in image 000027, using object 2.\n","⚠️ Multiple objects in image 000028, using object 2.\n","⚠️ Multiple objects in image 000029, using object 2.\n","⚠️ Multiple objects in image 000030, using object 2.\n","⚠️ Multiple objects in image 000031, using object 2.\n","⚠️ Multiple objects in image 000032, using object 2.\n","⚠️ Multiple objects in image 000033, using object 2.\n","⚠️ Multiple objects in image 000034, using object 2.\n","⚠️ Multiple objects in image 000035, using object 2.\n","⚠️ Multiple objects in image 000036, using object 2.\n","⚠️ Multiple objects in image 000037, using object 2.\n","⚠️ Multiple objects in image 000038, using object 2.\n","⚠️ Multiple objects in image 000039, using object 2.\n","⚠️ Multiple objects in image 000040, using object 2.\n","⚠️ Multiple objects in image 000041, using object 2.\n","⚠️ Multiple objects in image 000042, using object 2.\n","⚠️ Multiple objects in image 000043, using object 2.\n","⚠️ Multiple objects in image 000044, using object 2.\n","⚠️ Multiple objects in image 000045, using object 2.\n","⚠️ Multiple objects in image 000046, using object 2.\n","⚠️ Multiple objects in image 000047, using object 2.\n","⚠️ Multiple objects in image 000048, using object 2.\n","⚠️ Multiple objects in image 000049, using object 2.\n","⚠️ Multiple objects in image 000050, using object 2.\n","⚠️ Multiple objects in image 000051, using object 2.\n","⚠️ Multiple objects in image 000052, using object 2.\n","⚠️ Multiple objects in image 000053, using object 2.\n","⚠️ Multiple objects in image 000054, using object 2.\n","⚠️ Multiple objects in image 000055, using object 2.\n","⚠️ Multiple objects in image 000056, using object 2.\n","⚠️ Multiple objects in image 000057, using object 2.\n","⚠️ Multiple objects in image 000058, using object 2.\n","⚠️ Multiple objects in image 000059, using object 2.\n","⚠️ Multiple objects in image 000060, using object 2.\n","⚠️ Multiple objects in image 000061, using object 2.\n","⚠️ Multiple objects in image 000062, using object 2.\n","⚠️ Multiple objects in image 000063, using object 2.\n","⚠️ Multiple objects in image 000064, using object 2.\n","⚠️ Multiple objects in image 000065, using object 2.\n","⚠️ Multiple objects in image 000066, using object 2.\n","⚠️ Multiple objects in image 000067, using object 2.\n","⚠️ Multiple objects in image 000068, using object 2.\n","⚠️ Multiple objects in image 000069, using object 2.\n","⚠️ Multiple objects in image 000070, using object 2.\n","⚠️ Multiple objects in image 000071, using object 2.\n","⚠️ Multiple objects in image 000072, using object 2.\n","⚠️ Multiple objects in image 000073, using object 2.\n","⚠️ Multiple objects in image 000074, using object 2.\n","⚠️ Multiple objects in image 000075, using object 2.\n","⚠️ Multiple objects in image 000076, using object 2.\n","⚠️ Multiple objects in image 000077, using object 2.\n","⚠️ Multiple objects in image 000078, using object 2.\n","⚠️ Multiple objects in image 000079, using object 2.\n","⚠️ Multiple objects in image 000080, using object 2.\n","⚠️ Multiple objects in image 000081, using object 2.\n","⚠️ Multiple objects in image 000082, using object 2.\n","⚠️ Multiple objects in image 000083, using object 2.\n","⚠️ Multiple objects in image 000084, using object 2.\n","⚠️ Multiple objects in image 000085, using object 2.\n","⚠️ Multiple objects in image 000086, using object 2.\n","⚠️ Multiple objects in image 000087, using object 2.\n","⚠️ Multiple objects in image 000088, using object 2.\n","⚠️ Multiple objects in image 000089, using object 2.\n","⚠️ Multiple objects in image 000090, using object 2.\n","⚠️ Multiple objects in image 000091, using object 2.\n","⚠️ Multiple objects in image 000092, using object 2.\n","⚠️ Multiple objects in image 000093, using object 2.\n","⚠️ Multiple objects in image 000094, using object 2.\n","⚠️ Multiple objects in image 000095, using object 2.\n","⚠️ Multiple objects in image 000096, using object 2.\n","⚠️ Multiple objects in image 000097, using object 2.\n","⚠️ Multiple objects in image 000098, using object 2.\n","⚠️ Multiple objects in image 000099, using object 2.\n","⚠️ Multiple objects in image 000100, using object 2.\n","⚠️ Multiple objects in image 000101, using object 2.\n","⚠️ Multiple objects in image 000102, using object 2.\n","⚠️ Multiple objects in image 000103, using object 2.\n","⚠️ Multiple objects in image 000104, using object 2.\n","⚠️ Multiple objects in image 000105, using object 2.\n","⚠️ Multiple objects in image 000106, using object 2.\n","⚠️ Multiple objects in image 000107, using object 2.\n","⚠️ Multiple objects in image 000108, using object 2.\n","⚠️ Multiple objects in image 000109, using object 2.\n","⚠️ Multiple objects in image 000110, using object 2.\n","⚠️ Multiple objects in image 000111, using object 2.\n","⚠️ Multiple objects in image 000112, using object 2.\n","⚠️ Multiple objects in image 000113, using object 2.\n","⚠️ Multiple objects in image 000114, using object 2.\n","⚠️ Multiple objects in image 000115, using object 2.\n","⚠️ Multiple objects in image 000116, using object 2.\n","⚠️ Multiple objects in image 000117, using object 2.\n","⚠️ Multiple objects in image 000118, using object 2.\n","⚠️ Multiple objects in image 000119, using object 2.\n","⚠️ Multiple objects in image 000120, using object 2.\n","⚠️ Multiple objects in image 000121, using object 2.\n","⚠️ Multiple objects in image 000122, using object 2.\n","⚠️ Multiple objects in image 000123, using object 2.\n","⚠️ Multiple objects in image 000124, using object 2.\n","⚠️ Multiple objects in image 000125, using object 2.\n","⚠️ Multiple objects in image 000126, using object 2.\n","⚠️ Multiple objects in image 000127, using object 2.\n","⚠️ Multiple objects in image 000128, using object 2.\n","⚠️ Multiple objects in image 000129, using object 2.\n","⚠️ Multiple objects in image 000130, using object 2.\n","⚠️ Multiple objects in image 000131, using object 2.\n","⚠️ Multiple objects in image 000132, using object 2.\n","⚠️ Multiple objects in image 000133, using object 2.\n","⚠️ Multiple objects in image 000134, using object 2.\n","⚠️ Multiple objects in image 000135, using object 2.\n","⚠️ Multiple objects in image 000136, using object 2.\n","⚠️ Multiple objects in image 000137, using object 2.\n","⚠️ Multiple objects in image 000138, using object 2.\n","⚠️ Multiple objects in image 000139, using object 2.\n","⚠️ Multiple objects in image 000140, using object 2.\n","⚠️ Multiple objects in image 000141, using object 2.\n","⚠️ Multiple objects in image 000142, using object 2.\n","⚠️ Multiple objects in image 000143, using object 2.\n","⚠️ Multiple objects in image 000144, using object 2.\n","⚠️ Multiple objects in image 000145, using object 2.\n","⚠️ Multiple objects in image 000146, using object 2.\n","⚠️ Multiple objects in image 000147, using object 2.\n","⚠️ Multiple objects in image 000148, using object 2.\n","⚠️ Multiple objects in image 000149, using object 2.\n","⚠️ Multiple objects in image 000150, using object 2.\n","⚠️ Multiple objects in image 000151, using object 2.\n","⚠️ Multiple objects in image 000152, using object 2.\n","⚠️ Multiple objects in image 000153, using object 2.\n","⚠️ Multiple objects in image 000154, using object 2.\n","⚠️ Multiple objects in image 000155, using object 2.\n","⚠️ Multiple objects in image 000156, using object 2.\n","⚠️ Multiple objects in image 000157, using object 2.\n","⚠️ Multiple objects in image 000158, using object 2.\n","⚠️ Multiple objects in image 000159, using object 2.\n","⚠️ Multiple objects in image 000160, using object 2.\n","⚠️ Multiple objects in image 000161, using object 2.\n","⚠️ Multiple objects in image 000162, using object 2.\n","⚠️ Multiple objects in image 000163, using object 2.\n","⚠️ Multiple objects in image 000164, using object 2.\n","⚠️ Multiple objects in image 000165, using object 2.\n","⚠️ Multiple objects in image 000166, using object 2.\n","⚠️ Multiple objects in image 000167, using object 2.\n","⚠️ Multiple objects in image 000168, using object 2.\n","⚠️ Multiple objects in image 000169, using object 2.\n","⚠️ Multiple objects in image 000170, using object 2.\n","⚠️ Multiple objects in image 000171, using object 2.\n","⚠️ Multiple objects in image 000172, using object 2.\n","⚠️ Multiple objects in image 000173, using object 2.\n","⚠️ Multiple objects in image 000174, using object 2.\n","⚠️ Multiple objects in image 000175, using object 2.\n","⚠️ Multiple objects in image 000176, using object 2.\n","⚠️ Multiple objects in image 000177, using object 2.\n","⚠️ Multiple objects in image 000178, using object 2.\n","⚠️ Multiple objects in image 000179, using object 2.\n","⚠️ Multiple objects in image 000180, using object 2.\n","⚠️ Multiple objects in image 000181, using object 2.\n","⚠️ Multiple objects in image 000182, using object 2.\n","⚠️ Multiple objects in image 000183, using object 2.\n","⚠️ Multiple objects in image 000184, using object 2.\n","⚠️ Multiple objects in image 000185, using object 2.\n","⚠️ Multiple objects in image 000186, using object 2.\n","⚠️ Multiple objects in image 000187, using object 2.\n","⚠️ Multiple objects in image 000188, using object 2.\n","⚠️ Multiple objects in image 000189, using object 2.\n","⚠️ Multiple objects in image 000190, using object 2.\n","⚠️ Multiple objects in image 000191, using object 2.\n","⚠️ Multiple objects in image 000192, using object 2.\n","⚠️ Multiple objects in image 000193, using object 2.\n","⚠️ Multiple objects in image 000194, using object 2.\n","⚠️ Multiple objects in image 000195, using object 2.\n","⚠️ Multiple objects in image 000196, using object 2.\n","⚠️ Multiple objects in image 000197, using object 2.\n","⚠️ Multiple objects in image 000198, using object 2.\n","⚠️ Multiple objects in image 000199, using object 2.\n","⚠️ Multiple objects in image 000200, using object 2.\n","⚠️ Multiple objects in image 000201, using object 2.\n","⚠️ Multiple objects in image 000202, using object 2.\n","⚠️ Multiple objects in image 000203, using object 2.\n","⚠️ Multiple objects in image 000204, using object 2.\n","⚠️ Multiple objects in image 000205, using object 2.\n","⚠️ Multiple objects in image 000206, using object 2.\n","⚠️ Multiple objects in image 000207, using object 2.\n","⚠️ Multiple objects in image 000208, using object 2.\n","⚠️ Multiple objects in image 000209, using object 2.\n","⚠️ Multiple objects in image 000210, using object 2.\n","⚠️ Multiple objects in image 000211, using object 2.\n","⚠️ Multiple objects in image 000212, using object 2.\n","⚠️ Multiple objects in image 000213, using object 2.\n","⚠️ Multiple objects in image 000214, using object 2.\n","⚠️ Multiple objects in image 000215, using object 2.\n","⚠️ Multiple objects in image 000216, using object 2.\n","⚠️ Multiple objects in image 000217, using object 2.\n","⚠️ Multiple objects in image 000218, using object 2.\n","⚠️ Multiple objects in image 000219, using object 2.\n","⚠️ Multiple objects in image 000220, using object 2.\n","⚠️ Multiple objects in image 000221, using object 2.\n","⚠️ Multiple objects in image 000222, using object 2.\n","⚠️ Multiple objects in image 000223, using object 2.\n","⚠️ Multiple objects in image 000224, using object 2.\n","⚠️ Multiple objects in image 000225, using object 2.\n","⚠️ Multiple objects in image 000226, using object 2.\n","⚠️ Multiple objects in image 000227, using object 2.\n","⚠️ Multiple objects in image 000228, using object 2.\n","⚠️ Multiple objects in image 000229, using object 2.\n","⚠️ Multiple objects in image 000230, using object 2.\n","⚠️ Multiple objects in image 000231, using object 2.\n","⚠️ Multiple objects in image 000232, using object 2.\n","⚠️ Multiple objects in image 000233, using object 2.\n","⚠️ Multiple objects in image 000234, using object 2.\n","⚠️ Multiple objects in image 000235, using object 2.\n","⚠️ Multiple objects in image 000236, using object 2.\n","⚠️ Multiple objects in image 000237, using object 2.\n","⚠️ Multiple objects in image 000238, using object 2.\n","⚠️ Multiple objects in image 000239, using object 2.\n","⚠️ Multiple objects in image 000240, using object 2.\n","⚠️ Multiple objects in image 000241, using object 2.\n","⚠️ Multiple objects in image 000242, using object 2.\n","⚠️ Multiple objects in image 000243, using object 2.\n","⚠️ Multiple objects in image 000244, using object 2.\n","⚠️ Multiple objects in image 000245, using object 2.\n","⚠️ Multiple objects in image 000246, using object 2.\n","⚠️ Multiple objects in image 000247, using object 2.\n","⚠️ Multiple objects in image 000248, using object 2.\n","⚠️ Multiple objects in image 000249, using object 2.\n","⚠️ Multiple objects in image 000250, using object 2.\n","⚠️ Multiple objects in image 000251, using object 2.\n","⚠️ Multiple objects in image 000252, using object 2.\n","⚠️ Multiple objects in image 000253, using object 2.\n","⚠️ Multiple objects in image 000254, using object 2.\n","⚠️ Multiple objects in image 000255, using object 2.\n","⚠️ Multiple objects in image 000256, using object 2.\n","⚠️ Multiple objects in image 000257, using object 2.\n","⚠️ Multiple objects in image 000258, using object 2.\n","⚠️ Multiple objects in image 000259, using object 2.\n","⚠️ Multiple objects in image 000260, using object 2.\n","⚠️ Multiple objects in image 000261, using object 2.\n","⚠️ Multiple objects in image 000262, using object 2.\n","⚠️ Multiple objects in image 000263, using object 2.\n","⚠️ Multiple objects in image 000264, using object 2.\n","⚠️ Multiple objects in image 000265, using object 2.\n","⚠️ Multiple objects in image 000266, using object 2.\n","⚠️ Multiple objects in image 000267, using object 2.\n","⚠️ Multiple objects in image 000268, using object 2.\n","⚠️ Multiple objects in image 000269, using object 2.\n","⚠️ Multiple objects in image 000270, using object 2.\n","⚠️ Multiple objects in image 000271, using object 2.\n","⚠️ Multiple objects in image 000272, using object 2.\n","⚠️ Multiple objects in image 000273, using object 2.\n","⚠️ Multiple objects in image 000274, using object 2.\n","⚠️ Multiple objects in image 000275, using object 2.\n","⚠️ Multiple objects in image 000276, using object 2.\n","⚠️ Multiple objects in image 000277, using object 2.\n","⚠️ Multiple objects in image 000278, using object 2.\n","⚠️ Multiple objects in image 000279, using object 2.\n","⚠️ Multiple objects in image 000280, using object 2.\n","⚠️ Multiple objects in image 000281, using object 2.\n","⚠️ Multiple objects in image 000282, using object 2.\n","⚠️ Multiple objects in image 000283, using object 2.\n","⚠️ Multiple objects in image 000284, using object 2.\n","⚠️ Multiple objects in image 000285, using object 2.\n","⚠️ Multiple objects in image 000286, using object 2.\n","⚠️ Multiple objects in image 000287, using object 2.\n","⚠️ Multiple objects in image 000288, using object 2.\n","⚠️ Multiple objects in image 000289, using object 2.\n","⚠️ Multiple objects in image 000290, using object 2.\n","⚠️ Multiple objects in image 000291, using object 2.\n","⚠️ Multiple objects in image 000292, using object 2.\n","⚠️ Multiple objects in image 000293, using object 2.\n","⚠️ Multiple objects in image 000294, using object 2.\n","⚠️ Multiple objects in image 000295, using object 2.\n","⚠️ Multiple objects in image 000296, using object 2.\n","⚠️ Multiple objects in image 000297, using object 2.\n","⚠️ Multiple objects in image 000298, using object 2.\n","⚠️ Multiple objects in image 000299, using object 2.\n","⚠️ Multiple objects in image 000300, using object 2.\n","⚠️ Multiple objects in image 000301, using object 2.\n","⚠️ Multiple objects in image 000302, using object 2.\n","⚠️ Multiple objects in image 000303, using object 2.\n","⚠️ Multiple objects in image 000304, using object 2.\n","⚠️ Multiple objects in image 000305, using object 2.\n","⚠️ Multiple objects in image 000306, using object 2.\n","⚠️ Multiple objects in image 000307, using object 2.\n","⚠️ Multiple objects in image 000308, using object 2.\n","⚠️ Multiple objects in image 000309, using object 2.\n","⚠️ Multiple objects in image 000310, using object 2.\n","⚠️ Multiple objects in image 000311, using object 2.\n","⚠️ Multiple objects in image 000312, using object 2.\n","⚠️ Multiple objects in image 000313, using object 2.\n","⚠️ Multiple objects in image 000314, using object 2.\n","⚠️ Multiple objects in image 000315, using object 2.\n","⚠️ Multiple objects in image 000316, using object 2.\n","⚠️ Multiple objects in image 000317, using object 2.\n","⚠️ Multiple objects in image 000318, using object 2.\n","⚠️ Multiple objects in image 000319, using object 2.\n","⚠️ Multiple objects in image 000320, using object 2.\n","⚠️ Multiple objects in image 000321, using object 2.\n","⚠️ Multiple objects in image 000322, using object 2.\n","⚠️ Multiple objects in image 000323, using object 2.\n","⚠️ Multiple objects in image 000324, using object 2.\n","⚠️ Multiple objects in image 000325, using object 2.\n","⚠️ Multiple objects in image 000326, using object 2.\n","⚠️ Multiple objects in image 000327, using object 2.\n","⚠️ Multiple objects in image 000328, using object 2.\n","⚠️ Multiple objects in image 000329, using object 2.\n","⚠️ Multiple objects in image 000330, using object 2.\n","⚠️ Multiple objects in image 000331, using object 2.\n","⚠️ Multiple objects in image 000332, using object 2.\n","⚠️ Multiple objects in image 000333, using object 2.\n","⚠️ Multiple objects in image 000334, using object 2.\n","⚠️ Multiple objects in image 000335, using object 2.\n","⚠️ Multiple objects in image 000336, using object 2.\n","⚠️ Multiple objects in image 000337, using object 2.\n","⚠️ Multiple objects in image 000338, using object 2.\n","⚠️ Multiple objects in image 000339, using object 2.\n","⚠️ Multiple objects in image 000340, using object 2.\n","⚠️ Multiple objects in image 000341, using object 2.\n","⚠️ Multiple objects in image 000342, using object 2.\n","⚠️ Multiple objects in image 000343, using object 2.\n","⚠️ Multiple objects in image 000344, using object 2.\n","⚠️ Multiple objects in image 000345, using object 2.\n","⚠️ Multiple objects in image 000346, using object 2.\n","⚠️ Multiple objects in image 000347, using object 2.\n","⚠️ Multiple objects in image 000348, using object 2.\n","⚠️ Multiple objects in image 000349, using object 2.\n","⚠️ Multiple objects in image 000350, using object 2.\n","⚠️ Multiple objects in image 000351, using object 2.\n","⚠️ Multiple objects in image 000352, using object 2.\n","⚠️ Multiple objects in image 000353, using object 2.\n","⚠️ Multiple objects in image 000354, using object 2.\n","⚠️ Multiple objects in image 000355, using object 2.\n","⚠️ Multiple objects in image 000356, using object 2.\n","⚠️ Multiple objects in image 000357, using object 2.\n","⚠️ Multiple objects in image 000358, using object 2.\n","⚠️ Multiple objects in image 000359, using object 2.\n","⚠️ Multiple objects in image 000360, using object 2.\n","⚠️ Multiple objects in image 000361, using object 2.\n","⚠️ Multiple objects in image 000362, using object 2.\n","⚠️ Multiple objects in image 000363, using object 2.\n","⚠️ Multiple objects in image 000364, using object 2.\n","⚠️ Multiple objects in image 000365, using object 2.\n","⚠️ Multiple objects in image 000366, using object 2.\n","⚠️ Multiple objects in image 000367, using object 2.\n","⚠️ Multiple objects in image 000368, using object 2.\n","⚠️ Multiple objects in image 000369, using object 2.\n","⚠️ Multiple objects in image 000370, using object 2.\n","⚠️ Multiple objects in image 000371, using object 2.\n","⚠️ Multiple objects in image 000372, using object 2.\n","⚠️ Multiple objects in image 000373, using object 2.\n","⚠️ Multiple objects in image 000374, using object 2.\n","⚠️ Multiple objects in image 000375, using object 2.\n","⚠️ Multiple objects in image 000376, using object 2.\n","⚠️ Multiple objects in image 000377, using object 2.\n","⚠️ Multiple objects in image 000378, using object 2.\n","⚠️ Multiple objects in image 000379, using object 2.\n","⚠️ Multiple objects in image 000380, using object 2.\n","⚠️ Multiple objects in image 000381, using object 2.\n","⚠️ Multiple objects in image 000382, using object 2.\n","⚠️ Multiple objects in image 000383, using object 2.\n","⚠️ Multiple objects in image 000384, using object 2.\n","⚠️ Multiple objects in image 000385, using object 2.\n","⚠️ Multiple objects in image 000386, using object 2.\n","⚠️ Multiple objects in image 000387, using object 2.\n","⚠️ Multiple objects in image 000388, using object 2.\n","⚠️ Multiple objects in image 000389, using object 2.\n","⚠️ Multiple objects in image 000390, using object 2.\n","⚠️ Multiple objects in image 000391, using object 2.\n","⚠️ Multiple objects in image 000392, using object 2.\n","⚠️ Multiple objects in image 000393, using object 2.\n","⚠️ Multiple objects in image 000394, using object 2.\n","⚠️ Multiple objects in image 000395, using object 2.\n","⚠️ Multiple objects in image 000396, using object 2.\n","⚠️ Multiple objects in image 000397, using object 2.\n","⚠️ Multiple objects in image 000398, using object 2.\n","⚠️ Multiple objects in image 000399, using object 2.\n","⚠️ Multiple objects in image 000400, using object 2.\n","⚠️ Multiple objects in image 000401, using object 2.\n","⚠️ Multiple objects in image 000402, using object 2.\n","⚠️ Multiple objects in image 000403, using object 2.\n","⚠️ Multiple objects in image 000404, using object 2.\n","⚠️ Multiple objects in image 000405, using object 2.\n","⚠️ Multiple objects in image 000406, using object 2.\n","⚠️ Multiple objects in image 000407, using object 2.\n","⚠️ Multiple objects in image 000408, using object 2.\n","⚠️ Multiple objects in image 000409, using object 2.\n","⚠️ Multiple objects in image 000410, using object 2.\n","⚠️ Multiple objects in image 000411, using object 2.\n","⚠️ Multiple objects in image 000412, using object 2.\n","⚠️ Multiple objects in image 000413, using object 2.\n","⚠️ Multiple objects in image 000414, using object 2.\n","⚠️ Multiple objects in image 000415, using object 2.\n","⚠️ Multiple objects in image 000416, using object 2.\n","⚠️ Multiple objects in image 000417, using object 2.\n","⚠️ Multiple objects in image 000418, using object 2.\n","⚠️ Multiple objects in image 000419, using object 2.\n","⚠️ Multiple objects in image 000420, using object 2.\n","⚠️ Multiple objects in image 000421, using object 2.\n","⚠️ Multiple objects in image 000422, using object 2.\n","⚠️ Multiple objects in image 000423, using object 2.\n","⚠️ Multiple objects in image 000424, using object 2.\n","⚠️ Multiple objects in image 000425, using object 2.\n","⚠️ Multiple objects in image 000426, using object 2.\n","⚠️ Multiple objects in image 000427, using object 2.\n","⚠️ Multiple objects in image 000428, using object 2.\n","⚠️ Multiple objects in image 000429, using object 2.\n","⚠️ Multiple objects in image 000430, using object 2.\n","⚠️ Multiple objects in image 000431, using object 2.\n","⚠️ Multiple objects in image 000432, using object 2.\n","⚠️ Multiple objects in image 000433, using object 2.\n","⚠️ Multiple objects in image 000434, using object 2.\n","⚠️ Multiple objects in image 000435, using object 2.\n","⚠️ Multiple objects in image 000436, using object 2.\n","⚠️ Multiple objects in image 000437, using object 2.\n","⚠️ Multiple objects in image 000438, using object 2.\n","⚠️ Multiple objects in image 000439, using object 2.\n","⚠️ Multiple objects in image 000440, using object 2.\n","⚠️ Multiple objects in image 000441, using object 2.\n","⚠️ Multiple objects in image 000442, using object 2.\n","⚠️ Multiple objects in image 000443, using object 2.\n","⚠️ Multiple objects in image 000444, using object 2.\n","⚠️ Multiple objects in image 000445, using object 2.\n","⚠️ Multiple objects in image 000446, using object 2.\n","⚠️ Multiple objects in image 000447, using object 2.\n","⚠️ Multiple objects in image 000448, using object 2.\n","⚠️ Multiple objects in image 000449, using object 2.\n","⚠️ Multiple objects in image 000450, using object 2.\n","⚠️ Multiple objects in image 000451, using object 2.\n","⚠️ Multiple objects in image 000452, using object 2.\n","⚠️ Multiple objects in image 000453, using object 2.\n","⚠️ Multiple objects in image 000454, using object 2.\n","⚠️ Multiple objects in image 000455, using object 2.\n","⚠️ Multiple objects in image 000456, using object 2.\n","⚠️ Multiple objects in image 000457, using object 2.\n","⚠️ Multiple objects in image 000458, using object 2.\n","⚠️ Multiple objects in image 000459, using object 2.\n","⚠️ Multiple objects in image 000460, using object 2.\n","⚠️ Multiple objects in image 000461, using object 2.\n","⚠️ Multiple objects in image 000462, using object 2.\n","⚠️ Multiple objects in image 000463, using object 2.\n","⚠️ Multiple objects in image 000464, using object 2.\n","⚠️ Multiple objects in image 000465, using object 2.\n","⚠️ Multiple objects in image 000466, using object 2.\n","⚠️ Multiple objects in image 000467, using object 2.\n","⚠️ Multiple objects in image 000468, using object 2.\n","⚠️ Multiple objects in image 000469, using object 2.\n","⚠️ Multiple objects in image 000470, using object 2.\n","⚠️ Multiple objects in image 000471, using object 2.\n","⚠️ Multiple objects in image 000472, using object 2.\n","⚠️ Multiple objects in image 000473, using object 2.\n","⚠️ Multiple objects in image 000474, using object 2.\n","⚠️ Multiple objects in image 000475, using object 2.\n","⚠️ Multiple objects in image 000476, using object 2.\n","⚠️ Multiple objects in image 000477, using object 2.\n","⚠️ Multiple objects in image 000478, using object 2.\n","⚠️ Multiple objects in image 000479, using object 2.\n","⚠️ Multiple objects in image 000480, using object 2.\n","⚠️ Multiple objects in image 000481, using object 2.\n","⚠️ Multiple objects in image 000482, using object 2.\n","⚠️ Multiple objects in image 000483, using object 2.\n","⚠️ Multiple objects in image 000484, using object 2.\n","⚠️ Multiple objects in image 000485, using object 2.\n","⚠️ Multiple objects in image 000486, using object 2.\n","⚠️ Multiple objects in image 000487, using object 2.\n","⚠️ Multiple objects in image 000488, using object 2.\n","⚠️ Multiple objects in image 000489, using object 2.\n","⚠️ Multiple objects in image 000490, using object 2.\n","⚠️ Multiple objects in image 000491, using object 2.\n","⚠️ Multiple objects in image 000492, using object 2.\n","⚠️ Multiple objects in image 000493, using object 2.\n","⚠️ Multiple objects in image 000494, using object 2.\n","⚠️ Multiple objects in image 000495, using object 2.\n","⚠️ Multiple objects in image 000496, using object 2.\n","⚠️ Multiple objects in image 000497, using object 2.\n","⚠️ Multiple objects in image 000498, using object 2.\n","⚠️ Multiple objects in image 000499, using object 2.\n","⚠️ Multiple objects in image 000500, using object 2.\n","⚠️ Multiple objects in image 000501, using object 2.\n","⚠️ Multiple objects in image 000502, using object 2.\n","⚠️ Multiple objects in image 000503, using object 2.\n","⚠️ Multiple objects in image 000504, using object 2.\n","⚠️ Multiple objects in image 000505, using object 2.\n","⚠️ Multiple objects in image 000506, using object 2.\n","⚠️ Multiple objects in image 000507, using object 2.\n","⚠️ Multiple objects in image 000508, using object 2.\n","⚠️ Multiple objects in image 000509, using object 2.\n","⚠️ Multiple objects in image 000510, using object 2.\n","⚠️ Multiple objects in image 000511, using object 2.\n","⚠️ Multiple objects in image 000512, using object 2.\n","⚠️ Multiple objects in image 000513, using object 2.\n","⚠️ Multiple objects in image 000514, using object 2.\n","⚠️ Multiple objects in image 000515, using object 2.\n","⚠️ Multiple objects in image 000516, using object 2.\n","⚠️ Multiple objects in image 000517, using object 2.\n","⚠️ Multiple objects in image 000518, using object 2.\n","⚠️ Multiple objects in image 000519, using object 2.\n","⚠️ Multiple objects in image 000520, using object 2.\n","⚠️ Multiple objects in image 000521, using object 2.\n","⚠️ Multiple objects in image 000522, using object 2.\n","⚠️ Multiple objects in image 000523, using object 2.\n","⚠️ Multiple objects in image 000524, using object 2.\n","⚠️ Multiple objects in image 000525, using object 2.\n","⚠️ Multiple objects in image 000526, using object 2.\n","⚠️ Multiple objects in image 000527, using object 2.\n","⚠️ Multiple objects in image 000528, using object 2.\n","⚠️ Multiple objects in image 000529, using object 2.\n","⚠️ Multiple objects in image 000530, using object 2.\n","⚠️ Multiple objects in image 000531, using object 2.\n","⚠️ Multiple objects in image 000532, using object 2.\n","⚠️ Multiple objects in image 000533, using object 2.\n","⚠️ Multiple objects in image 000534, using object 2.\n","⚠️ Multiple objects in image 000535, using object 2.\n","⚠️ Multiple objects in image 000536, using object 2.\n","⚠️ Multiple objects in image 000537, using object 2.\n","⚠️ Multiple objects in image 000538, using object 2.\n","⚠️ Multiple objects in image 000539, using object 2.\n","⚠️ Multiple objects in image 000540, using object 2.\n","⚠️ Multiple objects in image 000541, using object 2.\n","⚠️ Multiple objects in image 000542, using object 2.\n","⚠️ Multiple objects in image 000543, using object 2.\n","⚠️ Multiple objects in image 000544, using object 2.\n","⚠️ Multiple objects in image 000545, using object 2.\n","⚠️ Multiple objects in image 000546, using object 2.\n","⚠️ Multiple objects in image 000547, using object 2.\n","⚠️ Multiple objects in image 000548, using object 2.\n","⚠️ Multiple objects in image 000549, using object 2.\n","⚠️ Multiple objects in image 000550, using object 2.\n","⚠️ Multiple objects in image 000551, using object 2.\n","⚠️ Multiple objects in image 000552, using object 2.\n","⚠️ Multiple objects in image 000553, using object 2.\n","⚠️ Multiple objects in image 000554, using object 2.\n","⚠️ Multiple objects in image 000555, using object 2.\n","⚠️ Multiple objects in image 000556, using object 2.\n","⚠️ Multiple objects in image 000557, using object 2.\n","⚠️ Multiple objects in image 000558, using object 2.\n","⚠️ Multiple objects in image 000559, using object 2.\n","⚠️ Multiple objects in image 000560, using object 2.\n","⚠️ Multiple objects in image 000561, using object 2.\n","⚠️ Multiple objects in image 000562, using object 2.\n","⚠️ Multiple objects in image 000563, using object 2.\n","⚠️ Multiple objects in image 000564, using object 2.\n","⚠️ Multiple objects in image 000565, using object 2.\n","⚠️ Multiple objects in image 000566, using object 2.\n","⚠️ Multiple objects in image 000567, using object 2.\n","⚠️ Multiple objects in image 000568, using object 2.\n","⚠️ Multiple objects in image 000569, using object 2.\n","⚠️ Multiple objects in image 000570, using object 2.\n","⚠️ Multiple objects in image 000571, using object 2.\n","⚠️ Multiple objects in image 000572, using object 2.\n","⚠️ Multiple objects in image 000573, using object 2.\n","⚠️ Multiple objects in image 000574, using object 2.\n","⚠️ Multiple objects in image 000575, using object 2.\n","⚠️ Multiple objects in image 000576, using object 2.\n","⚠️ Multiple objects in image 000577, using object 2.\n","⚠️ Multiple objects in image 000578, using object 2.\n","⚠️ Multiple objects in image 000579, using object 2.\n","⚠️ Multiple objects in image 000580, using object 2.\n","⚠️ Multiple objects in image 000581, using object 2.\n","⚠️ Multiple objects in image 000582, using object 2.\n","⚠️ Multiple objects in image 000583, using object 2.\n","⚠️ Multiple objects in image 000584, using object 2.\n","⚠️ Multiple objects in image 000585, using object 2.\n","⚠️ Multiple objects in image 000586, using object 2.\n","⚠️ Multiple objects in image 000587, using object 2.\n","⚠️ Multiple objects in image 000588, using object 2.\n","⚠️ Multiple objects in image 000589, using object 2.\n","⚠️ Multiple objects in image 000590, using object 2.\n","⚠️ Multiple objects in image 000591, using object 2.\n","⚠️ Multiple objects in image 000592, using object 2.\n","⚠️ Multiple objects in image 000593, using object 2.\n","⚠️ Multiple objects in image 000594, using object 2.\n","⚠️ Multiple objects in image 000595, using object 2.\n","⚠️ Multiple objects in image 000596, using object 2.\n","⚠️ Multiple objects in image 000597, using object 2.\n","⚠️ Multiple objects in image 000598, using object 2.\n","⚠️ Multiple objects in image 000599, using object 2.\n","⚠️ Multiple objects in image 000600, using object 2.\n","⚠️ Multiple objects in image 000601, using object 2.\n","⚠️ Multiple objects in image 000602, using object 2.\n","⚠️ Multiple objects in image 000603, using object 2.\n","⚠️ Multiple objects in image 000604, using object 2.\n","⚠️ Multiple objects in image 000605, using object 2.\n","⚠️ Multiple objects in image 000606, using object 2.\n","⚠️ Multiple objects in image 000607, using object 2.\n","⚠️ Multiple objects in image 000608, using object 2.\n","⚠️ Multiple objects in image 000609, using object 2.\n","⚠️ Multiple objects in image 000610, using object 2.\n","⚠️ Multiple objects in image 000611, using object 2.\n","⚠️ Multiple objects in image 000612, using object 2.\n","⚠️ Multiple objects in image 000613, using object 2.\n","⚠️ Multiple objects in image 000614, using object 2.\n","⚠️ Multiple objects in image 000615, using object 2.\n","⚠️ Multiple objects in image 000616, using object 2.\n","⚠️ Multiple objects in image 000617, using object 2.\n","⚠️ Multiple objects in image 000618, using object 2.\n","⚠️ Multiple objects in image 000619, using object 2.\n","⚠️ Multiple objects in image 000620, using object 2.\n","⚠️ Multiple objects in image 000621, using object 2.\n","⚠️ Multiple objects in image 000622, using object 2.\n","⚠️ Multiple objects in image 000623, using object 2.\n","⚠️ Multiple objects in image 000624, using object 2.\n","⚠️ Multiple objects in image 000625, using object 2.\n","⚠️ Multiple objects in image 000626, using object 2.\n","⚠️ Multiple objects in image 000627, using object 2.\n","⚠️ Multiple objects in image 000628, using object 2.\n","⚠️ Multiple objects in image 000629, using object 2.\n","⚠️ Multiple objects in image 000630, using object 2.\n","⚠️ Multiple objects in image 000631, using object 2.\n","⚠️ Multiple objects in image 000632, using object 2.\n","⚠️ Multiple objects in image 000633, using object 2.\n","⚠️ Multiple objects in image 000634, using object 2.\n","⚠️ Multiple objects in image 000635, using object 2.\n","⚠️ Multiple objects in image 000636, using object 2.\n","⚠️ Multiple objects in image 000637, using object 2.\n","⚠️ Multiple objects in image 000638, using object 2.\n","⚠️ Multiple objects in image 000639, using object 2.\n","⚠️ Multiple objects in image 000640, using object 2.\n","⚠️ Multiple objects in image 000641, using object 2.\n","⚠️ Multiple objects in image 000642, using object 2.\n","⚠️ Multiple objects in image 000643, using object 2.\n","⚠️ Multiple objects in image 000644, using object 2.\n","⚠️ Multiple objects in image 000645, using object 2.\n","⚠️ Multiple objects in image 000646, using object 2.\n","⚠️ Multiple objects in image 000647, using object 2.\n","⚠️ Multiple objects in image 000648, using object 2.\n","⚠️ Multiple objects in image 000649, using object 2.\n","⚠️ Multiple objects in image 000650, using object 2.\n","⚠️ Multiple objects in image 000651, using object 2.\n","⚠️ Multiple objects in image 000652, using object 2.\n","⚠️ Multiple objects in image 000653, using object 2.\n","⚠️ Multiple objects in image 000654, using object 2.\n","⚠️ Multiple objects in image 000655, using object 2.\n","⚠️ Multiple objects in image 000656, using object 2.\n","⚠️ Multiple objects in image 000657, using object 2.\n","⚠️ Multiple objects in image 000658, using object 2.\n","⚠️ Multiple objects in image 000659, using object 2.\n","⚠️ Multiple objects in image 000660, using object 2.\n","⚠️ Multiple objects in image 000661, using object 2.\n","⚠️ Multiple objects in image 000662, using object 2.\n","⚠️ Multiple objects in image 000663, using object 2.\n","⚠️ Multiple objects in image 000664, using object 2.\n","⚠️ Multiple objects in image 000665, using object 2.\n","⚠️ Multiple objects in image 000666, using object 2.\n","⚠️ Multiple objects in image 000667, using object 2.\n","⚠️ Multiple objects in image 000668, using object 2.\n","⚠️ Multiple objects in image 000669, using object 2.\n","⚠️ Multiple objects in image 000670, using object 2.\n","⚠️ Multiple objects in image 000671, using object 2.\n","⚠️ Multiple objects in image 000672, using object 2.\n","⚠️ Multiple objects in image 000673, using object 2.\n","⚠️ Multiple objects in image 000674, using object 2.\n","⚠️ Multiple objects in image 000675, using object 2.\n","⚠️ Multiple objects in image 000676, using object 2.\n","⚠️ Multiple objects in image 000677, using object 2.\n","⚠️ Multiple objects in image 000678, using object 2.\n","⚠️ Multiple objects in image 000679, using object 2.\n","⚠️ Multiple objects in image 000680, using object 2.\n","⚠️ Multiple objects in image 000681, using object 2.\n","⚠️ Multiple objects in image 000682, using object 2.\n","⚠️ Multiple objects in image 000683, using object 2.\n","⚠️ Multiple objects in image 000684, using object 2.\n","⚠️ Multiple objects in image 000685, using object 2.\n","⚠️ Multiple objects in image 000686, using object 2.\n","⚠️ Multiple objects in image 000687, using object 2.\n","⚠️ Multiple objects in image 000688, using object 2.\n","⚠️ Multiple objects in image 000689, using object 2.\n","⚠️ Multiple objects in image 000690, using object 2.\n","⚠️ Multiple objects in image 000691, using object 2.\n","⚠️ Multiple objects in image 000692, using object 2.\n","⚠️ Multiple objects in image 000693, using object 2.\n","⚠️ Multiple objects in image 000694, using object 2.\n","⚠️ Multiple objects in image 000695, using object 2.\n","⚠️ Multiple objects in image 000696, using object 2.\n","⚠️ Multiple objects in image 000697, using object 2.\n","⚠️ Multiple objects in image 000698, using object 2.\n","⚠️ Multiple objects in image 000699, using object 2.\n","⚠️ Multiple objects in image 000700, using object 2.\n","⚠️ Multiple objects in image 000701, using object 2.\n","⚠️ Multiple objects in image 000702, using object 2.\n","⚠️ Multiple objects in image 000703, using object 2.\n","⚠️ Multiple objects in image 000704, using object 2.\n","⚠️ Multiple objects in image 000705, using object 2.\n","⚠️ Multiple objects in image 000706, using object 2.\n","⚠️ Multiple objects in image 000707, using object 2.\n","⚠️ Multiple objects in image 000708, using object 2.\n","⚠️ Multiple objects in image 000709, using object 2.\n","⚠️ Multiple objects in image 000710, using object 2.\n","⚠️ Multiple objects in image 000711, using object 2.\n","⚠️ Multiple objects in image 000712, using object 2.\n","⚠️ Multiple objects in image 000713, using object 2.\n","⚠️ Multiple objects in image 000714, using object 2.\n","⚠️ Multiple objects in image 000715, using object 2.\n","⚠️ Multiple objects in image 000716, using object 2.\n","⚠️ Multiple objects in image 000717, using object 2.\n","⚠️ Multiple objects in image 000718, using object 2.\n","⚠️ Multiple objects in image 000719, using object 2.\n","⚠️ Multiple objects in image 000720, using object 2.\n","⚠️ Multiple objects in image 000721, using object 2.\n","⚠️ Multiple objects in image 000722, using object 2.\n","⚠️ Multiple objects in image 000723, using object 2.\n","⚠️ Multiple objects in image 000724, using object 2.\n","⚠️ Multiple objects in image 000725, using object 2.\n","⚠️ Multiple objects in image 000726, using object 2.\n","⚠️ Multiple objects in image 000727, using object 2.\n","⚠️ Multiple objects in image 000728, using object 2.\n","⚠️ Multiple objects in image 000729, using object 2.\n","⚠️ Multiple objects in image 000730, using object 2.\n","⚠️ Multiple objects in image 000731, using object 2.\n","⚠️ Multiple objects in image 000732, using object 2.\n","⚠️ Multiple objects in image 000733, using object 2.\n","⚠️ Multiple objects in image 000734, using object 2.\n","⚠️ Multiple objects in image 000735, using object 2.\n","⚠️ Multiple objects in image 000736, using object 2.\n","⚠️ Multiple objects in image 000737, using object 2.\n","⚠️ Multiple objects in image 000738, using object 2.\n","⚠️ Multiple objects in image 000739, using object 2.\n","⚠️ Multiple objects in image 000740, using object 2.\n","⚠️ Multiple objects in image 000741, using object 2.\n","⚠️ Multiple objects in image 000742, using object 2.\n","⚠️ Multiple objects in image 000743, using object 2.\n","⚠️ Multiple objects in image 000744, using object 2.\n","⚠️ Multiple objects in image 000745, using object 2.\n","⚠️ Multiple objects in image 000746, using object 2.\n","⚠️ Multiple objects in image 000747, using object 2.\n","⚠️ Multiple objects in image 000748, using object 2.\n","⚠️ Multiple objects in image 000749, using object 2.\n","⚠️ Multiple objects in image 000750, using object 2.\n","⚠️ Multiple objects in image 000751, using object 2.\n","⚠️ Multiple objects in image 000752, using object 2.\n","⚠️ Multiple objects in image 000753, using object 2.\n","⚠️ Multiple objects in image 000754, using object 2.\n","⚠️ Multiple objects in image 000755, using object 2.\n","⚠️ Multiple objects in image 000756, using object 2.\n","⚠️ Multiple objects in image 000757, using object 2.\n","⚠️ Multiple objects in image 000758, using object 2.\n","⚠️ Multiple objects in image 000759, using object 2.\n","⚠️ Multiple objects in image 000760, using object 2.\n","⚠️ Multiple objects in image 000761, using object 2.\n","⚠️ Multiple objects in image 000762, using object 2.\n","⚠️ Multiple objects in image 000763, using object 2.\n","⚠️ Multiple objects in image 000764, using object 2.\n","⚠️ Multiple objects in image 000765, using object 2.\n","⚠️ Multiple objects in image 000766, using object 2.\n","⚠️ Multiple objects in image 000767, using object 2.\n","⚠️ Multiple objects in image 000768, using object 2.\n","⚠️ Multiple objects in image 000769, using object 2.\n","⚠️ Multiple objects in image 000770, using object 2.\n","⚠️ Multiple objects in image 000771, using object 2.\n","⚠️ Multiple objects in image 000772, using object 2.\n","⚠️ Multiple objects in image 000773, using object 2.\n","⚠️ Multiple objects in image 000774, using object 2.\n","⚠️ Multiple objects in image 000775, using object 2.\n","⚠️ Multiple objects in image 000776, using object 2.\n","⚠️ Multiple objects in image 000777, using object 2.\n","⚠️ Multiple objects in image 000778, using object 2.\n","⚠️ Multiple objects in image 000779, using object 2.\n","⚠️ Multiple objects in image 000780, using object 2.\n","⚠️ Multiple objects in image 000781, using object 2.\n","⚠️ Multiple objects in image 000782, using object 2.\n","⚠️ Multiple objects in image 000783, using object 2.\n","⚠️ Multiple objects in image 000784, using object 2.\n","⚠️ Multiple objects in image 000785, using object 2.\n","⚠️ Multiple objects in image 000786, using object 2.\n","⚠️ Multiple objects in image 000787, using object 2.\n","⚠️ Multiple objects in image 000788, using object 2.\n","⚠️ Multiple objects in image 000789, using object 2.\n","⚠️ Multiple objects in image 000790, using object 2.\n","⚠️ Multiple objects in image 000791, using object 2.\n","⚠️ Multiple objects in image 000792, using object 2.\n","⚠️ Multiple objects in image 000793, using object 2.\n","⚠️ Multiple objects in image 000794, using object 2.\n","⚠️ Multiple objects in image 000795, using object 2.\n","⚠️ Multiple objects in image 000796, using object 2.\n","⚠️ Multiple objects in image 000797, using object 2.\n","⚠️ Multiple objects in image 000798, using object 2.\n","⚠️ Multiple objects in image 000799, using object 2.\n","⚠️ Multiple objects in image 000800, using object 2.\n","⚠️ Multiple objects in image 000801, using object 2.\n","⚠️ Multiple objects in image 000802, using object 2.\n","⚠️ Multiple objects in image 000803, using object 2.\n","⚠️ Multiple objects in image 000804, using object 2.\n","⚠️ Multiple objects in image 000805, using object 2.\n","⚠️ Multiple objects in image 000806, using object 2.\n","⚠️ Multiple objects in image 000807, using object 2.\n","⚠️ Multiple objects in image 000808, using object 2.\n","⚠️ Multiple objects in image 000809, using object 2.\n","⚠️ Multiple objects in image 000810, using object 2.\n","⚠️ Multiple objects in image 000811, using object 2.\n","⚠️ Multiple objects in image 000812, using object 2.\n","⚠️ Multiple objects in image 000813, using object 2.\n","⚠️ Multiple objects in image 000814, using object 2.\n","⚠️ Multiple objects in image 000815, using object 2.\n","⚠️ Multiple objects in image 000816, using object 2.\n","⚠️ Multiple objects in image 000817, using object 2.\n","⚠️ Multiple objects in image 000818, using object 2.\n","⚠️ Multiple objects in image 000819, using object 2.\n","⚠️ Multiple objects in image 000820, using object 2.\n","⚠️ Multiple objects in image 000821, using object 2.\n","⚠️ Multiple objects in image 000822, using object 2.\n","⚠️ Multiple objects in image 000823, using object 2.\n","⚠️ Multiple objects in image 000824, using object 2.\n","⚠️ Multiple objects in image 000825, using object 2.\n","⚠️ Multiple objects in image 000826, using object 2.\n","⚠️ Multiple objects in image 000827, using object 2.\n","⚠️ Multiple objects in image 000828, using object 2.\n","⚠️ Multiple objects in image 000829, using object 2.\n","⚠️ Multiple objects in image 000830, using object 2.\n","⚠️ Multiple objects in image 000831, using object 2.\n","⚠️ Multiple objects in image 000832, using object 2.\n","⚠️ Multiple objects in image 000833, using object 2.\n","⚠️ Multiple objects in image 000834, using object 2.\n","⚠️ Multiple objects in image 000835, using object 2.\n","⚠️ Multiple objects in image 000836, using object 2.\n","⚠️ Multiple objects in image 000837, using object 2.\n","⚠️ Multiple objects in image 000838, using object 2.\n","⚠️ Multiple objects in image 000839, using object 2.\n","⚠️ Multiple objects in image 000840, using object 2.\n","⚠️ Multiple objects in image 000841, using object 2.\n","⚠️ Multiple objects in image 000842, using object 2.\n","⚠️ Multiple objects in image 000843, using object 2.\n","⚠️ Multiple objects in image 000844, using object 2.\n","⚠️ Multiple objects in image 000845, using object 2.\n","⚠️ Multiple objects in image 000846, using object 2.\n","⚠️ Multiple objects in image 000847, using object 2.\n","⚠️ Multiple objects in image 000848, using object 2.\n","⚠️ Multiple objects in image 000849, using object 2.\n","⚠️ Multiple objects in image 000850, using object 2.\n","⚠️ Multiple objects in image 000851, using object 2.\n","⚠️ Multiple objects in image 000852, using object 2.\n","⚠️ Multiple objects in image 000853, using object 2.\n","⚠️ Multiple objects in image 000854, using object 2.\n","⚠️ Multiple objects in image 000855, using object 2.\n","⚠️ Multiple objects in image 000856, using object 2.\n","⚠️ Multiple objects in image 000857, using object 2.\n","⚠️ Multiple objects in image 000858, using object 2.\n","⚠️ Multiple objects in image 000859, using object 2.\n","⚠️ Multiple objects in image 000860, using object 2.\n","⚠️ Multiple objects in image 000861, using object 2.\n","⚠️ Multiple objects in image 000862, using object 2.\n","⚠️ Multiple objects in image 000863, using object 2.\n","⚠️ Multiple objects in image 000864, using object 2.\n","⚠️ Multiple objects in image 000865, using object 2.\n","⚠️ Multiple objects in image 000866, using object 2.\n","⚠️ Multiple objects in image 000867, using object 2.\n","⚠️ Multiple objects in image 000868, using object 2.\n","⚠️ Multiple objects in image 000869, using object 2.\n","⚠️ Multiple objects in image 000870, using object 2.\n","⚠️ Multiple objects in image 000871, using object 2.\n","⚠️ Multiple objects in image 000872, using object 2.\n","⚠️ Multiple objects in image 000873, using object 2.\n","⚠️ Multiple objects in image 000874, using object 2.\n","⚠️ Multiple objects in image 000875, using object 2.\n","⚠️ Multiple objects in image 000876, using object 2.\n","⚠️ Multiple objects in image 000877, using object 2.\n","⚠️ Multiple objects in image 000878, using object 2.\n","⚠️ Multiple objects in image 000879, using object 2.\n","⚠️ Multiple objects in image 000880, using object 2.\n","⚠️ Multiple objects in image 000881, using object 2.\n","⚠️ Multiple objects in image 000882, using object 2.\n","⚠️ Multiple objects in image 000883, using object 2.\n","⚠️ Multiple objects in image 000884, using object 2.\n","⚠️ Multiple objects in image 000885, using object 2.\n","⚠️ Multiple objects in image 000886, using object 2.\n","⚠️ Multiple objects in image 000887, using object 2.\n","⚠️ Multiple objects in image 000888, using object 2.\n","⚠️ Multiple objects in image 000889, using object 2.\n","⚠️ Multiple objects in image 000890, using object 2.\n","⚠️ Multiple objects in image 000891, using object 2.\n","⚠️ Multiple objects in image 000892, using object 2.\n","⚠️ Multiple objects in image 000893, using object 2.\n","⚠️ Multiple objects in image 000894, using object 2.\n","⚠️ Multiple objects in image 000895, using object 2.\n","⚠️ Multiple objects in image 000896, using object 2.\n","⚠️ Multiple objects in image 000897, using object 2.\n","⚠️ Multiple objects in image 000898, using object 2.\n","⚠️ Multiple objects in image 000899, using object 2.\n","⚠️ Multiple objects in image 000900, using object 2.\n","⚠️ Multiple objects in image 000901, using object 2.\n","⚠️ Multiple objects in image 000902, using object 2.\n","⚠️ Multiple objects in image 000903, using object 2.\n","⚠️ Multiple objects in image 000904, using object 2.\n","⚠️ Multiple objects in image 000905, using object 2.\n","⚠️ Multiple objects in image 000906, using object 2.\n","⚠️ Multiple objects in image 000907, using object 2.\n","⚠️ Multiple objects in image 000908, using object 2.\n","⚠️ Multiple objects in image 000909, using object 2.\n","⚠️ Multiple objects in image 000910, using object 2.\n","⚠️ Multiple objects in image 000911, using object 2.\n","⚠️ Multiple objects in image 000912, using object 2.\n","⚠️ Multiple objects in image 000913, using object 2.\n","⚠️ Multiple objects in image 000914, using object 2.\n","⚠️ Multiple objects in image 000915, using object 2.\n","⚠️ Multiple objects in image 000916, using object 2.\n","⚠️ Multiple objects in image 000917, using object 2.\n","⚠️ Multiple objects in image 000918, using object 2.\n","⚠️ Multiple objects in image 000919, using object 2.\n","⚠️ Multiple objects in image 000920, using object 2.\n","⚠️ Multiple objects in image 000921, using object 2.\n","⚠️ Multiple objects in image 000922, using object 2.\n","⚠️ Multiple objects in image 000923, using object 2.\n","⚠️ Multiple objects in image 000924, using object 2.\n","⚠️ Multiple objects in image 000925, using object 2.\n","⚠️ Multiple objects in image 000926, using object 2.\n","⚠️ Multiple objects in image 000927, using object 2.\n","⚠️ Multiple objects in image 000928, using object 2.\n","⚠️ Multiple objects in image 000929, using object 2.\n","⚠️ Multiple objects in image 000930, using object 2.\n","⚠️ Multiple objects in image 000931, using object 2.\n","⚠️ Multiple objects in image 000932, using object 2.\n","⚠️ Multiple objects in image 000933, using object 2.\n","⚠️ Multiple objects in image 000934, using object 2.\n","⚠️ Multiple objects in image 000935, using object 2.\n","⚠️ Multiple objects in image 000936, using object 2.\n","⚠️ Multiple objects in image 000937, using object 2.\n","⚠️ Multiple objects in image 000938, using object 2.\n","⚠️ Multiple objects in image 000939, using object 2.\n","⚠️ Multiple objects in image 000940, using object 2.\n","⚠️ Multiple objects in image 000941, using object 2.\n","⚠️ Multiple objects in image 000942, using object 2.\n","⚠️ Multiple objects in image 000943, using object 2.\n","⚠️ Multiple objects in image 000944, using object 2.\n","⚠️ Multiple objects in image 000945, using object 2.\n","⚠️ Multiple objects in image 000946, using object 2.\n","⚠️ Multiple objects in image 000947, using object 2.\n","⚠️ Multiple objects in image 000948, using object 2.\n","⚠️ Multiple objects in image 000949, using object 2.\n","⚠️ Multiple objects in image 000950, using object 2.\n","⚠️ Multiple objects in image 000951, using object 2.\n","⚠️ Multiple objects in image 000952, using object 2.\n","⚠️ Multiple objects in image 000953, using object 2.\n","⚠️ Multiple objects in image 000954, using object 2.\n","⚠️ Multiple objects in image 000955, using object 2.\n","⚠️ Multiple objects in image 000956, using object 2.\n","⚠️ Multiple objects in image 000957, using object 2.\n","⚠️ Multiple objects in image 000958, using object 2.\n","⚠️ Multiple objects in image 000959, using object 2.\n","⚠️ Multiple objects in image 000960, using object 2.\n","⚠️ Multiple objects in image 000961, using object 2.\n","⚠️ Multiple objects in image 000962, using object 2.\n","⚠️ Multiple objects in image 000963, using object 2.\n","⚠️ Multiple objects in image 000964, using object 2.\n","⚠️ Multiple objects in image 000965, using object 2.\n","⚠️ Multiple objects in image 000966, using object 2.\n","⚠️ Multiple objects in image 000967, using object 2.\n","⚠️ Multiple objects in image 000968, using object 2.\n","⚠️ Multiple objects in image 000969, using object 2.\n","⚠️ Multiple objects in image 000970, using object 2.\n","⚠️ Multiple objects in image 000971, using object 2.\n","⚠️ Multiple objects in image 000972, using object 2.\n","⚠️ Multiple objects in image 000973, using object 2.\n","⚠️ Multiple objects in image 000974, using object 2.\n","⚠️ Multiple objects in image 000975, using object 2.\n","⚠️ Multiple objects in image 000976, using object 2.\n","⚠️ Multiple objects in image 000977, using object 2.\n","⚠️ Multiple objects in image 000978, using object 2.\n","⚠️ Multiple objects in image 000979, using object 2.\n","⚠️ Multiple objects in image 000980, using object 2.\n","⚠️ Multiple objects in image 000981, using object 2.\n","⚠️ Multiple objects in image 000982, using object 2.\n","⚠️ Multiple objects in image 000983, using object 2.\n","⚠️ Multiple objects in image 000984, using object 2.\n","⚠️ Multiple objects in image 000985, using object 2.\n","⚠️ Multiple objects in image 000986, using object 2.\n","⚠️ Multiple objects in image 000987, using object 2.\n","⚠️ Multiple objects in image 000988, using object 2.\n","⚠️ Multiple objects in image 000989, using object 2.\n","⚠️ Multiple objects in image 000990, using object 2.\n","⚠️ Multiple objects in image 000991, using object 2.\n","⚠️ Multiple objects in image 000992, using object 2.\n","⚠️ Multiple objects in image 000993, using object 2.\n","⚠️ Multiple objects in image 000994, using object 2.\n","⚠️ Multiple objects in image 000995, using object 2.\n","⚠️ Multiple objects in image 000996, using object 2.\n","⚠️ Multiple objects in image 000997, using object 2.\n","⚠️ Multiple objects in image 000998, using object 2.\n","⚠️ Multiple objects in image 000999, using object 2.\n","⚠️ Multiple objects in image 001000, using object 2.\n","⚠️ Multiple objects in image 001001, using object 2.\n","⚠️ Multiple objects in image 001002, using object 2.\n","⚠️ Multiple objects in image 001003, using object 2.\n","⚠️ Multiple objects in image 001004, using object 2.\n","⚠️ Multiple objects in image 001005, using object 2.\n","⚠️ Multiple objects in image 001006, using object 2.\n","⚠️ Multiple objects in image 001007, using object 2.\n","⚠️ Multiple objects in image 001008, using object 2.\n","⚠️ Multiple objects in image 001009, using object 2.\n","⚠️ Multiple objects in image 001010, using object 2.\n","⚠️ Multiple objects in image 001011, using object 2.\n","⚠️ Multiple objects in image 001012, using object 2.\n","⚠️ Multiple objects in image 001013, using object 2.\n","⚠️ Multiple objects in image 001014, using object 2.\n","⚠️ Multiple objects in image 001015, using object 2.\n","⚠️ Multiple objects in image 001016, using object 2.\n","⚠️ Multiple objects in image 001017, using object 2.\n","⚠️ Multiple objects in image 001018, using object 2.\n","⚠️ Multiple objects in image 001019, using object 2.\n","⚠️ Multiple objects in image 001020, using object 2.\n","⚠️ Multiple objects in image 001021, using object 2.\n","⚠️ Multiple objects in image 001022, using object 2.\n","⚠️ Multiple objects in image 001023, using object 2.\n","⚠️ Multiple objects in image 001024, using object 2.\n","⚠️ Multiple objects in image 001025, using object 2.\n","⚠️ Multiple objects in image 001026, using object 2.\n","⚠️ Multiple objects in image 001027, using object 2.\n","⚠️ Multiple objects in image 001028, using object 2.\n","⚠️ Multiple objects in image 001029, using object 2.\n","⚠️ Multiple objects in image 001030, using object 2.\n","⚠️ Multiple objects in image 001031, using object 2.\n","⚠️ Multiple objects in image 001032, using object 2.\n","⚠️ Multiple objects in image 001033, using object 2.\n","⚠️ Multiple objects in image 001034, using object 2.\n","⚠️ Multiple objects in image 001035, using object 2.\n","⚠️ Multiple objects in image 001036, using object 2.\n","⚠️ Multiple objects in image 001037, using object 2.\n","⚠️ Multiple objects in image 001038, using object 2.\n","⚠️ Multiple objects in image 001039, using object 2.\n","⚠️ Multiple objects in image 001040, using object 2.\n","⚠️ Multiple objects in image 001041, using object 2.\n","⚠️ Multiple objects in image 001042, using object 2.\n","⚠️ Multiple objects in image 001043, using object 2.\n","⚠️ Multiple objects in image 001044, using object 2.\n","⚠️ Multiple objects in image 001045, using object 2.\n","⚠️ Multiple objects in image 001046, using object 2.\n","⚠️ Multiple objects in image 001047, using object 2.\n","⚠️ Multiple objects in image 001048, using object 2.\n","⚠️ Multiple objects in image 001049, using object 2.\n","⚠️ Multiple objects in image 001050, using object 2.\n","⚠️ Multiple objects in image 001051, using object 2.\n","⚠️ Multiple objects in image 001052, using object 2.\n","⚠️ Multiple objects in image 001053, using object 2.\n","⚠️ Multiple objects in image 001054, using object 2.\n","⚠️ Multiple objects in image 001055, using object 2.\n","⚠️ Multiple objects in image 001056, using object 2.\n","⚠️ Multiple objects in image 001057, using object 2.\n","⚠️ Multiple objects in image 001058, using object 2.\n","⚠️ Multiple objects in image 001059, using object 2.\n","⚠️ Multiple objects in image 001060, using object 2.\n","⚠️ Multiple objects in image 001061, using object 2.\n","⚠️ Multiple objects in image 001062, using object 2.\n","⚠️ Multiple objects in image 001063, using object 2.\n","⚠️ Multiple objects in image 001064, using object 2.\n","⚠️ Multiple objects in image 001065, using object 2.\n","⚠️ Multiple objects in image 001066, using object 2.\n","⚠️ Multiple objects in image 001067, using object 2.\n","⚠️ Multiple objects in image 001068, using object 2.\n","⚠️ Multiple objects in image 001069, using object 2.\n","⚠️ Multiple objects in image 001070, using object 2.\n","⚠️ Multiple objects in image 001071, using object 2.\n","⚠️ Multiple objects in image 001072, using object 2.\n","⚠️ Multiple objects in image 001073, using object 2.\n","⚠️ Multiple objects in image 001074, using object 2.\n","⚠️ Multiple objects in image 001075, using object 2.\n","⚠️ Multiple objects in image 001076, using object 2.\n","⚠️ Multiple objects in image 001077, using object 2.\n","⚠️ Multiple objects in image 001078, using object 2.\n","⚠️ Multiple objects in image 001079, using object 2.\n","⚠️ Multiple objects in image 001080, using object 2.\n","⚠️ Multiple objects in image 001081, using object 2.\n","⚠️ Multiple objects in image 001082, using object 2.\n","⚠️ Multiple objects in image 001083, using object 2.\n","⚠️ Multiple objects in image 001084, using object 2.\n","⚠️ Multiple objects in image 001085, using object 2.\n","⚠️ Multiple objects in image 001086, using object 2.\n","⚠️ Multiple objects in image 001087, using object 2.\n","⚠️ Multiple objects in image 001088, using object 2.\n","⚠️ Multiple objects in image 001089, using object 2.\n","⚠️ Multiple objects in image 001090, using object 2.\n","⚠️ Multiple objects in image 001091, using object 2.\n","⚠️ Multiple objects in image 001092, using object 2.\n","⚠️ Multiple objects in image 001093, using object 2.\n","⚠️ Multiple objects in image 001094, using object 2.\n","⚠️ Multiple objects in image 001095, using object 2.\n","⚠️ Multiple objects in image 001096, using object 2.\n","⚠️ Multiple objects in image 001097, using object 2.\n","⚠️ Multiple objects in image 001098, using object 2.\n","⚠️ Multiple objects in image 001099, using object 2.\n","⚠️ Multiple objects in image 001100, using object 2.\n","⚠️ Multiple objects in image 001101, using object 2.\n","⚠️ Multiple objects in image 001102, using object 2.\n","⚠️ Multiple objects in image 001103, using object 2.\n","⚠️ Multiple objects in image 001104, using object 2.\n","⚠️ Multiple objects in image 001105, using object 2.\n","⚠️ Multiple objects in image 001106, using object 2.\n","⚠️ Multiple objects in image 001107, using object 2.\n","⚠️ Multiple objects in image 001108, using object 2.\n","⚠️ Multiple objects in image 001109, using object 2.\n","⚠️ Multiple objects in image 001110, using object 2.\n","⚠️ Multiple objects in image 001111, using object 2.\n","⚠️ Multiple objects in image 001112, using object 2.\n","⚠️ Multiple objects in image 001113, using object 2.\n","⚠️ Multiple objects in image 001114, using object 2.\n","⚠️ Multiple objects in image 001115, using object 2.\n","⚠️ Multiple objects in image 001116, using object 2.\n","⚠️ Multiple objects in image 001117, using object 2.\n","⚠️ Multiple objects in image 001118, using object 2.\n","⚠️ Multiple objects in image 001119, using object 2.\n","⚠️ Multiple objects in image 001120, using object 2.\n","⚠️ Multiple objects in image 001121, using object 2.\n","⚠️ Multiple objects in image 001122, using object 2.\n","⚠️ Multiple objects in image 001123, using object 2.\n","⚠️ Multiple objects in image 001124, using object 2.\n","⚠️ Multiple objects in image 001125, using object 2.\n","⚠️ Multiple objects in image 001126, using object 2.\n","⚠️ Multiple objects in image 001127, using object 2.\n","⚠️ Multiple objects in image 001128, using object 2.\n","⚠️ Multiple objects in image 001129, using object 2.\n","⚠️ Multiple objects in image 001130, using object 2.\n","⚠️ Multiple objects in image 001131, using object 2.\n","⚠️ Multiple objects in image 001132, using object 2.\n","⚠️ Multiple objects in image 001133, using object 2.\n","⚠️ Multiple objects in image 001134, using object 2.\n","⚠️ Multiple objects in image 001135, using object 2.\n","⚠️ Multiple objects in image 001136, using object 2.\n","⚠️ Multiple objects in image 001137, using object 2.\n","⚠️ Multiple objects in image 001138, using object 2.\n","⚠️ Multiple objects in image 001139, using object 2.\n","⚠️ Multiple objects in image 001140, using object 2.\n","⚠️ Multiple objects in image 001141, using object 2.\n","⚠️ Multiple objects in image 001142, using object 2.\n","⚠️ Multiple objects in image 001143, using object 2.\n","⚠️ Multiple objects in image 001144, using object 2.\n","⚠️ Multiple objects in image 001145, using object 2.\n","⚠️ Multiple objects in image 001146, using object 2.\n","⚠️ Multiple objects in image 001147, using object 2.\n","⚠️ Multiple objects in image 001148, using object 2.\n","⚠️ Multiple objects in image 001149, using object 2.\n","⚠️ Multiple objects in image 001150, using object 2.\n","⚠️ Multiple objects in image 001151, using object 2.\n","⚠️ Multiple objects in image 001152, using object 2.\n","⚠️ Multiple objects in image 001153, using object 2.\n","⚠️ Multiple objects in image 001154, using object 2.\n","⚠️ Multiple objects in image 001155, using object 2.\n","⚠️ Multiple objects in image 001156, using object 2.\n","⚠️ Multiple objects in image 001157, using object 2.\n","⚠️ Multiple objects in image 001158, using object 2.\n","⚠️ Multiple objects in image 001159, using object 2.\n","⚠️ Multiple objects in image 001160, using object 2.\n","⚠️ Multiple objects in image 001161, using object 2.\n","⚠️ Multiple objects in image 001162, using object 2.\n","⚠️ Multiple objects in image 001163, using object 2.\n","⚠️ Multiple objects in image 001164, using object 2.\n","⚠️ Multiple objects in image 001165, using object 2.\n","⚠️ Multiple objects in image 001166, using object 2.\n","⚠️ Multiple objects in image 001167, using object 2.\n","⚠️ Multiple objects in image 001168, using object 2.\n","⚠️ Multiple objects in image 001169, using object 2.\n","⚠️ Multiple objects in image 001170, using object 2.\n","⚠️ Multiple objects in image 001171, using object 2.\n","⚠️ Multiple objects in image 001172, using object 2.\n","⚠️ Multiple objects in image 001173, using object 2.\n","⚠️ Multiple objects in image 001174, using object 2.\n","⚠️ Multiple objects in image 001175, using object 2.\n","⚠️ Multiple objects in image 001176, using object 2.\n","⚠️ Multiple objects in image 001177, using object 2.\n","⚠️ Multiple objects in image 001178, using object 2.\n","⚠️ Multiple objects in image 001179, using object 2.\n","⚠️ Multiple objects in image 001180, using object 2.\n","⚠️ Multiple objects in image 001181, using object 2.\n","⚠️ Multiple objects in image 001182, using object 2.\n","⚠️ Multiple objects in image 001183, using object 2.\n","⚠️ Multiple objects in image 001184, using object 2.\n","⚠️ Multiple objects in image 001185, using object 2.\n","⚠️ Multiple objects in image 001186, using object 2.\n","⚠️ Multiple objects in image 001187, using object 2.\n","⚠️ Multiple objects in image 001188, using object 2.\n","⚠️ Multiple objects in image 001189, using object 2.\n","⚠️ Multiple objects in image 001190, using object 2.\n","⚠️ Multiple objects in image 001191, using object 2.\n","⚠️ Multiple objects in image 001192, using object 2.\n","⚠️ Multiple objects in image 001193, using object 2.\n","⚠️ Multiple objects in image 001194, using object 2.\n","⚠️ Multiple objects in image 001195, using object 2.\n","⚠️ Multiple objects in image 001196, using object 2.\n","⚠️ Multiple objects in image 001197, using object 2.\n","⚠️ Multiple objects in image 001198, using object 2.\n","⚠️ Multiple objects in image 001199, using object 2.\n","⚠️ Multiple objects in image 001200, using object 2.\n"]},{"output_type":"stream","name":"stderr","text":["\rClass 02: 100%|██████████| 1214/1214 [00:00<00:00, 5907.71image/s]\n"]},{"output_type":"stream","name":"stdout","text":["⚠️ Multiple objects in image 001201, using object 2.\n","⚠️ Multiple objects in image 001202, using object 2.\n","⚠️ Multiple objects in image 001203, using object 2.\n","⚠️ Multiple objects in image 001204, using object 2.\n","⚠️ Multiple objects in image 001205, using object 2.\n","⚠️ Multiple objects in image 001206, using object 2.\n","⚠️ Multiple objects in image 001207, using object 2.\n","⚠️ Multiple objects in image 001208, using object 2.\n","⚠️ Multiple objects in image 001209, using object 2.\n","⚠️ Multiple objects in image 001210, using object 2.\n","⚠️ Multiple objects in image 001211, using object 2.\n","⚠️ Multiple objects in image 001212, using object 2.\n","⚠️ Multiple objects in image 001213, using object 2.\n","\n","📂 Processing class 04...\n"]},{"output_type":"stream","name":"stderr","text":["Class 04: 100%|██████████| 1201/1201 [00:00<00:00, 9547.27image/s] \n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 05...\n"]},{"output_type":"stream","name":"stderr","text":["Class 05: 100%|██████████| 1196/1196 [00:00<00:00, 11186.71image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 06...\n"]},{"output_type":"stream","name":"stderr","text":["Class 06: 100%|██████████| 1179/1179 [00:00<00:00, 11241.79image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 08...\n"]},{"output_type":"stream","name":"stderr","text":["Class 08: 100%|██████████| 1188/1188 [00:00<00:00, 10792.22image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 09...\n"]},{"output_type":"stream","name":"stderr","text":["Class 09: 100%|██████████| 1254/1254 [00:00<00:00, 11244.28image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 10...\n"]},{"output_type":"stream","name":"stderr","text":["Class 10: 100%|██████████| 1253/1253 [00:00<00:00, 10976.78image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 11...\n"]},{"output_type":"stream","name":"stderr","text":["Class 11: 100%|██████████| 1220/1220 [00:00<00:00, 11118.95image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 12...\n"]},{"output_type":"stream","name":"stderr","text":["Class 12: 100%|██████████| 1237/1237 [00:00<00:00, 10891.55image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 13...\n"]},{"output_type":"stream","name":"stderr","text":["Class 13: 100%|██████████| 1152/1152 [00:00<00:00, 10826.80image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 14...\n"]},{"output_type":"stream","name":"stderr","text":["Class 14: 100%|██████████| 1227/1227 [00:00<00:00, 10460.50image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 15...\n"]},{"output_type":"stream","name":"stderr","text":["Class 15: 100%|██████████| 1243/1243 [00:00<00:00, 10975.90image/s]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ All poses extracted and saved from gt.yml files.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import numpy as np\n","import yaml\n","from tqdm import tqdm\n","\n","# Base dataset path\n","base_root = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","class_ids = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n","\n","for class_id in class_ids:\n","    print(f\"\\n📂 Processing class {class_id}...\")\n","    class_path = os.path.join(base_root, class_id)\n","    gt_path    = os.path.join(class_path, \"gt.yml\")\n","    pose_dir   = os.path.join(class_path, \"pose\")\n","\n","    if not os.path.exists(gt_path):\n","        print(f\"⚠️ gt.yml not found for class {class_id} — skipped.\")\n","        continue\n","    os.makedirs(pose_dir, exist_ok=True)\n","\n","    with open(gt_path, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    for img_id, pose_list in tqdm(gt_data.items(), desc=f\"Class {class_id}\", unit=\"image\"):\n","        idx = int(img_id)\n","\n","        # If multiple objects, pick the second one (index 1)\n","        if len(pose_list) > 1:\n","            print(f\"⚠️ Multiple objects in image {idx:06d}, using object 2.\")\n","            selected_pose = pose_list[1]\n","        else:\n","            selected_pose = pose_list[0]\n","\n","        # Extract R, t and build 3x4 matrix\n","        R = np.array(selected_pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)\n","        t = np.array(selected_pose['cam_t_m2c'], dtype=np.float32).reshape(3, 1)  # mm\n","        RT = np.hstack([R, t])\n","\n","        save_path = os.path.join(pose_dir, f\"pose{idx:06d}.npy\")\n","        np.save(save_path, RT)\n","\n","print(\"\\n✅ All poses extracted and saved from gt.yml files.\")"]},{"cell_type":"markdown","metadata":{"id":"yim04eDar2j1"},"source":["### 4 - Modify RGB, Mask, Depth Files Format From 4 Digits to 6 Digits - Radius Map Needs Files in 6-Digit Format"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1053,"status":"ok","timestamp":1748563123594,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"yV8C9kelWjbe","outputId":"e4b3920f-f33a-4178-fe96-59f235921b66"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","====================🔧 Renaming class 01=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 02=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1214 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1215 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1214 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 04=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":["                                        "]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 05=====================\n","Folder     │  Count\n","------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\r"]},{"output_type":"stream","name":"stdout","text":["🌈 rgb      │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 06=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 08=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 09=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 10=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 11=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 12=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 13=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 14=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 15=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1243 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1225 files\n"]},{"output_type":"stream","name":"stderr","text":["                                        "]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1243 files\n"]},{"output_type":"stream","name":"stderr","text":["                                        "]},{"output_type":"stream","name":"stdout","text":["============================================================\n","            \n","🏁  All files renamed successfully.             \n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["from tqdm import tqdm\n","import os\n","\n","def rename_files_to_6_digit(base_dir, classes):\n","    line_width = 60\n","    for cls in classes:\n","        header = f\"🔧 Renaming class {cls}\"\n","        print(\"\\n\" + header.center(line_width, \"=\"))\n","        print(f\"{'Folder':<10} │ {'Count':>6}\")\n","        print(\"-\" * line_width)\n","\n","        for folder_name in ['rgb', 'mask', 'depth']:\n","            emoji = {\n","                'rgb':   '🌈',\n","                'mask':  '🛡️ ',\n","                'depth': '🌊 '\n","            }[folder_name]\n","\n","            folder_path = os.path.join(base_dir, cls, folder_name)\n","            if not os.path.isdir(folder_path):\n","                print(f\"{emoji} {folder_name:<8} │  {'—':>6}  (not found)\")\n","                continue\n","\n","            file_list = [\n","                f for f in os.listdir(folder_path)\n","                if os.path.splitext(f)[0].isdigit()\n","            ]\n","            print(f\"{emoji} {folder_name:<8} │ {len(file_list):>6} files\")\n","            for fname in tqdm(\n","                file_list,\n","                desc=f\"{cls}/{folder_name}\",\n","                unit=\"file\",\n","                leave=False,\n","                ncols=40\n","            ):\n","                name, ext = os.path.splitext(fname)\n","                idx = int(name)\n","                new_name = f\"{idx:06d}{ext}\"\n","                os.rename(\n","                    os.path.join(folder_path, fname),\n","                    os.path.join(folder_path, new_name)\n","                )\n","\n","        print(\"=\" * line_width)\n","    print(\"\\n🏁  All files renamed successfully.\".center(line_width))\n","\n","# === Set base path and class list ===\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","classes  = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","rename_files_to_6_digit(base_dir, classes)\n"]},{"cell_type":"markdown","metadata":{"id":"WxdBZpOTodXW"},"source":["###5 -  Convert Depth files from .png to .dpt - Radius Map Needs Depth Files With .dpt Format"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68092,"status":"ok","timestamp":1748563199824,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"ZPZWinGpongz","outputId":"528d8a39-dc58-4801-ead8-0b8b2f199194"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 01...\n"]},{"output_type":"stream","name":"stderr","text":["Class 01: 100%|██████████| 1236/1236 [00:04<00:00, 279.11file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 02...\n"]},{"output_type":"stream","name":"stderr","text":["Class 02: 100%|██████████| 1214/1214 [00:06<00:00, 178.34file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 04...\n"]},{"output_type":"stream","name":"stderr","text":["Class 04: 100%|██████████| 1201/1201 [00:04<00:00, 279.07file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 05...\n"]},{"output_type":"stream","name":"stderr","text":["Class 05: 100%|██████████| 1196/1196 [00:05<00:00, 226.76file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 06...\n"]},{"output_type":"stream","name":"stderr","text":["Class 06: 100%|██████████| 1179/1179 [00:05<00:00, 217.49file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 08...\n"]},{"output_type":"stream","name":"stderr","text":["Class 08: 100%|██████████| 1188/1188 [00:05<00:00, 213.22file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 09...\n"]},{"output_type":"stream","name":"stderr","text":["Class 09: 100%|██████████| 1254/1254 [00:04<00:00, 286.25file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 10...\n"]},{"output_type":"stream","name":"stderr","text":["Class 10: 100%|██████████| 1253/1253 [00:04<00:00, 276.32file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 11...\n"]},{"output_type":"stream","name":"stderr","text":["Class 11: 100%|██████████| 1220/1220 [00:05<00:00, 213.14file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 12...\n"]},{"output_type":"stream","name":"stderr","text":["Class 12: 100%|██████████| 1237/1237 [00:05<00:00, 216.27file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 13...\n"]},{"output_type":"stream","name":"stderr","text":["Class 13: 100%|██████████| 1152/1152 [00:04<00:00, 278.80file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 14...\n"]},{"output_type":"stream","name":"stderr","text":["Class 14: 100%|██████████| 1227/1227 [00:04<00:00, 278.09file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 15...\n"]},{"output_type":"stream","name":"stderr","text":["Class 15: 100%|██████████| 1243/1243 [00:07<00:00, 169.46file/s]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ All PNG files have been converted to .dpt format.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import numpy as np\n","import cv2\n","from PIL import Image\n","import struct\n","from tqdm import tqdm\n","\n","def save_dpt_file(png_path, dpt_path, scale=1.0):\n","    \"\"\"\n","    Converts a depth .png file to a .dpt file with proper binary format:\n","    [uint32 height][uint32 width][uint16 depth values]\n","    Then deletes the original .png file.\n","    \"\"\"\n","    # Read PNG (16-bit depth) safely\n","    depth_img = cv2.imread(png_path, cv2.IMREAD_UNCHANGED)  # preserves uint16\n","    if depth_img is None:\n","        raise ValueError(f\"Failed to read depth PNG: {png_path}\")\n","    depth_img = (depth_img.astype(np.float32) * scale).astype(np.uint16)\n","\n","    h, w = depth_img.shape\n","\n","    with open(dpt_path, 'wb') as f:\n","        f.write(struct.pack('I', h))\n","        f.write(struct.pack('I', w))\n","        f.write(depth_img.tobytes())\n","\n","    # Delete the original .png file after successful conversion\n","    os.remove(png_path)\n","\n","def convert_all_png_to_dpt_in_classes(base_dir, classes, scale=1.0):\n","    \"\"\"\n","    Processes all PNG files in the 'depth' folder of each class and creates corresponding .dpt files.\n","    \"\"\"\n","    for cls in classes:\n","        depth_dir = os.path.join(base_dir, cls, \"depth\")\n","        if not os.path.exists(depth_dir):\n","            print(f\"⚠️ Skipping class {cls}: no depth folder found.\")\n","            continue\n","\n","        print(f\"\\n🔍 Processing class {cls}...\")\n","\n","        png_files = sorted([f for f in os.listdir(depth_dir) if f.endswith(\".png\")])\n","        for fname in tqdm(png_files, desc=f\"Class {cls}\", unit=\"file\"):\n","            png_path = os.path.join(depth_dir, fname)\n","            dpt_path = os.path.join(depth_dir, os.path.splitext(fname)[0] + \".dpt\")\n","            save_dpt_file(png_path, dpt_path, scale)\n","\n","    print(\"\\n✅ All PNG files have been converted to .dpt format.\")\n","\n","# === Base dataset path and class list ===\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","# PNG values are already in mm → no scaling needed\n","scale = 1.0\n","\n","convert_all_png_to_dpt_in_classes(base_dir, classes, scale)"]},{"cell_type":"markdown","metadata":{"id":"za-C--7N2egf"},"source":["### 3 - Generate Radius Map\n","\n","We compute the mesh and keypoints to:\n","\n","* Mesh, accurately represent an object’s 3D shape.\n","* Reduce complexity by sampling a few representative points.\n","* Enable fast registration and matching across views.\n","* Support precise pose estimation via 3D–2D point correspondences.\n","* Improve efficiency in real-time or learning-based applications.\n"]},{"cell_type":"markdown","metadata":{"id":"9ppcG2yKdz_s"},"source":["### **Generate Radius Map**"]},{"cell_type":"code","source":["# If running in Colab, install these once:\n","# !pip install --quiet open3d numba tqdm\n","\n","import numpy as np\n","from PIL import Image\n","import open3d as o3d\n","import os\n","from numba import jit, prange\n","from tqdm import tqdm\n","import cv2\n","import argparse\n","from typing import List\n","\n","# List of Linemod classes to process\n","linemod_cls_names     = ['01']\n","\n","# Camera intrinsics (will cast to float64 later for max precision)\n","linemod_K = np.array([\n","    [572.4114,   0.0,     325.2611],\n","    [  0.0,     573.57043, 242.04899],\n","    [  0.0,       0.0,       1.0   ]\n","], dtype=np.float64)  # changed to float64 for higher precision\n","\n","# Paths (adjust if needed)\n","linemod_path          = \"/content/dataset/linemod/Linemod_preprocessed/data/\"\n","original_linemod_path = linemod_path\n","\n","# Depth scale: multiply raw depth value (read from PNG or DPT) by this factor to get millimetres.\n","# If PNG already mm use 1.0; if stored in metres*1000 choose accordingly.\n","DEPTH_SCALE = 1.0\n","\n","# ======== NEW: Argument parser so users can adjust paths / scale easily ========\n","parser = argparse.ArgumentParser(description=\"Generate per-pixel radius maps for LINEMOD dataset with high-precision float64 computation.\")\n","parser.add_argument(\"--dataset_path\", type=str, default=\"/content/dataset/linemod/Linemod_preprocessed/data/\",\n","                    help=\"Root path to LINEMOD-Preprocessed data directory (default: path used in original Colab script).\")\n","parser.add_argument(\"--classes\", type=str, nargs=\"*\", default=['01'],\n","                    help=\"Space-separated list of class IDs to process, e.g. 01 02 05 (default: 01)\")\n","parser.add_argument(\"--depth_scale\", type=float, default=1.0,\n","                    help=\"Scale factor that converts stored depth units to millimetres. Use 1.0 if depth already in mm; 1000 if depth in metres, etc.\")\n","# Robust to unknown extra args (e.g., from Jupyter/Colab)\n","args, _ = parser.parse_known_args()\n","\n","# After parsing, override globals to keep rest of script unchanged\n","linemod_path: str = args.dataset_path\n","original_linemod_path: str = linemod_path  # keep alias used later in code\n","linemod_cls_names: List[str] = args.classes\n","DEPTH_SCALE: float = args.depth_scale\n","# ======== END argparse section ========\n","\n","def project(xyz: np.ndarray, K: np.ndarray, RT: np.ndarray):\n","    \"\"\"Project 3D points into the image plane (float64 precision).\"\"\"\n","    xyz_cam = xyz @ RT[:, :3].T + RT[:, 3:].T\n","    proj    = xyz_cam @ K.T\n","    xy      = proj[:, :2] / proj[:, 2:3]\n","    return xy, xyz_cam\n","\n","def read_depth(path):\n","    \"\"\"Read .dpt or fallback PNG depth.\"\"\"\n","    if path.endswith(\".dpt\"):\n","        with open(path, \"rb\") as f:\n","            h, w  = np.fromfile(f, dtype=np.uint32, count=2)\n","            data  = np.fromfile(f, dtype=np.uint16, count=h*w)\n","        return data.reshape((h, w)).astype(np.float64)\n","    else:\n","        # Use cv2 to preserve original bit depth (e.g., 16-bit PNG)\n","        d = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","        if d is None:\n","            raise IOError(f\"Cannot read depth image: {path}\")\n","        return d.astype(np.float64)\n","\n","# ======== INSERT kernel definition HERE ========\n","@jit(nopython=True, parallel=True, fastmath=True)\n","def compute_radius_map(depth: np.ndarray, kp_cam: np.ndarray, K: np.ndarray) -> np.ndarray:\n","    \"\"\"Compute per-pixel Euclidean distance (in mm) to kp_cam with float64 precision.\"\"\"\n","    H, W = depth.shape\n","    radius_map = np.zeros((H, W), dtype=np.float64)\n","    fx = K[0, 0]\n","    fy = K[1, 1]\n","    cx = K[0, 2]\n","    cy = K[1, 2]\n","    for v in prange(H):\n","        for u in range(W):\n","            z = depth[v, u]\n","            if z == 0.0:\n","                continue\n","            x = (u - cx) * z / fx\n","            y = (v - cy) * z / fy\n","            dx = x - kp_cam[0]\n","            dy = y - kp_cam[1]\n","            dz = z - kp_cam[2]\n","            radius_map[v, u] = (dx * dx + dy * dy + dz * dz) ** 0.5\n","    return radius_map\n","# ======== END kernel ========\n","\n","# Track last folder per class\n","last_folder = {}\n","\n","for cls in linemod_cls_names:\n","    depth_dir = os.path.join(linemod_path, cls, \"depth\")\n","    if not os.path.isdir(depth_dir):\n","        continue\n","    dpt_files = [f for f in os.listdir(depth_dir) if f.endswith(\".dpt\")]\n","    if not dpt_files:\n","        continue\n","    keypts = np.load(os.path.join(linemod_path, cls, \"Outside9.npy\"))\n","\n","    # Print header\n","    print(\"\\n\" + f\"📂 Class {cls}\".center(80, \"─\"))\n","\n","    # One progress bar for this entire class\n","    total_steps = len(keypts) * len(dpt_files)\n","    pbar = tqdm(\n","        total=total_steps,\n","        ncols=80,\n","        bar_format=\"{desc} │{bar}| {n_fmt}/{total_fmt}\"\n","    )\n","\n","    # Preload mesh (optional)\n","    _ = o3d.io.read_point_cloud(os.path.join(linemod_path, cls, \"mesh.ply\"))\n","\n","    # Process each keypoint folder\n","    for idx_pt, kp in enumerate(keypts, start=1):\n","        folder_name = f\"Out_pt{idx_pt}_dm\"\n","        out_folder  = os.path.join(original_linemod_path, cls, folder_name)\n","        os.makedirs(out_folder, exist_ok=True)\n","        last_folder[cls] = folder_name\n","\n","        for fname in dpt_files:\n","            # Update description to show current folder\n","            pbar.set_description(f\"🔄 Class {cls} | 🔑 {folder_name}\")\n","\n","            # Compose related file paths\n","            base = os.path.splitext(fname)[0]\n","            depth_path = os.path.join(depth_dir, fname)\n","            mask_path  = os.path.join(linemod_path, cls, \"mask\", fname.replace(\".dpt\", \".png\"))\n","            pose_path  = os.path.join(linemod_path, cls, \"pose\", f\"pose{base}.npy\")\n","\n","            # Check existence\n","            if not os.path.isfile(pose_path):\n","                print(f\"⚠️  Skipped {cls}/{fname}: missing pose file {pose_path}\")\n","                pbar.update(1)\n","                continue\n","            if not os.path.isfile(mask_path):\n","                print(f\"⚠️  Skipped {cls}/{fname}: missing mask file {mask_path}\")\n","                pbar.update(1)\n","                continue\n","\n","            # Read & mask depth\n","            depth = read_depth(depth_path) * DEPTH_SCALE  # now float64\n","            mask  = np.asarray(Image.open(mask_path).convert(\"L\"))\n","            depth[mask == 0] = 0.0\n","\n","            # Load pose matrix (ensure float64)\n","            RT   = np.load(pose_path).astype(np.float64)\n","\n","            # Compute keypoint position in camera space (float64)\n","            _, kp_cam_xyz = project(np.array([kp], dtype=np.float64), linemod_K, RT)\n","            kp_cam = kp_cam_xyz[0]\n","\n","            # Build & save radius map using new kernel\n","            radius_map = compute_radius_map(depth, kp_cam, linemod_K)\n","            np.save(os.path.join(out_folder, base + \".npy\"), radius_map)\n","\n","            pbar.update(1)\n","\n","    # Close this class's bar\n","    pbar.close()\n","\n","# Final summary\n","print(\"\\n\" + \"🔔 Summary\".center(80, \"─\"))\n","for cls in linemod_cls_names:\n","    folder = last_folder.get(cls, \"—\")\n","    print(f\"Class {cls}  →  {folder}\")\n","print(\"✅ All processing complete\".center(80, \"─\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APk0T30mQ5NN","executionInfo":{"status":"ok","timestamp":1748563299636,"user_tz":-120,"elapsed":95598,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"091d13b3-a2a4-4cb8-909d-489a76fa312e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","───────────────────────────────────📂 Class 01───────────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Class 01 | 🔑 Out_pt9_dm:  │████████████████████████████████████| 11124/11124"]},{"output_type":"stream","name":"stdout","text":["\n","───────────────────────────────────🔔 Summary────────────────────────────────────\n","Class 01  →  Out_pt9_dm\n","───────────────────────────✅ All processing complete────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"qx11kUkndrmA"},"source":["### **Train Model**"]},{"cell_type":"markdown","metadata":{"id":"bT_d0-U-gkpW"},"source":["### Split Data: %70: Train - %20 Validation - %10 Test"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1326,"status":"ok","timestamp":1748563311679,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"_MLHwzbAvO4m","outputId":"9cf42c72-8186-429e-f683-a50e7dfdf600"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Splitting RGB frames for class 01 ---\n","✅ Class 01: train=865, val=247, test=124\n","\n","All classes processed successfully.\n"]}],"source":["#Split RGB Files to Train Validate Test\n","import os\n","import glob\n","import yaml\n","import random\n","\n","# ==== CONFIGURATION ====\n","# Base directory containing class subfolders\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","# List of class identifiers to process\n","classes = [\"01\"]\n","# Fixed seed for reproducibility in splitting\n","random.seed(42)\n","\n","for cls in classes:\n","    print(f\"\\n--- Splitting RGB frames for class {cls} ---\")\n","\n","    # 1) Gather all RGB image indices (filenames without extension)\n","    rgb_dir = os.path.join(base_dir, cls, \"rgb\")\n","    rgb_paths = sorted(glob.glob(os.path.join(rgb_dir, \"*.png\")))\n","    rgb_idx = [os.path.splitext(os.path.basename(p))[0] for p in rgb_paths]\n","\n","    # 2) Load GT keys from gt.yml, zero-pad them to six digits\n","    gt_path = os.path.join(base_dir, cls, \"gt.yml\")\n","    if not os.path.isfile(gt_path):\n","        print(f\"WARNING: gt.yml not found for class {cls}, skipping.\")\n","        continue\n","\n","    with open(gt_path, \"r\") as f:\n","        gt = yaml.safe_load(f)\n","    gt_keys = {f\"{int(k):06d}\" for k in gt.keys()}\n","\n","    # 3) Keep only indices present in both RGB and GT\n","    valid_idx = [i for i in rgb_idx if i in gt_keys]\n","    if not valid_idx:\n","        print(f\"WARNING: No valid frames for class {cls}, skipping.\")\n","        continue\n","\n","    # 4) Shuffle and split 70% train / 20% val / 10% test\n","    random.shuffle(valid_idx)\n","    n = len(valid_idx)\n","    cut1 = int(0.7 * n)\n","    cut2 = int(0.9 * n)\n","    train_idx = valid_idx[:cut1]\n","    val_idx   = valid_idx[cut1:cut2]\n","    test_idx  = valid_idx[cut2:]\n","\n","    # 5) Write splits to text files under a new \"Split\" folder\n","    split_dir = os.path.join(base_dir, cls, \"Split\")\n","    os.makedirs(split_dir, exist_ok=True)\n","    with open(os.path.join(split_dir, \"train.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(train_idx))\n","    with open(os.path.join(split_dir, \"val.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(val_idx))\n","    with open(os.path.join(split_dir, \"test.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(test_idx))\n","\n","    print(f\"✅ Class {cls}: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n","\n","print(\"\\nAll classes processed successfully.\")\n"]},{"cell_type":"markdown","source":["### Normalization over Dataset"],"metadata":{"id":"Dptivg10sXLy"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import argparse\n","from tqdm import tqdm\n","\n","# ===================== Argument parser =====================\n","parser = argparse.ArgumentParser(\n","    description=\"Dataset sanitation & normalization (RGB, depth, mask, pose, radius maps) for LINEMOD-Preprocessed.\")\n","parser.add_argument(\"--dataset_path\", type=str, default=\"/content/dataset/linemod/Linemod_preprocessed/data\",\n","                    help=\"Root path to dataset (default: Colab location)\")\n","parser.add_argument(\"--classes\", type=str, nargs=\"*\", default=None,\n","                    help=\"Classes to process (e.g. 01 02 05). Default: all numeric folders in dataset_path\")\n","parser.add_argument(\"--depth_unit_scale\", type=float, default=1000.0,\n","                    help=\"Factor to convert depth/radius from millimetres to metres (default: 1000)\")\n","parser.add_argument(\"--radius_unit_scale\", type=float, default=1000.0,\n","                    help=\"Factor to convert radius map values from millimetres to metres. Set to 1 if already metres\")\n","parser.add_argument(\"--depth_max_m\", type=float, default=10.0,\n","                    help=\"Maximum valid depth in metres for clipping & scaling (default: 10.0)\")\n","args, _ = parser.parse_known_args()\n","\n","ROOT = args.dataset_path\n","\n","# --------------------- derive class list -------------------\n","if args.classes:\n","    CLASSES = args.classes\n","else:\n","    # all directories with two-digit names\n","    CLASSES = sorted([d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d)) and d.isdigit()])\n","\n","OUT_PTS   = [f\"Out_pt{i}_dm\" for i in range(1,10)]\n","MEAN_RGB  = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","STD_RGB   = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","DEPTH_MAX = args.depth_max_m\n","\n","def read_depth_dpt_auto(path: str) -> np.ndarray:\n","    \"\"\"Read .dpt that can be either uint16 (mm) or float32 (m) based on file size.\"\"\"\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        expected_u16 = h * w * 2\n","        expected_f32 = h * w * 4\n","        data_bytes   = f.seek(0, os.SEEK_END) or f.tell() - 8  # minus header\n","        f.seek(8)\n","        if data_bytes == expected_f32:\n","            data = np.fromfile(f, dtype=np.float32, count=h*w)\n","        else:\n","            data = np.fromfile(f, dtype=np.uint16, count=h*w).astype(np.float32)\n","    return data.reshape(h, w)\n","\n","def sep(char=\"─\"):\n","    print(char * 80)\n","\n","for cls in CLASSES:\n","    cls_dir = os.path.join(ROOT, cls)\n","    if not os.path.isdir(cls_dir):\n","        sep()\n","        print(f\"🚫 Class folder missing: {cls_dir}\")\n","        continue\n","\n","    sep()\n","    print(f\"📂  Class {cls}  \".center(80, \"─\"))\n","\n","    # ---------- 1) Normalize radius-maps ----------\n","    sep()\n","    print(\"🌐  Radius maps (Out_pt*_dm) normalization\")\n","    for out_dir in OUT_PTS:\n","        dir_path = os.path.join(cls_dir, out_dir)\n","        if not os.path.isdir(dir_path):\n","            continue\n","        files = [f for f in os.listdir(dir_path) if f.endswith(\".npy\")]\n","        for fn in tqdm(files, desc=f\"{out_dir}\", ncols=80):\n","            path = os.path.join(dir_path, fn)\n","            try:\n","                arr = np.load(path)\n","                if arr.size == 0:\n","                    raise ValueError(\"empty\")\n","                # اگر قبلاً مقیاس شده (حداکثر < 20 متر)، رد شو\n","                if arr.max() > 20.0:\n","                    arr = arr / args.radius_unit_scale  # mm → m\n","                    np.save(path, arr.astype(np.float32))\n","            except Exception as e:\n","                print(f\"   ⚠️  [SKIP] {fn} | {str(e)}\")\n","\n","    # ---------- 2) Check & normalise train/val/test splits ----------\n","    split_dir = os.path.join(cls_dir, \"Split\")\n","    for split_file in (\"train.txt\", \"val.txt\", \"test.txt\"):\n","        split_path = os.path.join(split_dir, split_file)\n","        if not os.path.isfile(split_path):\n","            print(f\"ℹ️  No {split_file} present\")\n","            continue\n","        ids = [int(line.strip()) for line in open(split_path) if line.strip().isdigit()]\n","        if not ids:\n","            continue\n","        sep()\n","        print(f\"🗂️  Split: {split_file:<10} │ {len(ids)} images\")\n","\n","        # ---- RGB check (mean/std) ----\n","        for idx in tqdm(ids, desc=\"RGB  \", ncols=80):\n","            p = os.path.join(cls_dir, \"rgb\", f\"{idx:06d}.png\")\n","            try:\n","                im = Image.open(p).convert(\"RGB\")\n","                _  = (np.asarray(im, np.float32) / 255.0 - MEAN_RGB) / STD_RGB\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP rgb {idx:06d}] │ {str(e)}\")\n","\n","        # ---- Depth normalisation (to 0-1) ----\n","        for idx in tqdm(ids, desc=\"Depth\", ncols=80):\n","            p = os.path.join(cls_dir, \"depth\", f\"{idx:06d}.dpt\")\n","            try:\n","                dp = read_depth_dpt_auto(p)\n","                if dp.max() > 20:  # likely mm\n","                    dp = dp / args.depth_unit_scale  # → metres\n","                dp = np.clip(dp, 0, DEPTH_MAX) / DEPTH_MAX  # 0-1 for inspection\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP depth {idx:06d}] │ {str(e)}\")\n","\n","        # ---- Mask check ----\n","        for idx in tqdm(ids, desc=\"Mask \", ncols=80):\n","            p = os.path.join(cls_dir, \"mask\", f\"{idx:06d}.png\")\n","            try:\n","                _ = np.asarray(Image.open(p).convert(\"L\"), np.uint8)\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP mask {idx:06d}] │ {str(e)}\")\n","\n","        # ---- Pose translation fix ----\n","        for idx in tqdm(ids, desc=\"Pose  \", ncols=80):\n","            p = os.path.join(cls_dir, \"pose\", f\"pose{idx:06d}.npy\")\n","            try:\n","                pose = np.load(p).astype(np.float32)\n","                t    = pose[:3,3]\n","                if np.linalg.norm(t) > 20:  # likely mm\n","                    pose[:3,3] = t / 1000.0  # mm → m\n","                    np.save(p, pose)\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP pose {idx:06d}] │ {str(e)}\")\n","\n","    sep()\n","    print(f\"✅ Done class {cls}\".center(80, \"=\"))\n","    sep()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FCouNlsqbwpJ","executionInfo":{"status":"ok","timestamp":1748563818821,"user_tz":-120,"elapsed":100470,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"fadbd951-7c38-4eaf-a88d-17979f72b354"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 01  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n"]},{"output_type":"stream","name":"stderr","text":["Out_pt1_dm: 100%|██████████████████████████| 1236/1236 [00:11<00:00, 104.53it/s]\n","Out_pt2_dm: 100%|██████████████████████████| 1236/1236 [00:10<00:00, 113.03it/s]\n","Out_pt3_dm: 100%|██████████████████████████| 1236/1236 [00:11<00:00, 108.46it/s]\n","Out_pt4_dm: 100%|██████████████████████████| 1236/1236 [00:10<00:00, 114.29it/s]\n","Out_pt5_dm: 100%|██████████████████████████| 1236/1236 [00:10<00:00, 119.67it/s]\n","Out_pt6_dm: 100%|██████████████████████████| 1236/1236 [00:04<00:00, 265.86it/s]\n","Out_pt7_dm: 100%|██████████████████████████| 1236/1236 [00:04<00:00, 257.00it/s]\n","Out_pt8_dm: 100%|██████████████████████████| 1236/1236 [00:03<00:00, 387.36it/s]\n","Out_pt9_dm: 100%|██████████████████████████| 1236/1236 [00:05<00:00, 241.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","🗂️  Split: train.txt  │ 865 images\n"]},{"output_type":"stream","name":"stderr","text":["RGB  : 100%|██████████████████████████████████| 865/865 [00:15<00:00, 55.71it/s]\n","Depth: 100%|████████████████████████████████| 865/865 [00:00<00:00, 1168.10it/s]\n","Mask : 100%|█████████████████████████████████| 865/865 [00:02<00:00, 310.73it/s]\n","Pose  : 100%|███████████████████████████████| 865/865 [00:00<00:00, 4077.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","🗂️  Split: val.txt    │ 247 images\n"]},{"output_type":"stream","name":"stderr","text":["RGB  : 100%|██████████████████████████████████| 247/247 [00:04<00:00, 58.50it/s]\n","Depth: 100%|████████████████████████████████| 247/247 [00:00<00:00, 1224.73it/s]\n","Mask : 100%|█████████████████████████████████| 247/247 [00:00<00:00, 311.98it/s]\n","Pose  : 100%|███████████████████████████████| 247/247 [00:00<00:00, 4282.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","🗂️  Split: test.txt   │ 124 images\n"]},{"output_type":"stream","name":"stderr","text":["RGB  : 100%|██████████████████████████████████| 124/124 [00:02<00:00, 58.04it/s]\n","Depth: 100%|████████████████████████████████| 124/124 [00:00<00:00, 1103.73it/s]\n","Mask : 100%|█████████████████████████████████| 124/124 [00:00<00:00, 307.21it/s]\n","Pose  : 100%|███████████████████████████████| 124/124 [00:00<00:00, 3653.63it/s]"]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 01=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 02  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 02=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 04  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 04=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 05  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 05=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 06  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 06=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 08  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 08=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 09  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 09=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 10  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 10=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 11  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 11=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 12  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 12=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 13  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 13=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 14  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 14=================================\n","────────────────────────────────────────────────────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","─────────────────────────────────📂  Class 15  ──────────────────────────────────\n","────────────────────────────────────────────────────────────────────────────────\n","🌐  Radius maps (Out_pt*_dm) normalization\n","ℹ️  No train.txt present\n","ℹ️  No val.txt present\n","ℹ️  No test.txt present\n","────────────────────────────────────────────────────────────────────────────────\n","================================✅ Done class 15=================================\n","────────────────────────────────────────────────────────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# **Train**"],"metadata":{"id":"2KsgE4ZxQxhJ"}},{"cell_type":"code","source":["!pip install colorama"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-sn2AhNkYBw","executionInfo":{"status":"ok","timestamp":1748563861179,"user_tz":-120,"elapsed":2519,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"9f6f42dc-6736-487d-9384-3145f6d032e1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.6\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Enhanced RCVPose: Deep Learning Model for 6D Pose Estimation\n","\n","This script is designed to train a 6D pose estimation model using RGB images, depth maps, masks, pose data, and radius maps (Outside9 points) for multiple objects. The code is fully documented in English and all data paths are explicitly set according to the provided folder structure.\n","\n","Folder Structure for Each Object (e.g., 01, 02, ...):\n","- Out_pt1_dm to Out_pt9_dm: Folders containing .npy files for each radius map\n","- depth: Contains .dpt files (depth images)\n","- mask: Contains .png files (object masks)\n","- pose: Contains pose files named as pose00000.npy, pose00001.npy, ...\n","- rgb: Contains .png files (RGB images)\n","- Split: Contains train.txt, val.txt, test.txt (lists of sample names)\n","- Outside9.npy: Reference points for the object\n","- gt.yml, info.yml: Metadata files\n","- mesh.ply: 3D mesh model\n","- test.txt, train.txt: Additional text files (if needed)\n","\n","All measurements are in millimeters.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import cv2\n","import os\n","from tqdm import tqdm\n","import logging\n","from datetime import datetime\n","from google.colab import drive\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.nn import functional as F\n","import time\n","from colorama import init, Fore, Style\n","from torch.cuda.amp import autocast, GradScaler\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","\n","# Initialize colorama for colored terminal output\n","init()\n","\n","# =========================\n","# Configuration Section\n","# =========================\n","CONFIG = {\n","    # Base directory containing all object folders (01, 02, ...)\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',\n","    # List of object IDs to train (edit this list to select which objects to train)\n","    'OBJECT_IDS': ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15'],\n","    # Subdirectory names inside each object folder\n","    'RGB_DIR': 'rgb',              # RGB images (.png)\n","    'DEPTH_DIR': 'depth',          # Depth maps (.dpt)\n","    'MASK_DIR': 'mask',            # Object masks (.png)\n","    'POSE_DIR': 'pose',            # Pose data (pose00000.npy, ...)\n","    'RADIUS_BASE_DIR': '.',        # Base dir for Out_pt*_dm folders (relative to object folder)\n","    'RADIUS_PREFIX': 'Out_pt',     # Prefix for radius map folders\n","    'RADIUS_SUFFIX': '_dm',        # Suffix for radius map folders\n","    'NUM_RADIUS_POINTS': 9,        # Number of radius map points\n","    'SPLIT_DIR': 'Split',          # Directory containing split files\n","    'TRAIN_SPLIT': 'train.txt',    # Training set filenames\n","    'VAL_SPLIT': 'val.txt',        # Validation set filenames\n","    'TEST_SPLIT': 'test.txt',      # Test set filenames\n","    # Training parameters\n","    'BATCH_SIZE': 8,               # Reduced batch size for memory efficiency\n","    'NUM_WORKERS': 2,              # Reduced number of workers\n","    'NUM_EPOCHS': 100,             # Number of training epochs\n","    'LEARNING_RATE': 0.001,        # Initial learning rate\n","    'ACCUMULATION_STEPS': 4,       # Number of mini-batches to accumulate gradients over\n","    # Model saving\n","    'MODELS_DIR': '/content/models',  # Directory to save trained models\n","}\n","\n","# =========================\n","# Logger for Training Output\n","# =========================\n","class TrainingLogger:\n","    \"\"\"\n","    Logger class for professional, colored, and well-separated output during training.\n","    \"\"\"\n","    def __init__(self):\n","        self.start_time = time.time()\n","        self.epoch_times = []\n","        self.separator = \"=\" * 100\n","        self.sub_separator = \"-\" * 100\n","    def log_section(self, message):\n","        print(f\"\\n{Fore.CYAN}{self.separator}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}🚀 {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{self.separator}{Style.RESET_ALL}\\n\")\n","    def log_subsection(self, message, no_lines=False):\n","        if no_lines:\n","            print(f\"{Fore.YELLOW}{message}{Style.RESET_ALL}\")\n","        else:\n","            print(f\"\\n{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\")\n","            print(f\"{Fore.YELLOW}📌 {message}{Style.RESET_ALL}\")\n","            print(f\"{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\\n\")\n","    def log_info(self, message):\n","        print(f\"{Fore.GREEN}ℹ️  {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.GREEN}{'.' * 50}{Style.RESET_ALL}\")\n","    def log_warning(self, message):\n","        print(f\"\\n{Fore.YELLOW}⚠️  {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.YELLOW}{'!' * 50}{Style.RESET_ALL}\\n\")\n","    def log_error(self, message):\n","        print(f\"\\n{Fore.RED}❌ {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.RED}{'#' * 50}{Style.RESET_ALL}\\n\")\n","    def log_success(self, message):\n","        print(f\"\\n{Fore.GREEN}✅ {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.GREEN}{'*' * 50}{Style.RESET_ALL}\\n\")\n","    def log_metrics(self, metrics, phase=\"Training\", no_lines=False):\n","        if no_lines:\n","            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}📊 {phase} Metrics:{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'-' * 50}{Style.RESET_ALL}\")\n","            for key, value in metrics.items():\n","                print(f\"{Fore.MAGENTA}   {key}: {value:.4f}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\\n\")\n","        else:\n","            print(f\"\\n{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}📊 {phase} Metrics:{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'-' * 50}{Style.RESET_ALL}\")\n","            for key, value in metrics.items():\n","                print(f\"{Fore.MAGENTA}   {key}: {value:.4f}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\\n\")\n","    def log_model_save(self, path):\n","        print(f\"\\n{Fore.BLUE}{'=' * 50}{Style.RESET_ALL}\")\n","        print(f\"{Fore.BLUE}💾 Model saved at: {path}{Style.RESET_ALL}\")\n","        print(f\"{Fore.BLUE}{'=' * 50}{Style.RESET_ALL}\\n\")\n","    def log_time(self, epoch, total_epochs):\n","        elapsed = time.time() - self.start_time\n","        avg_epoch_time = elapsed / (epoch + 1)\n","        remaining_time = avg_epoch_time * (total_epochs - epoch - 1)\n","        print(f\"\\n{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}⏱️  Time Information:{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{'-' * 50}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Elapsed Time: {self.format_time(elapsed)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Average Epoch Time: {self.format_time(avg_epoch_time)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Estimated Remaining Time: {self.format_time(remaining_time)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\\n\")\n","    def log_epoch_start(self, epoch, total_epochs):\n","        print(f\"\\n{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}🔄 Starting Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\\n\")\n","    def log_epoch_end(self, epoch, total_epochs, metrics):\n","        print(f\"\\n{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}✅ Completed Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n","        self.log_metrics(metrics, f\"Epoch {epoch + 1} Summary\")\n","        print(f\"{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\\n\")\n","    @staticmethod\n","    def format_time(seconds):\n","        hours = int(seconds // 3600)\n","        minutes = int((seconds % 3600) // 60)\n","        seconds = int(seconds % 60)\n","        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n","\n","def safe_read_depth(path):\n","    \"\"\"\n","    Safely read a depth image. If the file is .dpt, try to read as binary; otherwise, use OpenCV.\n","    Returns a numpy array or raises a FileNotFoundError if not found or unreadable.\n","    \"\"\"\n","    if not os.path.exists(path):\n","        raise FileNotFoundError(f\"Depth image not found: {path}\")\n","    if path.endswith('.dpt'):\n","        try:\n","            with open(path, 'rb') as f:\n","                h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","                data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","                if data.size != h*w:\n","                    raise ValueError(f\"Depth data size mismatch in file: {path}\")\n","                return data.reshape(h, w).astype(np.float32)\n","        except Exception as e:\n","            raise IOError(f\"Failed to read .dpt file: {path}. Error: {e}\")\n","    # For other formats, use OpenCV\n","    depth_img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n","    if depth_img is None:\n","        raise IOError(f\"Failed to read depth image: {path}\")\n","    return depth_img.astype(np.float32)\n","\n","# =========================\n","# Custom Dataset Definition\n","# =========================\n","class CustomDataset(Dataset):\n","    \"\"\"\n","    Custom PyTorch Dataset for loading RGB, depth, mask, pose, and radius map data for a single object.\n","    Each sample consists of:\n","      - RGB image (.png)\n","      - Depth map (.dpt)\n","      - Mask (.png)\n","      - Pose (pose{base_name}.npy)\n","      - Radius maps (9 .npy files, one from each Out_pt*_dm folder)\n","    All measurements are in millimeters.\n","    \"\"\"\n","    def __init__(self, rgb_dir, depth_dir, mask_dir, pose_dir, radius_base_dir,\n","                 transform_rgb=None, transform_depth=None, transform_mask=None,\n","                 augmentation=None):\n","        \"\"\"\n","        Added `augmentation` callable to allow on-the-fly data augmentation (e.g. cut-out/rotation).\n","        \"\"\"\n","        self.rgb_dir = rgb_dir\n","        self.depth_dir = depth_dir\n","        self.mask_dir = mask_dir\n","        self.pose_dir = pose_dir\n","        self.radius_base_dir = radius_base_dir\n","        self.transform_rgb = transform_rgb\n","        self.transform_depth = transform_depth\n","        self.transform_mask = transform_mask\n","        self.augmentation = augmentation\n","        self.filenames = sorted([f for f in os.listdir(rgb_dir) if f.endswith('.png')])\n","    def __len__(self):\n","        return len(self.filenames)\n","    def __getitem__(self, idx):\n","        filename = self.filenames[idx]\n","        base_name = filename.split('.')[0]  # e.g., '000000'\n","        # Load RGB image (H, W, 3) in uint8\n","        rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","        rgb_img = cv2.imread(rgb_path)\n","        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","        rgb_img = Image.fromarray(rgb_img)  # Convert to PIL.Image\n","        # Load depth map safely (handles .dpt and other formats)\n","        depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","        depth_img = safe_read_depth(depth_path).astype(np.float32)  # (H, W) in float32 (mm)\n","        depth_img = depth_img / 1000.0  # scale to metres for numerical stability\n","        depth_img = np.expand_dims(depth_img, axis=2)  # (H, W, 1)\n","        # Keep full float precision – use PIL 'F' mode which supports 32-bit float\n","        depth_img = Image.fromarray(depth_img.squeeze(), mode='F')\n","        # Load mask (H, W) in uint8\n","        mask_path = os.path.join(self.mask_dir, f'{base_name}.png')\n","        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","        mask = np.expand_dims(mask.astype(np.float32) / 255.0, axis=2)  # Normalize mask to [0, 1] and add channel\n","        mask = (mask * 255).astype(np.uint8)\n","        mask = Image.fromarray(mask.squeeze(), mode='L')\n","        # Load pose (with 'pose' prefix)\n","        pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","        pose = np.load(pose_path)\n","        # Convert pose to (7,) vector if needed\n","        if pose.shape == (3, 4):\n","            rot = R.from_matrix(pose[:, :3]).as_quat()  # (x, y, z, w)\n","            trans = pose[:, 3]\n","            pose_vec = np.concatenate([trans, rot])  # (3,) + (4,) = (7,)\n","            pose = pose_vec\n","        elif pose.shape == (4, 4):\n","            rot = R.from_matrix(pose[:3, :3]).as_quat()\n","            trans = pose[:3, 3]\n","            pose_vec = np.concatenate([trans, rot])\n","            pose = pose_vec\n","        # If pose is already (7,), do nothing\n","        # Load radius maps for all 9 points\n","        radius_maps = []\n","        for pt_idx in range(1, CONFIG['NUM_RADIUS_POINTS'] + 1):\n","            radius_folder = f\"Out_pt{pt_idx}_dm\"\n","            radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","            radius_map = np.load(radius_path)\n","            # --- FULL RESOLUTION: Do NOT resize radius maps ---\n","            # radius_map = cv2.resize(radius_map, (256, 256), interpolation=cv2.INTER_LINEAR)  # <-- REMOVE or COMMENT OUT\n","            radius_maps.append(radius_map)\n","        radius_maps = np.array(radius_maps)\n","        # ---------- Optional Augmentations ----------\n","        if self.augmentation is not None:\n","            rgb_img, depth_img, mask = self.augmentation(rgb_img, depth_img, mask)\n","\n","        # ---------- Apply separate transforms ----------\n","        if self.transform_rgb:\n","            rgb_img = self.transform_rgb(rgb_img)\n","        if self.transform_depth:\n","            depth_img = self.transform_depth(depth_img)\n","        if self.transform_mask:\n","            mask = self.transform_mask(mask)\n","        return {\n","            'rgb': rgb_img.float(),\n","            'depth': depth_img.float(),\n","            'mask': mask.float(),\n","            'pose': torch.FloatTensor(pose),\n","            'radius_maps': torch.FloatTensor(radius_maps)\n","        }\n","\n","class AdvancedAugmentation:\n","    \"\"\"\n","    Applies various transformations to input images to improve model robustness.\n","    This helps the model learn better by seeing different variations of the same image.\n","    \"\"\"\n","    def __init__(self):\n","        # Set up color transformations\n","        self.color_jitter = transforms.ColorJitter(\n","            brightness=0.2,  # Random brightness adjustment\n","            contrast=0.2,    # Random contrast adjustment\n","            saturation=0.2,  # Random saturation adjustment\n","            hue=0.1         # Random hue adjustment\n","        )\n","\n","    def __call__(self, rgb, depth, mask):\n","        # Apply color transformations to RGB image\n","        rgb = self.color_jitter(rgb)\n","\n","        # Random rotation between -10 and 10 degrees (expand=False keeps size)\n","        angle = np.random.uniform(-10, 10)\n","        rgb = transforms.functional.rotate(rgb, angle, expand=False)\n","        depth = transforms.functional.rotate(depth, angle, expand=False)\n","        mask = transforms.functional.rotate(mask, angle, expand=False)\n","\n","        # NOTE: Removed random scaling to avoid variable output sizes that break DataLoader collation\n","\n","        # Add small random Gaussian noise (1% of depth value range)\n","        depth_np = np.array(depth).astype(np.float32)\n","        noise = np.random.normal(scale=0.01, size=depth_np.shape).astype(np.float32)\n","        depth_np = depth_np + noise\n","        depth = Image.fromarray(depth_np, mode='F')\n","\n","        return rgb, depth, mask\n","\n","class AttentionModule(nn.Module):\n","    \"\"\"\n","    Attention mechanism to focus on important parts of the image.\n","    This helps the model pay more attention to relevant features.\n","    \"\"\"\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        # Create query, key, and value projections\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))  # Learnable parameter\n","\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","\n","        # Generate attention map\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","\n","        # Calculate attention scores\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","\n","        # Apply attention to values\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    \"\"\"\n","    Feature Pyramid Network for multi-scale feature extraction.\n","    Accepts a list of input channels for each feature map.\n","    \"\"\"\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        # Create lateral connections for each input channel\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        # Create output convolutions\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","    def forward(self, features):\n","        # features should be a list of feature maps from different layers\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        # Top-down pathway\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        # Final convolutions\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    \"\"\"\n","    Main model for 6D pose estimation.\n","    Combines RGB and depth information to predict object pose.\n","    Uses FPN with correct feature extraction from ResNet.\n","    Now uses ResNet50 (original, higher memory usage).\n","    \"\"\"\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        # Use ResNet50 as backbone (original)\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])   # Output: 256\n","        self.rgb_layer2 = list(resnet.children())[5]                    # Output: 512\n","        self.rgb_layer3 = list(resnet.children())[6]                    # Output: 1024\n","        self.rgb_layer4 = list(resnet.children())[7]                    # Output: 2048\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        # Initialise depth conv1 with mean of RGB weights for better convergence\n","        with torch.no_grad():\n","            resnet_depth.conv1.weight.copy_(resnet.conv1.weight.mean(dim=1, keepdim=True))\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","        # Fusion block\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        # Pose head (global pooling + FC)\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","        # Outside9 head: output 9 full-resolution maps\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)  # 9 channel output\n","        )\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)  # [B, 256, H, W]\n","        # Pose: global pooling + FC\n","        pooled = self.global_pool(fused)  # [B, 256, 1, 1]\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","        # Outside9: full maps\n","        outside9 = self.outside9_head(fused)  # [B, 9, h, w]\n","        # --- ALWAYS UPSAMPLE to input image size (full resolution) ---\n","        target_size = (rgb.shape[2], rgb.shape[3])  # (H, W)\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","        return pose, outside9\n","\n","class GeometricLoss(nn.Module):\n","    \"\"\"\n","    Loss function that combines translation, rotation, and point correspondence errors.\n","    All measurements are in millimeters.\n","    \"\"\"\n","    def __init__(self):\n","        super(GeometricLoss, self).__init__()\n","\n","    def forward(self, pred_pose, target_pose, pred_outside9, target_outside9):\n","        # Translation (MSE)\n","        trans_loss = F.mse_loss(pred_pose[:, :3], target_pose[:, :3])\n","\n","        # --- Rotation: geodesic distance between unit quaternions ---\n","        pred_rot = F.normalize(pred_pose[:, 3:], dim=1)\n","        target_rot = F.normalize(target_pose[:, 3:], dim=1)\n","        dot = torch.sum(pred_rot * target_rot, dim=1).clamp(-1.0 + 1e-7, 1.0 - 1e-7)\n","        rot_error = 2.0 * torch.acos(torch.abs(dot))  # radians\n","        rot_loss = rot_error.mean()\n","\n","        # Points loss\n","        points_loss = F.mse_loss(pred_outside9, target_outside9)\n","\n","        # Weighted total loss (tunable hyper-parameters)\n","        total_loss = trans_loss + 0.5 * rot_loss + 0.3 * points_loss\n","        return total_loss, trans_loss, rot_loss, points_loss\n","\n","def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n","    \"\"\"\n","    Train the model with advanced features and professional logging.\n","    Uses gradient accumulation and mixed precision training for memory efficiency.\n","    \"\"\"\n","    logger = TrainingLogger()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    logger.log_section(\"Initializing Training\")\n","    logger.log_info(f\"Using device: {device}\")\n","    model = model.to(device)\n","    os.makedirs(CONFIG['MODELS_DIR'], exist_ok=True)\n","    logger.log_success(f\"Created models directory at: {CONFIG['MODELS_DIR']}\")\n","\n","    criterion = GeometricLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n","    scheduler = optim.lr_scheduler.OneCycleLR(\n","        optimizer,\n","        max_lr=learning_rate,\n","        epochs=num_epochs,\n","        steps_per_epoch=len(train_loader)\n","    )\n","\n","    # Initialize gradient scaler for mixed precision training\n","    scaler = GradScaler()\n","\n","    best_val_loss = float('inf')\n","    accumulation_steps = CONFIG.get('ACCUMULATION_STEPS', 4)\n","    logger.log_section(\"Starting Training Loop\")\n","\n","    for epoch in range(num_epochs):\n","        torch.cuda.empty_cache()  # Free up GPU memory at the start of each epoch\n","        epoch_start_time = time.time()\n","        # #### EPOCH HEADER ####\n","        print(f\"\\n{'#'*12} Epoch {epoch+1}/{num_epochs} {'#'*12}\")\n","        logger.log_epoch_start(epoch, num_epochs)\n","        model.train()\n","        train_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n","        # Training Phase (no line separators)\n","        logger.log_subsection(\"Training Phase\", no_lines=True)\n","        optimizer.zero_grad()\n","        for i, batch in enumerate(tqdm(train_loader, desc=f'🔄 Training')):\n","            rgb = batch['rgb'].to(device)\n","            depth = batch['depth'].to(device)\n","            pose_target = batch['pose'].to(device)\n","            outside9_target = batch['radius_maps'].to(device)\n","            with autocast():\n","                pose_pred, outside9_pred = model(rgb, depth)\n","                total_loss, trans_loss, rot_loss, points_loss = criterion(\n","                    pose_pred, pose_target, outside9_pred, outside9_target\n","                )\n","            scaler.scale(total_loss / accumulation_steps).backward()\n","            if (i + 1) % accumulation_steps == 0:\n","                # Unscale before clipping & optimiser step\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                scheduler.step()\n","            train_losses['total'] += total_loss.item()\n","            train_losses['trans'] += trans_loss.item()\n","            train_losses['rot'] += rot_loss.item()\n","            train_losses['points'] += points_loss.item()\n","        # Final optimiser step for leftover minibatches < accumulation_steps\n","        if (i + 1) % accumulation_steps != 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","        for k in train_losses:\n","            train_losses[k] /= len(train_loader)\n","        logger.log_metrics(train_losses, \"Training\", no_lines=True)\n","        # ----\n","        print(\"----\")\n","        model.eval()\n","        val_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n","        # Validation Phase (no line separators)\n","        logger.log_subsection(\"Validation Phase\", no_lines=True)\n","        with torch.no_grad():\n","            for batch in tqdm(val_loader, desc=f'🔍 Validation'):\n","                rgb = batch['rgb'].to(device)\n","                depth = batch['depth'].to(device)\n","                pose_target = batch['pose'].to(device)\n","                outside9_target = batch['radius_maps'].to(device)\n","                with autocast():\n","                    pose_pred, outside9_pred = model(rgb, depth)\n","                    total_loss, trans_loss, rot_loss, points_loss = criterion(\n","                        pose_pred, pose_target, outside9_pred, outside9_target\n","                    )\n","                val_losses['total'] += total_loss.item()\n","                val_losses['trans'] += trans_loss.item()\n","                val_losses['rot'] += rot_loss.item()\n","                val_losses['points'] += points_loss.item()\n","        for k in val_losses:\n","            val_losses[k] /= len(val_loader)\n","        logger.log_metrics(val_losses, \"Validation\", no_lines=True)\n","        # #### EPOCH FOOTER ####\n","        print(f\"{'#'*12} End Epoch {epoch+1}/{num_epochs} {'#'*12}\\n\")\n","\n","        if val_losses['total'] < best_val_loss:\n","            best_val_loss = val_losses['total']\n","            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","            save_path = os.path.join(CONFIG['MODELS_DIR'], f'best_model_{timestamp}.pth')\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': val_losses['total'],\n","                'val_metrics': val_losses,\n","                'config': CONFIG\n","            }, save_path)\n","            logger.log_success(f\"New best model saved with validation loss: {val_losses['total']:.4f}\")\n","            logger.log_model_save(save_path)\n","\n","            # Save backup to Google Drive\n","            drive_save_path = os.path.join('/content/drive/MyDrive/models', f'best_model_{timestamp}.pth')\n","            os.makedirs(os.path.dirname(drive_save_path), exist_ok=True)\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': val_losses['total'],\n","                'val_metrics': val_losses,\n","                'config': CONFIG\n","            }, drive_save_path)\n","            logger.log_success(f\"Backup copy saved to Google Drive\")\n","            logger.log_model_save(drive_save_path)\n","\n","        logger.log_time(epoch, num_epochs)\n","        logger.log_epoch_end(epoch, num_epochs, {\n","            'train_loss': train_losses['total'],\n","            'val_loss': val_losses['total']\n","        })\n","\n","    logger.log_section(\"Training Completed\")\n","    logger.log_success(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n","\n","def load_split_file(split_file):\n","    \"\"\"\n","    Load filenames from a split file.\n","\n","    Args:\n","        split_file (str): Path to the split file (Train.txt, Test.txt, or Validation.txt)\n","\n","    Returns:\n","        list: List of filenames (without extension)\n","    \"\"\"\n","    with open(split_file, 'r') as f:\n","        filenames = [line.strip() for line in f.readlines()]\n","    return filenames\n","\n","def main():\n","    logger = TrainingLogger()\n","    logger.log_section(\"Starting RCVPose Training for Selected Objects\")\n","\n","    # Use the list of object IDs specified in CONFIG['OBJECT_IDS']\n","    object_ids = CONFIG['OBJECT_IDS']\n","    base_data_dir = CONFIG['BASE_DIR']\n","\n","    for object_id in object_ids:\n","        logger.log_section(f\"Training for Object {object_id}\")\n","        object_dir = os.path.join(base_data_dir, object_id)\n","\n","        rgb_dir = os.path.join(object_dir, CONFIG['RGB_DIR'])\n","        depth_dir = os.path.join(object_dir, CONFIG['DEPTH_DIR'])\n","        mask_dir = os.path.join(object_dir, CONFIG['MASK_DIR'])\n","        pose_dir = os.path.join(object_dir, CONFIG['POSE_DIR'])\n","        radius_dir = os.path.join(object_dir, CONFIG['RADIUS_BASE_DIR'])\n","        split_dir = os.path.join(object_dir, CONFIG['SPLIT_DIR'])\n","\n","        logger.log_info(\"Loading split files\")\n","        # Load split files\n","        train_filenames = load_split_file(os.path.join(split_dir, CONFIG['TRAIN_SPLIT']))\n","        val_filenames = load_split_file(os.path.join(split_dir, CONFIG['VAL_SPLIT']))\n","        test_filenames = load_split_file(os.path.join(split_dir, CONFIG['TEST_SPLIT']))\n","\n","        logger.log_success(f\"Loaded {len(train_filenames)} training samples for Object {object_id}\")\n","        logger.log_success(f\"Loaded {len(val_filenames)} validation samples for Object {object_id}\")\n","        logger.log_success(f\"Loaded {len(test_filenames)} test samples for Object {object_id}\")\n","\n","        logger.log_info(\"Setting up data transforms and augmentation\")\n","        # Define transforms and augmentation WITHOUT resizing (full resolution)\n","        transform_rgb = transforms.Compose([\n","            # No resizing! Use full resolution\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        transform_depth = transforms.Lambda(lambda img: torch.from_numpy(np.array(img, dtype=np.float32)).unsqueeze(0))\n","        transform_mask = transforms.Compose([\n","            # No resizing! Use full resolution\n","            transforms.ToTensor()\n","        ])\n","        augmentation = AdvancedAugmentation()\n","\n","        logger.log_info(\"Creating datasets\")\n","        # Create datasets\n","        full_dataset = CustomDataset(rgb_dir, depth_dir, mask_dir, pose_dir, radius_dir,\n","                                   transform_rgb=transform_rgb,\n","                                   transform_depth=transform_depth,\n","                                   transform_mask=transform_mask,\n","                                   augmentation=augmentation)\n","\n","        # Restore original train/val split\n","        train_dataset = torch.utils.data.Subset(full_dataset,\n","            [i for i, f in enumerate(full_dataset.filenames) if f.split('.')[0] in train_filenames])\n","        val_dataset = torch.utils.data.Subset(full_dataset,\n","            [i for i, f in enumerate(full_dataset.filenames) if f.split('.')[0] in val_filenames])\n","\n","        logger.log_info(\"Creating data loaders\")\n","        # Create data loaders with optimized settings\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=CONFIG['BATCH_SIZE'],\n","            shuffle=True,\n","            num_workers=CONFIG['NUM_WORKERS'],\n","            pin_memory=True,\n","            persistent_workers=True,\n","            prefetch_factor=2\n","        )\n","        val_loader = DataLoader(\n","            val_dataset,\n","            batch_size=CONFIG['BATCH_SIZE'],\n","            shuffle=False,\n","            num_workers=CONFIG['NUM_WORKERS'],\n","            pin_memory=True,\n","            persistent_workers=True,\n","            prefetch_factor=2\n","        )\n","\n","        logger.log_info(\"Initializing model\")\n","        # Initialize model with ResNet18 backbone\n","        model = EnhancedRCVPose()\n","\n","        # Freeze BatchNorm layers to mitigate small-batch issues\n","        for m in model.modules():\n","            if isinstance(m, nn.BatchNorm2d):\n","                m.eval()\n","                for p in m.parameters():\n","                    p.requires_grad = False\n","\n","        logger.log_section(f\"Starting Training Process for Object {object_id}\")\n","        # Start training\n","        train_model(model, train_loader, val_loader,\n","                    num_epochs=CONFIG['NUM_EPOCHS'],\n","                    learning_rate=CONFIG['LEARNING_RATE'])\n","\n","        logger.log_section(f\"Training Process Completed for Object {object_id}\")\n","\n","    logger.log_section(\"All Object Trainings Completed\")\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BPjhiytw3HkQ","executionInfo":{"status":"error","timestamp":1748568251427,"user_tz":-120,"elapsed":4146826,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"4af37c61-993e-413f-efe9-9fdd75a3f44f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","====================================================================================================\n","🚀 Starting RCVPose Training for Selected Objects\n","====================================================================================================\n","\n","\n","====================================================================================================\n","🚀 Training for Object 01\n","====================================================================================================\n","\n","ℹ️  Loading split files\n","..................................................\n","\n","✅ Loaded 865 training samples for Object 01\n","**************************************************\n","\n","\n","✅ Loaded 247 validation samples for Object 01\n","**************************************************\n","\n","\n","✅ Loaded 124 test samples for Object 01\n","**************************************************\n","\n","ℹ️  Setting up data transforms and augmentation\n","..................................................\n","ℹ️  Creating datasets\n","..................................................\n","ℹ️  Creating data loaders\n","..................................................\n","ℹ️  Initializing model\n","..................................................\n","\n","====================================================================================================\n","🚀 Starting Training Process for Object 01\n","====================================================================================================\n","\n","\n","====================================================================================================\n","🚀 Initializing Training\n","====================================================================================================\n","\n","ℹ️  Using device: cuda\n","..................................................\n","\n","✅ Created models directory at: /content/models\n","**************************************************\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-8d5da6f69726>:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["\n","====================================================================================================\n","🚀 Starting Training Loop\n","====================================================================================================\n","\n","\n","############ Epoch 1/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 1/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["\r🔄 Training:   0%|          | 0/109 [00:00<?, ?it/s]<ipython-input-22-8d5da6f69726>:510: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.9721\n","   trans: 0.1647\n","   rot: 1.6135\n","   points: 0.0023\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["\r🔍 Validation:   0%|          | 0/31 [00:00<?, ?it/s]<ipython-input-22-8d5da6f69726>:552: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.7283\n","   trans: 0.0388\n","   rot: 1.3754\n","   points: 0.0062\n","==================================================\n","\n","############ End Epoch 1/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.7283\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_001621.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_001621.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:01:18\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:10:08\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 1/100\n","\n","==================================================\n","📊 Epoch 1 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.9721\n","   val_loss: 0.7283\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 2/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 2/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:03<00:00,  1.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.6599\n","   trans: 0.0561\n","   rot: 1.2064\n","   points: 0.0020\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.4419\n","   trans: 0.0300\n","   rot: 0.8227\n","   points: 0.0019\n","==================================================\n","\n","############ End Epoch 2/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.4419\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_001740.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_001740.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:02:36\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:07:59\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 2/100\n","\n","==================================================\n","📊 Epoch 2 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.6599\n","   val_loss: 0.4419\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 3/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 3/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.5122\n","   trans: 0.0445\n","   rot: 0.9348\n","   points: 0.0011\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.3388\n","   trans: 0.0130\n","   rot: 0.6510\n","   points: 0.0011\n","==================================================\n","\n","############ End Epoch 3/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.3388\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_001858.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_001858.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:03:54\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:06:30\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 3/100\n","\n","==================================================\n","📊 Epoch 3 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.5122\n","   val_loss: 0.3388\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 4/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 4/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.4470\n","   trans: 0.0403\n","   rot: 0.8129\n","   points: 0.0008\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.2904\n","   trans: 0.0123\n","   rot: 0.5558\n","   points: 0.0008\n","==================================================\n","\n","############ End Epoch 4/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.2904\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_002017.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_002017.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:05:13\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:05:17\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 4/100\n","\n","==================================================\n","📊 Epoch 4 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.4470\n","   val_loss: 0.2904\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 5/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 5/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.4166\n","   trans: 0.0387\n","   rot: 0.7554\n","   points: 0.0008\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.2635\n","   trans: 0.0078\n","   rot: 0.5109\n","   points: 0.0007\n","==================================================\n","\n","############ End Epoch 5/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.2635\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_002135.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_002135.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:06:31\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:04:01\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 5/100\n","\n","==================================================\n","📊 Epoch 5 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.4166\n","   val_loss: 0.2635\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 6/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 6/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:05<00:00,  1.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.3860\n","   trans: 0.0372\n","   rot: 0.6972\n","   points: 0.0007\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.2402\n","   trans: 0.0095\n","   rot: 0.4609\n","   points: 0.0007\n","==================================================\n","\n","############ End Epoch 6/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.2402\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_002254.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_002254.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:07:52\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:03:18\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 6/100\n","\n","==================================================\n","📊 Epoch 6 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.3860\n","   val_loss: 0.2402\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 7/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 7/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:05<00:00,  1.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.3520\n","   trans: 0.0333\n","   rot: 0.6369\n","   points: 0.0007\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.2221\n","   trans: 0.0152\n","   rot: 0.4134\n","   points: 0.0007\n","==================================================\n","\n","############ End Epoch 7/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.2221\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_002415.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_002415.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:09:11\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:02:00\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 7/100\n","\n","==================================================\n","📊 Epoch 7 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.3520\n","   val_loss: 0.2221\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 8/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 8/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:05<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.3279\n","   trans: 0.0318\n","   rot: 0.5919\n","   points: 0.0007\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.2152\n","   trans: 0.0119\n","   rot: 0.4062\n","   points: 0.0008\n","==================================================\n","\n","############ End Epoch 8/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.2152\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_002533.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_002533.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:10:30\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 02:00:47\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 8/100\n","\n","==================================================\n","📊 Epoch 8 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.3279\n","   val_loss: 0.2152\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 9/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 9/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:05<00:00,  1.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.3489\n","   trans: 0.0363\n","   rot: 0.6249\n","   points: 0.0008\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.2452\n","   trans: 0.0115\n","   rot: 0.4667\n","   points: 0.0013\n","==================================================\n","\n","############ End Epoch 9/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:11:46\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:59:02\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 9/100\n","\n","==================================================\n","📊 Epoch 9 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.3489\n","   val_loss: 0.2452\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 10/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 10/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.3251\n","   trans: 0.0360\n","   rot: 0.5778\n","   points: 0.0008\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1712\n","   trans: 0.0082\n","   rot: 0.3256\n","   points: 0.0008\n","==================================================\n","\n","############ End Epoch 10/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1712\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_002808.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_002808.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:13:05\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:57:52\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 10/100\n","\n","==================================================\n","📊 Epoch 10 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.3251\n","   val_loss: 0.1712\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 11/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 11/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.3006\n","   trans: 0.0367\n","   rot: 0.5273\n","   points: 0.0007\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1950\n","   trans: 0.0196\n","   rot: 0.3505\n","   points: 0.0007\n","==================================================\n","\n","############ End Epoch 11/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:14:22\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:56:15\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 11/100\n","\n","==================================================\n","📊 Epoch 11 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.3006\n","   val_loss: 0.1950\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 12/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 12/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2855\n","   trans: 0.0355\n","   rot: 0.4996\n","   points: 0.0007\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1664\n","   trans: 0.0112\n","   rot: 0.3102\n","   points: 0.0006\n","==================================================\n","\n","############ End Epoch 12/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1664\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_003044.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_003044.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:15:41\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:55:05\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 12/100\n","\n","==================================================\n","📊 Epoch 12 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2855\n","   val_loss: 0.1664\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 13/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 13/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2717\n","   trans: 0.0329\n","   rot: 0.4773\n","   points: 0.0005\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1543\n","   trans: 0.0101\n","   rot: 0.2882\n","   points: 0.0005\n","==================================================\n","\n","############ End Epoch 13/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1543\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_003204.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_003204.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:17:00\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:53:51\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 13/100\n","\n","==================================================\n","📊 Epoch 13 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2717\n","   val_loss: 0.1543\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 14/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 14/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2589\n","   trans: 0.0320\n","   rot: 0.4537\n","   points: 0.0005\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1407\n","   trans: 0.0110\n","   rot: 0.2591\n","   points: 0.0005\n","==================================================\n","\n","############ End Epoch 14/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1407\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_003323.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_003323.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:18:19\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:52:33\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 14/100\n","\n","==================================================\n","📊 Epoch 14 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2589\n","   val_loss: 0.1407\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 15/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 15/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2491\n","   trans: 0.0314\n","   rot: 0.4351\n","   points: 0.0005\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1495\n","   trans: 0.0083\n","   rot: 0.2821\n","   points: 0.0006\n","==================================================\n","\n","############ End Epoch 15/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:19:35\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:51:01\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 15/100\n","\n","==================================================\n","📊 Epoch 15 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2491\n","   val_loss: 0.1495\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 16/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 16/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2603\n","   trans: 0.0314\n","   rot: 0.4575\n","   points: 0.0005\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1548\n","   trans: 0.0081\n","   rot: 0.2931\n","   points: 0.0005\n","==================================================\n","\n","############ End Epoch 16/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:20:51\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:49:31\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 16/100\n","\n","==================================================\n","📊 Epoch 16 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2603\n","   val_loss: 0.1548\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 17/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 17/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2514\n","   trans: 0.0308\n","   rot: 0.4409\n","   points: 0.0005\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1405\n","   trans: 0.0133\n","   rot: 0.2540\n","   points: 0.0006\n","==================================================\n","\n","############ End Epoch 17/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1405\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_003714.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_003714.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:22:10\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:48:14\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 17/100\n","\n","==================================================\n","📊 Epoch 17 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2514\n","   val_loss: 0.1405\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 18/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 18/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2429\n","   trans: 0.0287\n","   rot: 0.4280\n","   points: 0.0005\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1384\n","   trans: 0.0143\n","   rot: 0.2478\n","   points: 0.0005\n","==================================================\n","\n","############ End Epoch 18/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1384\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_003832.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_003832.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:23:30\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:47:05\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 18/100\n","\n","==================================================\n","📊 Epoch 18 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2429\n","   val_loss: 0.1384\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 19/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 19/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2392\n","   trans: 0.0324\n","   rot: 0.4134\n","   points: 0.0005\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1227\n","   trans: 0.0084\n","   rot: 0.2283\n","   points: 0.0004\n","==================================================\n","\n","############ End Epoch 19/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1227\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_003952.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_003952.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:24:49\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:45:48\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 19/100\n","\n","==================================================\n","📊 Epoch 19 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2392\n","   val_loss: 0.1227\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 20/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 20/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2233\n","   trans: 0.0296\n","   rot: 0.3871\n","   points: 0.0004\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1098\n","   trans: 0.0088\n","   rot: 0.2019\n","   points: 0.0004\n","==================================================\n","\n","############ End Epoch 20/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1098\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_004111.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_004111.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:26:07\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:44:29\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 20/100\n","\n","==================================================\n","📊 Epoch 20 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2233\n","   val_loss: 0.1098\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 21/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 21/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2150\n","   trans: 0.0264\n","   rot: 0.3771\n","   points: 0.0004\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1142\n","   trans: 0.0077\n","   rot: 0.2128\n","   points: 0.0004\n","==================================================\n","\n","############ End Epoch 21/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:27:23\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:43:01\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 21/100\n","\n","==================================================\n","📊 Epoch 21 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2150\n","   val_loss: 0.1142\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 22/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 22/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2210\n","   trans: 0.0282\n","   rot: 0.3854\n","   points: 0.0004\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1230\n","   trans: 0.0065\n","   rot: 0.2327\n","   points: 0.0004\n","==================================================\n","\n","############ End Epoch 22/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:28:38\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:41:34\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 22/100\n","\n","==================================================\n","📊 Epoch 22 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2210\n","   val_loss: 0.1230\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 23/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 23/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2091\n","   trans: 0.0261\n","   rot: 0.3659\n","   points: 0.0003\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1092\n","   trans: 0.0065\n","   rot: 0.2051\n","   points: 0.0004\n","==================================================\n","\n","############ End Epoch 23/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1092\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_004500.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_004500.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:29:58\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:40:19\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 23/100\n","\n","==================================================\n","📊 Epoch 23 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2091\n","   val_loss: 0.1092\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 24/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 24/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2074\n","   trans: 0.0258\n","   rot: 0.3629\n","   points: 0.0003\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1114\n","   trans: 0.0067\n","   rot: 0.2092\n","   points: 0.0003\n","==================================================\n","\n","############ End Epoch 24/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:31:13\n","   Average Epoch Time: 00:01:18\n","   Estimated Remaining Time: 01:38:53\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 24/100\n","\n","==================================================\n","📊 Epoch 24 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2074\n","   val_loss: 0.1114\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 25/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 25/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2022\n","   trans: 0.0255\n","   rot: 0.3532\n","   points: 0.0003\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1174\n","   trans: 0.0080\n","   rot: 0.2185\n","   points: 0.0004\n","==================================================\n","\n","############ End Epoch 25/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:32:29\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:37:28\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 25/100\n","\n","==================================================\n","📊 Epoch 25 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2022\n","   val_loss: 0.1174\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 26/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 26/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1944\n","   trans: 0.0252\n","   rot: 0.3382\n","   points: 0.0003\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1243\n","   trans: 0.0070\n","   rot: 0.2346\n","   points: 0.0003\n","==================================================\n","\n","############ End Epoch 26/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:33:45\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:36:05\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 26/100\n","\n","==================================================\n","📊 Epoch 26 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1944\n","   val_loss: 0.1243\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 27/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 27/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1982\n","   trans: 0.0248\n","   rot: 0.3466\n","   points: 0.0003\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1070\n","   trans: 0.0075\n","   rot: 0.1988\n","   points: 0.0003\n","==================================================\n","\n","############ End Epoch 27/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.1070\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_005007.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_005007.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:35:04\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:34:49\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 27/100\n","\n","==================================================\n","📊 Epoch 27 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1982\n","   val_loss: 0.1070\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 28/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 28/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.2022\n","   trans: 0.0243\n","   rot: 0.3557\n","   points: 0.0003\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1445\n","   trans: 0.0088\n","   rot: 0.2712\n","   points: 0.0003\n","==================================================\n","\n","############ End Epoch 28/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:36:20\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:33:26\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 28/100\n","\n","==================================================\n","📊 Epoch 28 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.2022\n","   val_loss: 0.1445\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 29/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 29/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1961\n","   trans: 0.0235\n","   rot: 0.3451\n","   points: 0.0003\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1155\n","   trans: 0.0111\n","   rot: 0.2086\n","   points: 0.0003\n","==================================================\n","\n","############ End Epoch 29/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:37:36\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:32:04\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 29/100\n","\n","==================================================\n","📊 Epoch 29 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1961\n","   val_loss: 0.1155\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 30/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 30/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1895\n","   trans: 0.0245\n","   rot: 0.3298\n","   points: 0.0002\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1276\n","   trans: 0.0101\n","   rot: 0.2350\n","   points: 0.0002\n","==================================================\n","\n","############ End Epoch 30/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:38:52\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:30:41\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 30/100\n","\n","==================================================\n","📊 Epoch 30 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1895\n","   val_loss: 0.1276\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 31/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 31/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1846\n","   trans: 0.0223\n","   rot: 0.3244\n","   points: 0.0002\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1416\n","   trans: 0.0103\n","   rot: 0.2625\n","   points: 0.0003\n","==================================================\n","\n","############ End Epoch 31/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:40:07\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:29:18\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 31/100\n","\n","==================================================\n","📊 Epoch 31 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1846\n","   val_loss: 0.1416\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 32/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 32/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1914\n","   trans: 0.0259\n","   rot: 0.3309\n","   points: 0.0002\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1659\n","   trans: 0.0148\n","   rot: 0.3020\n","   points: 0.0003\n","==================================================\n","\n","############ End Epoch 32/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:41:23\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:27:57\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 32/100\n","\n","==================================================\n","📊 Epoch 32 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1914\n","   val_loss: 0.1659\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 33/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 33/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1893\n","   trans: 0.0248\n","   rot: 0.3288\n","   points: 0.0002\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1223\n","   trans: 0.0061\n","   rot: 0.2323\n","   points: 0.0002\n","==================================================\n","\n","############ End Epoch 33/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:42:39\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:26:35\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 33/100\n","\n","==================================================\n","📊 Epoch 33 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1893\n","   val_loss: 0.1223\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 34/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 34/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1790\n","   trans: 0.0224\n","   rot: 0.3132\n","   points: 0.0002\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1210\n","   trans: 0.0063\n","   rot: 0.2294\n","   points: 0.0002\n","==================================================\n","\n","############ End Epoch 34/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:43:54\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:25:14\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 34/100\n","\n","==================================================\n","📊 Epoch 34 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1790\n","   val_loss: 0.1210\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 35/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 35/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1698\n","   trans: 0.0226\n","   rot: 0.2944\n","   points: 0.0002\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1165\n","   trans: 0.0076\n","   rot: 0.2176\n","   points: 0.0002\n","==================================================\n","\n","############ End Epoch 35/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:45:10\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:23:53\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 35/100\n","\n","==================================================\n","📊 Epoch 35 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1698\n","   val_loss: 0.1165\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 36/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 36/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1734\n","   trans: 0.0217\n","   rot: 0.3034\n","   points: 0.0002\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1180\n","   trans: 0.0123\n","   rot: 0.2113\n","   points: 0.0002\n","==================================================\n","\n","############ End Epoch 36/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:46:26\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:22:32\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 36/100\n","\n","==================================================\n","📊 Epoch 36 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1734\n","   val_loss: 0.1180\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 37/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 37/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1658\n","   trans: 0.0197\n","   rot: 0.2922\n","   points: 0.0001\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.0955\n","   trans: 0.0056\n","   rot: 0.1796\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 37/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.0955\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250530_010248.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250530_010248.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:47:44\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:21:16\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 37/100\n","\n","==================================================\n","📊 Epoch 37 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1658\n","   val_loss: 0.0955\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 38/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 38/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1636\n","   trans: 0.0197\n","   rot: 0.2879\n","   points: 0.0001\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1204\n","   trans: 0.0084\n","   rot: 0.2239\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 38/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:48:59\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:19:56\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 38/100\n","\n","==================================================\n","📊 Epoch 38 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1636\n","   val_loss: 0.1204\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 39/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 39/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1720\n","   trans: 0.0206\n","   rot: 0.3028\n","   points: 0.0001\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1511\n","   trans: 0.0082\n","   rot: 0.2858\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 39/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:50:15\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:18:36\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 39/100\n","\n","==================================================\n","📊 Epoch 39 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1720\n","   val_loss: 0.1511\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 40/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 40/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1783\n","   trans: 0.0202\n","   rot: 0.3161\n","   points: 0.0001\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1131\n","   trans: 0.0070\n","   rot: 0.2119\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 40/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:51:31\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:17:17\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 40/100\n","\n","==================================================\n","📊 Epoch 40 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1783\n","   val_loss: 0.1131\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 41/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 41/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1631\n","   trans: 0.0191\n","   rot: 0.2880\n","   points: 0.0001\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1003\n","   trans: 0.0072\n","   rot: 0.1862\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 41/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:52:47\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:15:58\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 41/100\n","\n","==================================================\n","📊 Epoch 41 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1631\n","   val_loss: 0.1003\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 42/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 42/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1547\n","   trans: 0.0181\n","   rot: 0.2732\n","   points: 0.0001\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.0990\n","   trans: 0.0084\n","   rot: 0.1810\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 42/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:54:04\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:14:39\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 42/100\n","\n","==================================================\n","📊 Epoch 42 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1547\n","   val_loss: 0.0990\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 43/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 43/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1495\n","   trans: 0.0183\n","   rot: 0.2624\n","   points: 0.0001\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1251\n","   trans: 0.0101\n","   rot: 0.2301\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 43/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:55:20\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:13:21\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 43/100\n","\n","==================================================\n","📊 Epoch 43 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1495\n","   val_loss: 0.1251\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 44/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 44/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1575\n","   trans: 0.0189\n","   rot: 0.2773\n","   points: 0.0000\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1095\n","   trans: 0.0085\n","   rot: 0.2020\n","   points: 0.0001\n","==================================================\n","\n","############ End Epoch 44/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:56:36\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:12:02\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 44/100\n","\n","==================================================\n","📊 Epoch 44 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1575\n","   val_loss: 0.1095\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 45/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 45/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1589\n","   trans: 0.0188\n","   rot: 0.2802\n","   points: 0.0000\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1240\n","   trans: 0.0127\n","   rot: 0.2227\n","   points: 0.0000\n","==================================================\n","\n","############ End Epoch 45/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:57:52\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:10:43\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 45/100\n","\n","==================================================\n","📊 Epoch 45 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1589\n","   val_loss: 0.1240\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 46/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 46/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1772\n","   trans: 0.0212\n","   rot: 0.3121\n","   points: 0.0000\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.1782\n","   trans: 0.0146\n","   rot: 0.3272\n","   points: 0.0000\n","==================================================\n","\n","############ End Epoch 46/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:59:07\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:09:24\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 46/100\n","\n","==================================================\n","📊 Epoch 46 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1772\n","   val_loss: 0.1782\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 47/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 47/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:04<00:00,  1.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 47/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:00:23\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:08:05\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 47/100\n","\n","==================================================\n","📊 Epoch 47 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 48/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 48/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 48/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:01:37\n","   Average Epoch Time: 00:01:17\n","   Estimated Remaining Time: 01:06:45\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 48/100\n","\n","==================================================\n","📊 Epoch 48 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 49/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 49/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 49/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:02:51\n","   Average Epoch Time: 00:01:16\n","   Estimated Remaining Time: 01:05:25\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 49/100\n","\n","==================================================\n","📊 Epoch 49 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 50/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 50/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 50/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:04:04\n","   Average Epoch Time: 00:01:16\n","   Estimated Remaining Time: 01:04:04\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 50/100\n","\n","==================================================\n","📊 Epoch 50 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 51/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 51/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:11<00:00,  2.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 51/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:05:18\n","   Average Epoch Time: 00:01:16\n","   Estimated Remaining Time: 01:02:45\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 51/100\n","\n","==================================================\n","📊 Epoch 51 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 52/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 52/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 52/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:06:32\n","   Average Epoch Time: 00:01:16\n","   Estimated Remaining Time: 01:01:25\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 52/100\n","\n","==================================================\n","📊 Epoch 52 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 53/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 53/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 53/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:07:46\n","   Average Epoch Time: 00:01:16\n","   Estimated Remaining Time: 01:00:06\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 53/100\n","\n","==================================================\n","📊 Epoch 53 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 54/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 54/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [00:59<00:00,  1.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:10<00:00,  2.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: nan\n","   trans: nan\n","   rot: nan\n","   points: nan\n","==================================================\n","\n","############ End Epoch 54/100 ############\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 01:08:57\n","   Average Epoch Time: 00:01:16\n","   Estimated Remaining Time: 00:58:44\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 54/100\n","\n","==================================================\n","📊 Epoch 54 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: nan\n","   val_loss: nan\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 55/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 55/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training:  12%|█▏        | 13/109 [00:08<01:00,  1.59it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-8d5da6f69726>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-8d5da6f69726>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting Training Process for Object {object_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         train_model(model, train_loader, val_loader,\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_EPOCHS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     learning_rate=CONFIG['LEARNING_RATE'])\n","\u001b[0;32m<ipython-input-22-8d5da6f69726>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    513\u001b[0m                     \u001b[0mpose_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutside9_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutside9_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 )\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;31m# Unscale before clipping & optimiser step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# **Validation**"],"metadata":{"id":"BbAqYcV9HLro"}},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","import cv2\n","from tqdm import tqdm\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.cuda.amp import autocast\n","\n","# =========================\n","# CONFIGURATION SECTION\n","# =========================\n","CONFIG = {\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',  # Root directory containing all object folders\n","    'OBJECT_ID': '01',  # Object ID to validate (e.g., '01')\n","    'BATCH_SIZE': 1,  # For validation, use batch size 1 for accurate metrics\n","    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n","    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'MODELS_DIR': '/content/models',  # Directory containing saved models\n","    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n","    'ADD_THRESHOLD_MM': 10.0,  # Success if ADD < 10 mm\n","    'NUM_WORKERS': 2,\n","}\n","\n","RGB_TRANSFORM = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# For depth we keep lambda to tensor float32 (metres)\n","DEPTH_TRANSFORM = lambda img: torch.from_numpy(np.array(img, dtype=np.float32)).unsqueeze(0)\n","\n","# =========================\n","# DEPTH READER FOR .dpt FILES\n","# =========================\n","def read_depth_dpt(path):\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","    return data.reshape(h, w).astype(np.float32)\n","\n","# =========================\n","# MODEL ARCHITECTURE\n","# =========================\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","\n","    def forward(self, features):\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        # Use ResNet50 as backbone\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])   # Output: 256\n","        self.rgb_layer2 = list(resnet.children())[5]                    # Output: 512\n","        self.rgb_layer3 = list(resnet.children())[6]                    # Output: 1024\n","        self.rgb_layer4 = list(resnet.children())[7]                    # Output: 2048\n","\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","\n","        # Fusion block\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Pose head\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","\n","        # Outside9 head\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)\n","        )\n","\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)\n","\n","        # Pose prediction\n","        pooled = self.global_pool(fused)\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","\n","        # Outside9 prediction\n","        outside9 = self.outside9_head(fused)\n","        target_size = (rgb.shape[2], rgb.shape[3])\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","\n","        return pose, outside9\n","\n","# =========================\n","# DATASET CLASS\n","# =========================\n","class ValidationDataset(Dataset):\n","    def __init__(self, base_dir, object_id, num_radius_points=9):\n","        self.base_dir = base_dir\n","        self.object_id = object_id\n","        self.num_radius_points = num_radius_points\n","\n","        # Set up paths\n","        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n","        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n","        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n","        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n","        self.radius_base_dir = os.path.join(base_dir, object_id)\n","\n","        # Load validation split\n","        split_file = os.path.join(base_dir, object_id, 'Split', 'val.txt')\n","        with open(split_file, 'r') as f:\n","            self.filenames = [line.strip() for line in f.readlines()]\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            filename = self.filenames[idx]\n","            base_name = filename.split('.')[0]\n","            # Load RGB image\n","            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","            rgb_img = cv2.imread(rgb_path)\n","            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","            rgb_img = RGB_TRANSFORM(Image.fromarray(rgb_img))\n","            # Load depth image (.dpt files need special reader)\n","            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","            if not os.path.exists(depth_path):\n","                raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n","            depth_np = read_depth_dpt(depth_path)\n","            if depth_np is None:\n","                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n","            depth_np = depth_np / 1000.0  # to metres (consistent with training)\n","            depth_img = DEPTH_TRANSFORM(Image.fromarray(depth_np, mode='F'))\n","            # Load pose\n","            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","            pose = np.load(pose_path)\n","            if pose.shape == (3, 4):\n","                rot = R.from_matrix(pose[:, :3]).as_quat()\n","                trans = pose[:, 3]\n","                pose = np.concatenate([trans, rot])\n","            elif pose.shape == (4, 4):\n","                rot = R.from_matrix(pose[:3, :3]).as_quat()\n","                trans = pose[:3, 3]\n","                pose = np.concatenate([trans, rot])\n","            # Load radius maps\n","            radius_maps = []\n","            for pt_idx in range(1, self.num_radius_points + 1):\n","                radius_folder = f\"Out_pt{pt_idx}_dm\"\n","                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","                radius_map = np.load(radius_path)\n","                radius_maps.append(radius_map)\n","            radius_maps = np.array(radius_maps)\n","            return {\n","                'rgb': rgb_img,\n","                'depth': depth_img,\n","                'pose': torch.FloatTensor(pose),\n","                'radius_maps': torch.FloatTensor(radius_maps)\n","            }\n","        except Exception as e:\n","            print(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n","            return None\n","\n","def safe_collate(batch):\n","    batch = [b for b in batch if b is not None]\n","    if len(batch) == 0:\n","        return None\n","    return torch.utils.data.dataloader.default_collate(batch)\n","\n","# =========================\n","# METRIC FUNCTIONS\n","# =========================\n","def translation_rmse(pred, gt):\n","    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n","\n","def rotation_error(pred, gt, deg=True):\n","    pred_q = pred[3:] / np.linalg.norm(pred[3:])\n","    gt_q = gt[3:] / np.linalg.norm(gt[3:])\n","    dot = np.clip(np.abs(np.dot(pred_q, gt_q)), -1.0 + 1e-7, 1.0 - 1e-7)\n","    angle = 2.0 * np.arccos(dot)  # radians\n","    return np.degrees(angle) if deg else angle\n","\n","def add_metric(pred_pose, gt_pose, mesh_points):\n","    if mesh_points is None:\n","        return 0.0\n","\n","    # Convert quaternions to rotation matrices\n","    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n","    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n","\n","    # Transform mesh points\n","    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n","    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n","\n","    # Compute mean distance\n","    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n","    return mean_dist  # mm\n","\n","def get_latest_model(models_dir):\n","    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n","    if not model_files:\n","        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n","    return max(model_files, key=os.path.getctime)\n","\n","# =========================\n","# VALIDATION PIPELINE\n","# =========================\n","def validate():\n","    print(\"\\n================= RCVPose Validation (Colab) =================\")\n","\n","    # Find and load latest model\n","    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n","    print(f\"Using latest model: {model_path}\")\n","\n","    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n","    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    # Load validation data\n","    dataset = ValidationDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n","    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=CONFIG['NUM_WORKERS'], collate_fn=safe_collate, pin_memory=True)\n","\n","    # Load mesh for ADD metric\n","    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n","    mesh_points = None\n","    try:\n","        if os.path.exists(mesh_path):\n","            import open3d as o3d\n","            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n","    except ImportError:\n","        print(\"open3d is not installed. Please run: !pip install open3d\")\n","\n","    # Metrics accumulators\n","    trans_errors = []\n","    rot_errors = []\n","    points_errors = []\n","    add_errors = []\n","    add_success = 0\n","\n","    # Validation loop (skip samples with missing/corrupt files)\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Validating\"):\n","            if batch is None:\n","                continue  # all samples in this worker were None\n","            rgb = batch['rgb'].to(CONFIG['DEVICE'])\n","            depth = batch['depth'].to(CONFIG['DEVICE'])\n","            pose_gt = batch['pose'].to(CONFIG['DEVICE'])\n","            radius_maps_gt = batch['radius_maps'].to(CONFIG['DEVICE'])\n","            # Forward pass with AMP for speed\n","            with autocast():\n","                pose_pred, radius_maps_pred = model(rgb, depth)\n","            # Move predictions to CPU for metric computation\n","            pose_pred = pose_pred.cpu().numpy()\n","            pose_gt = pose_gt.cpu().numpy()\n","            radius_maps_pred = radius_maps_pred.cpu().numpy()\n","            radius_maps_gt = radius_maps_gt.cpu().numpy()\n","            # Compute metrics\n","            for j in range(len(pose_pred)):\n","                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n","                trans_errors.append(trans_error)\n","                rot_errors.append(rotation_error(pose_pred[j], pose_gt[j]))\n","                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n","                points_errors.append(points_error)\n","                if mesh_points is not None:\n","                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n","                    add_errors.append(add_error)\n","                    if add_error < CONFIG['ADD_THRESHOLD_MM']:\n","                        add_success += 1\n","\n","    # Compute and print final metrics\n","    print(\"\\nValidation Results:\")\n","    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n","    print(f\"Rotation Error (deg): {np.mean(rot_errors):.2f}\")\n","    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n","    if mesh_points is not None:\n","        print(f\"ADD (mm): {np.mean(add_errors):.3f}\")\n","        print(f\"ADD Success (<{CONFIG['ADD_THRESHOLD_MM']}mm): {add_success/len(trans_errors)*100:.2f}%\")\n","\n","    return {\n","        'trans_rmse': np.mean(trans_errors),\n","        'rot_error': np.mean(rot_errors),\n","        'points_mse': np.mean(points_errors),\n","        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n","        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n","    }\n","\n","if __name__ == \"__main__\":\n","    validate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QoKBaCBCHLHI","executionInfo":{"status":"ok","timestamp":1748568272335,"user_tz":-120,"elapsed":12120,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"89644d96-3ead-4103-f504-e9549f76067a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================= RCVPose Validation (Colab) =================\n","Using latest model: /content/models/best_model_20250530_010248.pth\n"]},{"output_type":"stream","name":"stderr","text":["\rValidating:   0%|          | 0/247 [00:00<?, ?it/s]<ipython-input-23-ae9bd51244bd>:320: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","Validating: 100%|██████████| 247/247 [00:10<00:00, 23.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Validation Results:\n","Translation RMSE: 0.0710 mm\n","Rotation Error (deg): 10.34\n","Points MSE: 0.0001\n","ADD (mm): 5.309\n","ADD Success (<10.0mm): 95.55%\n"]}]},{"cell_type":"markdown","source":["# **Test**"],"metadata":{"id":"q1BtA8hCbrYK"}},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","import cv2\n","from tqdm import tqdm\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.cuda.amp import autocast\n","\n","# =========================\n","# CONFIGURATION SECTION\n","# =========================\n","CONFIG = {\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',  # Root directory containing all object folders\n","    'OBJECT_ID': '01',  # Object ID to test (e.g., '01')\n","    'BATCH_SIZE': 1,  # For test, use batch size 1 for accurate metrics\n","    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n","    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'MODELS_DIR': '/content/models',  # Directory containing saved models\n","    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n","    'ADD_THRESHOLD_MM': 10.0,\n","    'NUM_WORKERS': 2,\n","}\n","\n","RGB_TRANSFORM = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","DEPTH_TRANSFORM = lambda img: torch.from_numpy(np.array(img, dtype=np.float32)).unsqueeze(0)\n","\n","# =========================\n","# DEPTH READER FOR .dpt FILES\n","# =========================\n","def read_depth_dpt(path):\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","    return data.reshape(h, w).astype(np.float32)\n","\n","# =========================\n","# MODEL ARCHITECTURE\n","# =========================\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","    def forward(self, features):\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])\n","        self.rgb_layer2 = list(resnet.children())[5]\n","        self.rgb_layer3 = list(resnet.children())[6]\n","        self.rgb_layer4 = list(resnet.children())[7]\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)\n","        )\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)\n","        pooled = self.global_pool(fused)\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","        outside9 = self.outside9_head(fused)\n","        target_size = (rgb.shape[2], rgb.shape[3])\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","        return pose, outside9\n","\n","# =========================\n","# DATASET CLASS\n","# =========================\n","class TestDataset(Dataset):\n","    def __init__(self, base_dir, object_id, num_radius_points=9):\n","        self.base_dir = base_dir\n","        self.object_id = object_id\n","        self.num_radius_points = num_radius_points\n","        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n","        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n","        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n","        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n","        self.radius_base_dir = os.path.join(base_dir, object_id)\n","        split_file = os.path.join(base_dir, object_id, 'Split', 'test.txt')\n","        with open(split_file, 'r') as f:\n","            self.filenames = [line.strip() for line in f.readlines()]\n","    def __len__(self):\n","        return len(self.filenames)\n","    def __getitem__(self, idx):\n","        try:\n","            filename = self.filenames[idx]\n","            base_name = filename.split('.')[0]\n","            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","            rgb_img = cv2.imread(rgb_path)\n","            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","            rgb_img = RGB_TRANSFORM(Image.fromarray(rgb_img))\n","            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","            depth_np = read_depth_dpt(depth_path)\n","            if depth_np is None:\n","                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n","            depth_np = depth_np / 1000.0  # metres\n","            depth_img = DEPTH_TRANSFORM(Image.fromarray(depth_np, mode='F'))\n","            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","            pose = np.load(pose_path)\n","            if pose.shape == (3, 4):\n","                rot = R.from_matrix(pose[:, :3]).as_quat()\n","                trans = pose[:, 3]\n","                pose = np.concatenate([trans, rot])\n","            elif pose.shape == (4, 4):\n","                rot = R.from_matrix(pose[:3, :3]).as_quat()\n","                trans = pose[:3, 3]\n","                pose = np.concatenate([trans, rot])\n","            radius_maps = []\n","            for pt_idx in range(1, self.num_radius_points + 1):\n","                radius_folder = f\"Out_pt{pt_idx}_dm\"\n","                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","                radius_map = np.load(radius_path)\n","                radius_maps.append(radius_map)\n","            radius_maps = np.array(radius_maps)\n","            return {\n","                'rgb': rgb_img,\n","                'depth': depth_img,\n","                'pose': torch.FloatTensor(pose),\n","                'radius_maps': torch.FloatTensor(radius_maps)\n","            }\n","        except Exception as e:\n","            print(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n","            return None\n","\n","def safe_collate(batch):\n","    batch = [b for b in batch if b is not None]\n","    if len(batch) == 0:\n","        return None\n","    return torch.utils.data.dataloader.default_collate(batch)\n","\n","# =========================\n","# METRIC FUNCTIONS\n","# =========================\n","def translation_rmse(pred, gt):\n","    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n","\n","def rotation_error(pred, gt, deg=True):\n","    pred_q = pred[3:] / np.linalg.norm(pred[3:])\n","    gt_q = gt[3:] / np.linalg.norm(gt[3:])\n","    dot = np.clip(np.abs(np.dot(pred_q, gt_q)), -1.0 + 1e-7, 1.0 - 1e-7)\n","    angle = 2.0 * np.arccos(dot)\n","    return np.degrees(angle) if deg else angle\n","\n","def add_metric(pred_pose, gt_pose, mesh_points):\n","    if mesh_points is None:\n","        return 0.0\n","    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n","    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n","    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n","    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n","    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n","    return mean_dist  # mm\n","\n","def get_latest_model(models_dir):\n","    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n","    if not model_files:\n","        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n","    return max(model_files, key=os.path.getctime)\n","\n","# =========================\n","# TEST PIPELINE\n","# =========================\n","def test():\n","    print(\"\\n================= RCVPose Test (Colab) =================\")\n","    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n","    print(f\"Using latest model: {model_path}\")\n","    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n","    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","    dataset = TestDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n","    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=CONFIG['NUM_WORKERS'], collate_fn=safe_collate, pin_memory=True)\n","    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n","    mesh_points = None\n","    try:\n","        if os.path.exists(mesh_path):\n","            import open3d as o3d\n","            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n","    except ImportError:\n","        print(\"open3d is not installed. Please run: !pip install open3d\")\n","    trans_errors = []\n","    rot_errors = []\n","    points_errors = []\n","    add_errors = []\n","    add_success = 0\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Testing\"):\n","            if batch is None:\n","                continue\n","            rgb = batch['rgb'].to(CONFIG['DEVICE'])\n","            depth = batch['depth'].to(CONFIG['DEVICE'])\n","            pose_gt = batch['pose'].to(CONFIG['DEVICE'])\n","            radius_maps_gt = batch['radius_maps'].to(CONFIG['DEVICE'])\n","            with autocast():\n","                pose_pred, radius_maps_pred = model(rgb, depth)\n","            pose_pred = pose_pred.cpu().numpy()\n","            pose_gt = pose_gt.cpu().numpy()\n","            radius_maps_pred = radius_maps_pred.cpu().numpy()\n","            radius_maps_gt = radius_maps_gt.cpu().numpy()\n","            for j in range(len(pose_pred)):\n","                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n","                trans_errors.append(trans_error)\n","                rot_error = rotation_error(pose_pred[j], pose_gt[j])\n","                rot_errors.append(rot_error)\n","                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n","                points_errors.append(points_error)\n","                if mesh_points is not None:\n","                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n","                    add_errors.append(add_error)\n","                    if add_error < CONFIG['ADD_THRESHOLD_MM']:\n","                        add_success += 1\n","    print(\"\\nTest Results:\")\n","    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n","    print(f\"Rotation Error (deg): {np.mean(rot_errors):.2f}\")\n","    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n","    if mesh_points is not None:\n","        print(f\"ADD (mm): {np.mean(add_errors):.3f}\")\n","        print(f\"ADD Success (<{CONFIG['ADD_THRESHOLD_MM']}mm): {add_success/len(trans_errors)*100:.2f}%\")\n","    return {\n","        'trans_rmse': np.mean(trans_errors),\n","        'rot_error': np.mean(rot_errors),\n","        'points_mse': np.mean(points_errors),\n","        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n","        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n","    }\n","\n","if __name__ == \"__main__\":\n","    test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4Mhrqi_bu5q","executionInfo":{"status":"ok","timestamp":1748568292155,"user_tz":-120,"elapsed":7313,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"f182669c-ef20-4415-f20d-4b09f83493c7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================= RCVPose Test (Colab) =================\n","Using latest model: /content/models/best_model_20250530_010248.pth\n"]},{"output_type":"stream","name":"stderr","text":["\rTesting:   0%|          | 0/124 [00:00<?, ?it/s]<ipython-input-24-2b2162de47cc>:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","Testing: 100%|██████████| 124/124 [00:05<00:00, 21.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Results:\n","Translation RMSE: 0.0693 mm\n","Rotation Error (deg): 10.81\n","Points MSE: 0.0001\n","ADD (mm): 5.466\n","ADD Success (<10.0mm): 94.35%\n"]}]},{"cell_type":"markdown","metadata":{"id":"8yar8tY6XPh1"},"source":["# **All data are in millimeter scale.**"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1C6CXJWlRC5Mqfws-zB9kqh4-OeMSd4YM","timestamp":1747998293619},{"file_id":"1LEXC3BfIpeRC4cTCPIhH1QZ05N01bh1k","timestamp":1747955566766},{"file_id":"1NxmXsK5njA3LFwUTTPIei7t7qWxkHZim","timestamp":1747877518212},{"file_id":"1Ed8hajlaGmwSAwDFUftKdDTn5ReDDJjw","timestamp":1747775767352}],"authorship_tag":"ABX9TyO83tHScJYUp49Q5a2yDX1k"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}