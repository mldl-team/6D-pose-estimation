{"cells":[{"cell_type":"markdown","metadata":{"id":"SMWItJ1CUp56"},"source":["# **Configure Google Colab Settings**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84931,"status":"ok","timestamp":1748809625499,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"St4iy9xVUvr8","outputId":"6a061cab-fe60-4159-8d9b-5de20a15a43b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Connect to personal Google Drive space\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1748809625621,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"6e80OV_qUcfQ","outputId":"ded6d25f-75df-448b-c18a-c5d6975b356c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["#Change directory\n","%cd /content/\n","#Create Paths\n","!mkdir /content/dataset/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82055,"status":"ok","timestamp":1748809712258,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"7kIVCfM8UwgR","outputId":"17f71287-21ba-42c6-ad96-00a82ad1d413"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Extracting 62142 files...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Unzipping: 100%|██████████| 62142/62142 [01:20<00:00, 775.93file/s]  "]},{"output_type":"stream","name":"stdout","text":["\n"," Extraction complete.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import zipfile\n","import os\n","from tqdm import tqdm\n","\n","zip_path = \"/content/drive/MyDrive/Linemod_preprocessed.zip\"\n","extract_to = \"/content/dataset/\"\n","\n","# Create the output directory if it doesn't exist\n","os.makedirs(extract_to, exist_ok=True)\n","\n","# Open the zip file\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    # Retrieve the list of files in the archive\n","    file_list = zip_ref.infolist()\n","\n","    print(f\" Extracting {len(file_list)} files...\\n\")\n","    for file in tqdm(file_list, desc=\"Unzipping\", unit=\"file\"):\n","        # Extract each file to the target directory\n","        zip_ref.extract(file, extract_to)\n","\n","print(\"\\n Extraction complete.\")"]},{"cell_type":"markdown","metadata":{"id":"Rio2kCCbU0rl"},"source":["# **Configure Github**"]},{"cell_type":"markdown","metadata":{"id":"zFNfdbV7BQgc"},"source":["### Install Git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":35078,"status":"ok","timestamp":1748569010748,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"WskY0S65VGM9","outputId":"7633e9e7-c58c-4c5d-bf70-c8417ab0dee0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.12).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.12).\n","Calculating upgrade... Done\n","The following packages have been kept back:\n","  libcudnn9-cuda-12 libcudnn9-dev-cuda-12 libnccl-dev libnccl2\n","The following packages will be upgraded:\n","  base-files binutils binutils-common binutils-x86-64-linux-gnu\n","  cuda-toolkit-12-config-common cuda-toolkit-config-common e2fsprogs\n","  libbinutils libc-bin libcap2 libctf-nobfd0 libctf0 libext2fs2 libgnutls30\n","  libldap-2.5-0 libpam-modules libpam-modules-bin libpam-runtime libpam0g\n","  libperl5.34 libseccomp2 libss2 libtasn1-6 libudev1 linux-libc-dev logsave\n","  openssl perl perl-base perl-modules-5.34 python3-pkg-resources\n","31 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n","Need to get 19.3 MB of archives.\n","After this operation, 289 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 base-files amd64 12ubuntu4.7 [61.9 kB]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-config-common 12.9.37-1 [16.5 kB]\n","Get:3 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libperl5.34 amd64 5.34.0-3ubuntu1.4 [4,820 kB]\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-config-common 12.9.37-1 [16.5 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl amd64 5.34.0-3ubuntu1.4 [232 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-base amd64 5.34.0-3ubuntu1.4 [1,759 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-modules-5.34 all 5.34.0-3ubuntu1.4 [2,977 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-bin amd64 2.35-0ubuntu3.10 [706 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam0g amd64 1.4.0-11ubuntu2.5 [59.8 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-modules-bin amd64 1.4.0-11ubuntu2.5 [37.4 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-modules amd64 1.4.0-11ubuntu2.5 [280 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logsave amd64 1.46.5-2ubuntu1.2 [10.1 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libext2fs2 amd64 1.46.5-2ubuntu1.2 [208 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 e2fsprogs amd64 1.46.5-2ubuntu1.2 [590 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcap2 amd64 1:2.44-1ubuntu0.22.04.2 [18.3 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-runtime all 1.4.0-11ubuntu2.5 [40.2 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtasn1-6 amd64 4.18.0-4ubuntu0.1 [43.5 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgnutls30 amd64 3.7.3-4ubuntu1.6 [969 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libseccomp2 amd64 2.5.3-2ubuntu3~22.04.1 [47.4 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libss2 amd64 1.46.5-2ubuntu1.2 [12.3 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssl amd64 3.0.2-0ubuntu1.19 [1,186 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf0 amd64 2.38-4ubuntu2.8 [103 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf-nobfd0 amd64 2.38-4ubuntu2.8 [108 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.38-4ubuntu2.8 [2,324 kB]\n","Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libbinutils amd64 2.38-4ubuntu2.8 [661 kB]\n","Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils amd64 2.38-4ubuntu2.8 [3,196 B]\n","Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-common amd64 2.38-4ubuntu2.8 [223 kB]\n","Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libldap-2.5-0 amd64 2.5.19+dfsg-0ubuntu0.22.04.1 [184 kB]\n","Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 linux-libc-dev amd64 5.15.0-140.150 [1,313 kB]\n","Fetched 19.3 MB in 9s (2,188 kB/s)\n","Extracting templates from packages: 100%\n","Preconfiguring packages ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../base-files_12ubuntu4.7_amd64.deb ...\n","Unpacking base-files (12ubuntu4.7) over (12ubuntu4.6) ...\n","Setting up base-files (12ubuntu4.7) ...\n","Installing new version of config file /etc/issue ...\n","Installing new version of config file /etc/issue.net ...\n","Installing new version of config file /etc/lsb-release ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libperl5.34_5.34.0-3ubuntu1.4_amd64.deb ...\n","Unpacking libperl5.34:amd64 (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Preparing to unpack .../perl_5.34.0-3ubuntu1.4_amd64.deb ...\n","Unpacking perl (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Preparing to unpack .../perl-base_5.34.0-3ubuntu1.4_amd64.deb ...\n","Unpacking perl-base (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Setting up perl-base (5.34.0-3ubuntu1.4) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../perl-modules-5.34_5.34.0-3ubuntu1.4_all.deb ...\n","Unpacking perl-modules-5.34 (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n","Preparing to unpack .../libc-bin_2.35-0ubuntu3.10_amd64.deb ...\n","Unpacking libc-bin (2.35-0ubuntu3.10) over (2.35-0ubuntu3.8) ...\n","Setting up libc-bin (2.35-0ubuntu3.10) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam0g_1.4.0-11ubuntu2.5_amd64.deb ...\n","Unpacking libpam0g:amd64 (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam0g:amd64 (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam-modules-bin_1.4.0-11ubuntu2.5_amd64.deb ...\n","Unpacking libpam-modules-bin (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam-modules-bin (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam-modules_1.4.0-11ubuntu2.5_amd64.deb ...\n","Unpacking libpam-modules:amd64 (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam-modules:amd64 (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../logsave_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking logsave (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Preparing to unpack .../libext2fs2_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking libext2fs2:amd64 (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Setting up libext2fs2:amd64 (1.46.5-2ubuntu1.2) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../e2fsprogs_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking e2fsprogs (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Preparing to unpack .../libcap2_1%3a2.44-1ubuntu0.22.04.2_amd64.deb ...\n","Unpacking libcap2:amd64 (1:2.44-1ubuntu0.22.04.2) over (1:2.44-1ubuntu0.22.04.1) ...\n","Setting up libcap2:amd64 (1:2.44-1ubuntu0.22.04.2) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libpam-runtime_1.4.0-11ubuntu2.5_all.deb ...\n","Unpacking libpam-runtime (1.4.0-11ubuntu2.5) over (1.4.0-11ubuntu2.4) ...\n","Setting up libpam-runtime (1.4.0-11ubuntu2.5) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libudev1_249.11-0ubuntu3.15_amd64.deb ...\n","Unpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\n","Setting up libudev1:amd64 (249.11-0ubuntu3.15) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libtasn1-6_4.18.0-4ubuntu0.1_amd64.deb ...\n","Unpacking libtasn1-6:amd64 (4.18.0-4ubuntu0.1) over (4.18.0-4build1) ...\n","Setting up libtasn1-6:amd64 (4.18.0-4ubuntu0.1) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libgnutls30_3.7.3-4ubuntu1.6_amd64.deb ...\n","Unpacking libgnutls30:amd64 (3.7.3-4ubuntu1.6) over (3.7.3-4ubuntu1.5) ...\n","Setting up libgnutls30:amd64 (3.7.3-4ubuntu1.6) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../libseccomp2_2.5.3-2ubuntu3~22.04.1_amd64.deb ...\n","Unpacking libseccomp2:amd64 (2.5.3-2ubuntu3~22.04.1) over (2.5.3-2ubuntu2) ...\n","Setting up libseccomp2:amd64 (2.5.3-2ubuntu3~22.04.1) ...\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../00-libss2_1.46.5-2ubuntu1.2_amd64.deb ...\n","Unpacking libss2:amd64 (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n","Preparing to unpack .../01-openssl_3.0.2-0ubuntu1.19_amd64.deb ...\n","Unpacking openssl (3.0.2-0ubuntu1.19) over (3.0.2-0ubuntu1.16) ...\n","Preparing to unpack .../02-libctf0_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking libctf0:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../03-libctf-nobfd0_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking libctf-nobfd0:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../04-binutils-x86-64-linux-gnu_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking binutils-x86-64-linux-gnu (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../05-libbinutils_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking libbinutils:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../06-binutils_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking binutils (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../07-binutils-common_2.38-4ubuntu2.8_amd64.deb ...\n","Unpacking binutils-common:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n","Preparing to unpack .../08-cuda-toolkit-12-config-common_12.9.37-1_all.deb ...\n","Unpacking cuda-toolkit-12-config-common (12.9.37-1) over (12.5.82-1) ...\n","Preparing to unpack .../09-cuda-toolkit-config-common_12.9.37-1_all.deb ...\n","Unpacking cuda-toolkit-config-common (12.9.37-1) over (12.5.82-1) ...\n","Preparing to unpack .../10-libldap-2.5-0_2.5.19+dfsg-0ubuntu0.22.04.1_amd64.deb ...\n","Unpacking libldap-2.5-0:amd64 (2.5.19+dfsg-0ubuntu0.22.04.1) over (2.5.17+dfsg-0ubuntu0.22.04.1) ...\n","Preparing to unpack .../11-linux-libc-dev_5.15.0-140.150_amd64.deb ...\n","Unpacking linux-libc-dev:amd64 (5.15.0-140.150) over (5.15.0-113.123) ...\n","Preparing to unpack .../12-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n","Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.2) ...\n","Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n","Setting up cuda-toolkit-config-common (12.9.37-1) ...\n","Setting up binutils-common:amd64 (2.38-4ubuntu2.8) ...\n","Setting up linux-libc-dev:amd64 (5.15.0-140.150) ...\n","Setting up libctf-nobfd0:amd64 (2.38-4ubuntu2.8) ...\n","Setting up perl-modules-5.34 (5.34.0-3ubuntu1.4) ...\n","Setting up libldap-2.5-0:amd64 (2.5.19+dfsg-0ubuntu0.22.04.1) ...\n","Setting up libss2:amd64 (1.46.5-2ubuntu1.2) ...\n","Setting up logsave (1.46.5-2ubuntu1.2) ...\n","Setting up libbinutils:amd64 (2.38-4ubuntu2.8) ...\n","Setting up openssl (3.0.2-0ubuntu1.19) ...\n","Setting up cuda-toolkit-12-config-common (12.9.37-1) ...\n","Setting up libctf0:amd64 (2.38-4ubuntu2.8) ...\n","Setting up libperl5.34:amd64 (5.34.0-3ubuntu1.4) ...\n","Setting up e2fsprogs (1.46.5-2ubuntu1.2) ...\n","Setting up perl (5.34.0-3ubuntu1.4) ...\n","Setting up binutils-x86-64-linux-gnu (2.38-4ubuntu2.8) ...\n","Setting up binutils (2.38-4ubuntu2.8) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.10) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n"]}],"source":["#Install/Upgrade Git\n","!apt-get install git\n","!apt upgrade git"]},{"cell_type":"markdown","metadata":{"id":"BIXMgN0jVTDp"},"source":["### Clone project from Github\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4934,"status":"ok","timestamp":1748809727490,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"GlWFu5IEiZHW","outputId":"c0f75c4e-566e-4079-dfdc-630b6ae39b8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into '6D-pose-estimation'...\n","remote: Enumerating objects: 155, done.\u001b[K\n","remote: Counting objects: 100% (36/36), done.\u001b[K\n","remote: Compressing objects: 100% (25/25), done.\u001b[K\n","remote: Total 155 (delta 7), reused 32 (delta 5), pack-reused 119 (from 2)\u001b[K\n","Receiving objects: 100% (155/155), 121.76 MiB | 41.59 MiB/s, done.\n","Resolving deltas: 100% (27/27), done.\n","/content/6D-pose-estimation\n"]}],"source":["!git clone https://github.com/mldl-team/6D-pose-estimation.git\n","%cd 6D-pose-estimation"]},{"cell_type":"markdown","metadata":{"id":"cjH76VX3BTow"},"source":["### Login"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"o2MnT90rh5SV","executionInfo":{"status":"ok","timestamp":1748825840080,"user_tz":-120,"elapsed":300,"user":{"displayName":"ml6d","userId":"04537147647663067774"}}},"outputs":[],"source":["#Config Git Before Push\n","!git config --global user.email \"erfan.alerom@gmail.com\"\n","!git config --global user.name \"erythm\"\n","\n","#Authentication\n","!git remote set-url origin https://erythm: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@github.com/mldl-team/6D-pose-estimation.git"]},{"cell_type":"markdown","metadata":{"id":"IRP4-CsXVbZL"},"source":["### Copy modified code into the project dir"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":512,"status":"ok","timestamp":1748825832285,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"MLzANWzrVLv_","outputId":"1cbde518-9088-47f2-b28d-473fa145093b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/6D-pose-estimation\n"]}],"source":["%cd /content/6D-pose-estimation\n","!cp -r \"/content/drive/MyDrive/Colab Notebooks/6D-Pose-Estimation.ipynb\" \"/content/6D-pose-estimation\""]},{"cell_type":"markdown","source":["### Push"],"metadata":{"id":"FYynj1jMhvzv"}},{"cell_type":"code","source":["!cd /content/6D-pose-estimation\n","!git add 6D-Pose-Estimation.ipynb\n","!git commit -m \"\"\n","!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oa81mstghGVS","executionInfo":{"status":"ok","timestamp":1748446285504,"user_tz":-120,"elapsed":1716,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"e05b4ab0-2f54-4fc5-d7eb-1527a9491aaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[sina 33d4288] Add 6D Pose Estimation notebook\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite 6D-Pose-Estimation.ipynb (95%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 8 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 33.23 KiB | 1.95 MiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\n","To https://github.com/mldl-team/6D-pose-estimation.git\n","   a87cfb1..33d4288  sina -> sina\n"]}]},{"cell_type":"markdown","metadata":{"id":"ja4g4pbfWAOK"},"source":["# **Configure Python**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2930,"status":"ok","timestamp":1748809742064,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"tf4W2glvWGIg","outputId":"4daf91b3-b083-478c-cb89-e2fbce8e91a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n","Collecting jedi>=0.16 (from ipython)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jedi\n","Successfully installed jedi-0.19.2\n"]}],"source":["!pip install ipython\n","\n","#Usage of the library is to display input or output images\n","from IPython.display import Image, display"]},{"cell_type":"markdown","metadata":{"id":"VpvHIshEiF5O"},"source":["# **Configure PyTorch**"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"collapsed":true,"executionInfo":{"elapsed":69280,"status":"ok","timestamp":1748809814405,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"GL955jklWLg6","outputId":"130142b5-f5bc-4072-a2ca-fc73e4e882ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]},{"output_type":"execute_result","data":{"text/plain":["'2.6.0+cu124'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["#Install PyTorch\n","!pip install torch\n","!pip install torch torchvision opencv-python matplotlib tqdm pyyaml\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","\n","#Usage of the library is the primary neural network computation framework.\n","import torch\n","torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"eI7EMHExgdOs"},"source":["# **Configure YOLO**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3824,"status":"ok","timestamp":1748346779889,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"mkcXvJosWJG7","outputId":"334e3a3f-1efd-4684-9e96-969818a898f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.145-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.145-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.145 ultralytics-thop-2.0.14\n","/bin/bash: -c: line 2: syntax error: unexpected end of file\n","Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["#Usage of the library is To run Object Detection & Image Classification with YOLO\n","!pip install ultralytics\n","\n","#Check availability of ultralytics\n","!ultralytics.checks()\n","\n","import ultralytics\n","\n","from ultralytics import YOLO"]},{"cell_type":"markdown","metadata":{"id":"cxJlZbFTlShT"},"source":["# **Configure OpenCV**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2143,"status":"ok","timestamp":1748793105795,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"dCEtKW0eTrST","outputId":"85ab1df0-96c9-4a7c-ee0e-7c2a4968a19d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"]}],"source":["#Usage of the library is for real‐time image and video processing\n","!pip install opencv-python\n","\n","import cv2"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":142,"status":"ok","timestamp":1748793171850,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"zvAaOEWyWNu4","outputId":"1e703ad9-6128-435b-98ac-6c141a437736"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jun  1 15:52:50 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["#Check if we have an access to nvidia\n","#If \"/bin/bash: line 1: nvidia-smi: command not found\" appeared, change Runtime to GPU\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"IiEXNi0d1krR"},"source":["# **Configure Open3D**\n","### **Open3D is used for 3D geometry processing in Pose Estimation, SciPy for scientific computations**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":30635,"status":"ok","timestamp":1748809854571,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"6V3FOZ9H0pF8","outputId":"2acf51c6-7afe-400c-a2a4-2abdc9588dac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting open3d\n","  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Collecting dash>=2.6.0 (from open3d)\n","  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n","Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n","Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n","Collecting configargparse (from open3d)\n","  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n","Collecting ipywidgets>=8.0.4 (from open3d)\n","  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n","Collecting addict (from open3d)\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.2.1)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n","Collecting pyquaternion (from open3d)\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n","Collecting flask>=3.0.0 (from open3d)\n","  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n","Collecting werkzeug>=3.0.0 (from open3d)\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.13.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n","Collecting retrying (from dash>=2.6.0->open3d)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.2.1)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n","Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n","Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n","  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.25.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.4.26)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n","Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n","Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Installing collected packages: addict, widgetsnbextension, werkzeug, retrying, pyquaternion, configargparse, comm, flask, ipywidgets, dash, open3d\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 3.1.1\n","    Uninstalling Flask-3.1.1:\n","      Successfully uninstalled Flask-3.1.1\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7.1 dash-3.0.4 flask-3.0.3 ipywidgets-8.1.7 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.14\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.0.6)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.5.21)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}],"source":["!pip install open3d scipy numpy\n","!pip install opencv-python tensorboard pyyaml scipy scikit-image\n","!pip install tensorboardX\n","\n","import open3d as o3d\n","from scipy.spatial import distance_matrix\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"hcrD4hz007gZ"},"source":["### **Manage file paths with os Library**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"OiDNB_0YWPhe","executionInfo":{"status":"ok","timestamp":1748809854590,"user_tz":-120,"elapsed":1,"user":{"displayName":"ml6d","userId":"04537147647663067774"}}},"outputs":[],"source":["import os\n","import glob\n","\n","#Examples\n","#img_path = os.path.join('data', 'images', img_name)\n","#images = glob.glob('data/images/*.jpg')\n","#labels = glob.glob('data/labels/*.txt')"]},{"cell_type":"markdown","metadata":{"id":"WZxMTz5eKiig"},"source":["### **Progress Bar**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2062,"status":"ok","timestamp":1748809856654,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"OvswuY3cKfa_","outputId":"f329112b-1bf8-4b75-8257-966aff47f50e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"]}],"source":["!pip install tqdm\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"BYVEIJsVY16K"},"source":["# **Object Detection**\n"]},{"cell_type":"markdown","metadata":{"id":"hXrj5tdH6FsL"},"source":["### **Import all the Tools**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":933,"status":"ok","timestamp":1747403978128,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"JqLkUNE_ZGLC","outputId":"9b398b85-7dc6-4f4a-c76d-b3a68973ea57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 21.5M/21.5M [00:00<00:00, 140MB/s] \n"]},{"data":{"text/plain":["YOLO(\n","  (model): DetectionModel(\n","    (model): Sequential(\n","      (0): Conv(\n","        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (1): Conv(\n","        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (2): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (3): Conv(\n","        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (4): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0-1): 2 x Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (5): Conv(\n","        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (6): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0-1): 2 x Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (7): Conv(\n","        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (8): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (9): SPPF(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n","      )\n","      (10): Upsample(scale_factor=2.0, mode='nearest')\n","      (11): Concat()\n","      (12): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (13): Upsample(scale_factor=2.0, mode='nearest')\n","      (14): Concat()\n","      (15): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (16): Conv(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (17): Concat()\n","      (18): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (19): Conv(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (20): Concat()\n","      (21): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (22): Detect(\n","        (cv2): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (cv3): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (dfl): DFL(\n","          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","  )\n",")"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","\n","#Choose YOLO Model\n","model = YOLO('yolov8s.pt')\n","#Choose Computing Device - CUDA: Compute Unified Device Architecture\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","#Transfer Model to Computing Device\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"00BexPmO6J9N"},"source":["### **Make Dataset Ready**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":188095,"status":"ok","timestamp":1747412607059,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"LX6Kpu2tdb9r","outputId":"5b28b097-5d56-47a3-9541-de8961c1ce99"},"outputs":[{"name":"stdout","output_type":"stream","text":["🔍 Class distribution:\n","Class  1: 1236 samples\n","Class  2: 1214 samples\n","Class  4: 1201 samples\n","Class  5: 1196 samples\n","Class  6: 1179 samples\n","Class  8: 1188 samples\n","Class  9: 1254 samples\n","Class 10: 1253 samples\n","Class 11: 1220 samples\n","Class 12: 1237 samples\n","Class 13: 1152 samples\n","Class 14: 1227 samples\n","Class 15: 1243 samples\n","\n","⚙️ Processing and splitting images...\n","    Train Count  Validation Count\n","0           988               248\n","1           971               243\n","2           960               241\n","3           956               240\n","4           943               236\n","5           950               238\n","6          1003               251\n","7          1002               251\n","8           976               244\n","9           989               248\n","10          921               231\n","11          981               246\n","12          994               249\n","✅ YOLO dataset generated, split, and labeled.\n"]}],"source":["import os\n","import yaml\n","import random\n","import shutil\n","from PIL import Image\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","\n","#  CONFIGURATION\n","!mkdir /content/dataset/linemod/Linemod_ready\n","root_dir = '/content/dataset/Linemod_preprocessed/data'\n","output_base = '/content/dataset/Linemod_ready'  # Base output folder\n","img_out_train = os.path.join(output_base, 'images/train')\n","img_out_val = os.path.join(output_base, 'images/val')\n","label_out_train = os.path.join(output_base, 'labels/train')\n","label_out_val = os.path.join(output_base, 'labels/val')\n","\n","# Create directories if they don't exist\n","for path in [img_out_train, img_out_val, label_out_train, label_out_val]:\n","    os.makedirs(path, exist_ok=True)\n","\n","# Define valid class IDs and their YOLO-mapped indices (0-based)\n","existing_classes = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n","#Create {cls_id: idx}: {0:1} - {1:2} - {2:4} - {3:5} ...\n","class_map = {cls_id: idx for idx, cls_id in enumerate(existing_classes)}\n","\n","# ================ STEP 1: GATHER SAMPLES ===================\n","samples_by_class = defaultdict(list)\n","for cls in existing_classes:\n","    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n","    rgb_folder = os.path.join(folder_path, 'rgb')\n","    gt_file = os.path.join(folder_path, 'gt.yml')\n","\n","    if not os.path.exists(gt_file):\n","        print(f\"⚠️ Class {cls:02d} missing → skipped.\")\n","        continue\n","\n","    with open(gt_file, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    for img_id in gt_data:\n","        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n","        if os.path.exists(img_path):\n","            samples_by_class[cls].append(img_id)\n","\n","# Display class distribution\n","print(\"🔍 Class distribution:\")\n","for cls, samples in samples_by_class.items():\n","    print(f\"Class {cls:>2}: {len(samples)} samples\")\n","\n","# =============== STEP 2: LABEL AND IMAGE PROCESSING =================\n","def save_yolo_labels(sample_list, mode, cls, gt_data, rgb_folder):\n","    label_dir = label_out_train if mode == 'train' else label_out_val\n","    img_dir = img_out_train if mode == 'train' else img_out_val\n","    label_id = class_map[cls]\n","\n","    for img_id in sample_list:\n","        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n","        img = Image.open(img_path)\n","        w, h = img.size\n","\n","        bbox = gt_data[img_id][0]['obj_bb']\n","        x, y, bw, bh = bbox\n","        x_center = (x + bw / 2) / w\n","        y_center = (y + bh / 2) / h\n","        norm_bw = bw / w\n","        norm_bh = bh / h\n","\n","        # Save label\n","        label_file = f\"{cls:02d}_{int(img_id):04d}.txt\"\n","        with open(os.path.join(label_dir, label_file), 'w') as f:\n","            f.write(f\"{label_id} {x_center:.6f} {y_center:.6f} {norm_bw:.6f} {norm_bh:.6f}\\n\")\n","\n","        # Save image\n","        img_out_path = os.path.join(img_dir, f\"{cls:02d}_{int(img_id):04d}.png\")\n","        shutil.copy(img_path, img_out_path)\n","\n","# Split, save labels and copy images\n","print(\"\\n⚙️ Processing and splitting images...\")\n","for cls, samples in samples_by_class.items():\n","    random.shuffle(samples)\n","    split_idx = int(0.8 * len(samples))\n","    train_samples = samples[:split_idx]\n","    val_samples = samples[split_idx:]\n","\n","    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n","    rgb_folder = os.path.join(folder_path, 'rgb')\n","    gt_file = os.path.join(folder_path, 'gt.yml')\n","\n","    with open(gt_file, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    save_yolo_labels(train_samples, 'train', cls, gt_data, rgb_folder)\n","    save_yolo_labels(val_samples, 'val', cls, gt_data, rgb_folder)\n","\n","# ============= STEP 3: LABEL DISTRIBUTION STATS ===============\n","def count_labels(label_dir):\n","    counter = Counter()\n","    for filename in os.listdir(label_dir):\n","        if filename.endswith('.txt'):\n","            with open(os.path.join(label_dir, filename), 'r') as f:\n","                for line in f:\n","                    class_id = int(line.strip().split()[0])\n","                    counter[class_id] += 1\n","    return counter\n","\n","train_counts = count_labels(label_out_train)\n","val_counts = count_labels(label_out_val)\n","\n","import pandas as pd\n","df_stats = pd.DataFrame({\n","    \"Train Count\": pd.Series(train_counts),\n","    \"Validation Count\": pd.Series(val_counts)\n","}).fillna(0).astype(int)\n","\n","print(df_stats)\n","\n","print(\"✅ YOLO dataset generated, split, and labeled.\")"]},{"cell_type":"markdown","metadata":{"id":"Y5UQxMsZ6RT2"},"source":[]},{"cell_type":"markdown","metadata":{"id":"raqAJg_s6Y8D"},"source":["### **Train the Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWdLRQ-qyHUQ"},"outputs":[],"source":["model.train(\n","    data='/content/project/6D-pose-estimation/configs/linemod_final.yaml',\n","    epochs=15,\n","    imgsz=640,\n","    batch=8,\n","    device=0,\n","    patience=5,\n","    weight_decay=0.0005\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXhj_4zwy4Gt"},"outputs":[],"source":["!cp -r runs/detect/train ~/6d/yolo_logs/"]},{"cell_type":"markdown","metadata":{"id":"ZACuf_m66sGQ"},"source":["### **Save Trained Model Metrics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YD2UGsFky-pW"},"outputs":[],"source":["from ultralytics import YOLO\n","\n","model = YOLO(\"model.pt\")\n","metrics = model.val(data=\"linemod_final.yaml\", plots=True, save=True)"]},{"cell_type":"markdown","metadata":{"id":"Z05pObtQAvQb"},"source":["# **Pose Estimation**"]},{"cell_type":"markdown","metadata":{"id":"NLw9Lzbazk4D"},"source":["### **Prepare File & Folders for RCVPose**"]},{"cell_type":"markdown","metadata":{"id":"DingMrNW0bPZ"},"source":["### 1 - Move Objects' Models From models Folder To Class Folders & Rename Them To mesh.ply"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1748809906863,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"0n9I3pMgxT0u","outputId":"e81d0b04-1ab6-4f94-af1b-2c5951c1b9e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Copying obj_14.ply → /content/dataset/Linemod_preprocessed/data/14/mesh.ply\n","Copying obj_13.ply → /content/dataset/Linemod_preprocessed/data/13/mesh.ply\n","Copying obj_06.ply → /content/dataset/Linemod_preprocessed/data/06/mesh.ply\n","Folder 03 does not exist → skipped\n","Copying obj_01.ply → /content/dataset/Linemod_preprocessed/data/01/mesh.ply\n","Copying obj_12.ply → /content/dataset/Linemod_preprocessed/data/12/mesh.ply\n","Copying obj_05.ply → /content/dataset/Linemod_preprocessed/data/05/mesh.ply\n","Copying obj_11.ply → /content/dataset/Linemod_preprocessed/data/11/mesh.ply\n","Copying obj_04.ply → /content/dataset/Linemod_preprocessed/data/04/mesh.ply\n","Copying obj_02.ply → /content/dataset/Linemod_preprocessed/data/02/mesh.ply\n","Folder 07 does not exist → skipped\n","Copying obj_08.ply → /content/dataset/Linemod_preprocessed/data/08/mesh.ply\n","Copying obj_15.ply → /content/dataset/Linemod_preprocessed/data/15/mesh.ply\n","Copying obj_10.ply → /content/dataset/Linemod_preprocessed/data/10/mesh.ply\n","Copying obj_09.ply → /content/dataset/Linemod_preprocessed/data/09/mesh.ply\n","\n"," Model files have been moved and renamed to mesh.ply.\n"]}],"source":["import os\n","import shutil\n","\n","data_path = \"/content/dataset/Linemod_preprocessed/data\"\n","models_path = \"/content/dataset/Linemod_preprocessed/models\"\n","\n","for filename in os.listdir(models_path):\n","    # Only process files matching the pattern obj_<id>.ply\n","    if not filename.startswith(\"obj_\") or not filename.endswith(\".ply\"):\n","        continue\n","\n","    # Extract the model identifier (e.g., \"01\")\n","    model_id = filename.split(\"_\")[1].split(\".\")[0]  # example: \"01\"\n","    src_file = os.path.join(models_path, filename)\n","    dst_folder = os.path.join(data_path, model_id)\n","\n","    # Skip if the destination folder does not exist(3,7)\n","    if not os.path.isdir(dst_folder):\n","        print(f\"Folder {model_id} does not exist → skipped\")\n","        continue\n","\n","    # Copy and rename the model file to mesh.ply\n","    dst_file = os.path.join(dst_folder, \"mesh.ply\")\n","    print(f\"Copying {filename} → {dst_file}\")\n","    shutil.copy2(src_file, dst_file)\n","\n","print(\"\\n Model files have been moved and renamed to mesh.ply.\")"]},{"cell_type":"markdown","metadata":{"id":"vunah5M8cYW-"},"source":["### 2 - Create Outside9.npy From Objects' Models\n","A NumPy file containing a [9, 3] array of 3D keypoint coordinates sampled from the object mesh using Farthest-Point Sampling. These five points are chosen to maximally span the surface of the model. During data preprocessing, each keypoint is used to generate a per-frame radial distance map: for every pixel with valid depth, its Euclidean distance in 3D space to each keypoint is computed and stored. These distance maps serve as ground-truth supervision when training a network to predict 3D distance fields from RGB-D inputs.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":505,"status":"ok","timestamp":1748809915305,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"g0L1ky8r2svp","outputId":"72c9fc81-02f0-436a-ff8d-ec125742da31"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Processing class 01\n"," Saved /content/dataset/Linemod_preprocessed/data/01/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/01/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 02\n"," Saved /content/dataset/Linemod_preprocessed/data/02/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/02/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 04\n"," Saved /content/dataset/Linemod_preprocessed/data/04/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/04/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 05\n"," Saved /content/dataset/Linemod_preprocessed/data/05/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/05/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 06\n"," Saved /content/dataset/Linemod_preprocessed/data/06/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/06/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 08\n"," Saved /content/dataset/Linemod_preprocessed/data/08/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/08/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 09\n"," Saved /content/dataset/Linemod_preprocessed/data/09/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/09/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 10\n"," Saved /content/dataset/Linemod_preprocessed/data/10/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/10/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 11\n"," Saved /content/dataset/Linemod_preprocessed/data/11/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/11/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 12\n"," Saved /content/dataset/Linemod_preprocessed/data/12/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/12/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 13\n"," Saved /content/dataset/Linemod_preprocessed/data/13/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/13/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 14\n"," Saved /content/dataset/Linemod_preprocessed/data/14/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/14/Outside9.npy loaded successfully with shape (9, 3)\n","\n"," Processing class 15\n"," Saved /content/dataset/Linemod_preprocessed/data/15/Outside9.npy (shape (9, 3))\n"," Verified file: /content/dataset/Linemod_preprocessed/data/15/Outside9.npy loaded successfully with shape (9, 3)\n","\n"]}],"source":["import os\n","import numpy as np\n","import open3d as o3d\n","\n","def fps(points: np.ndarray, k: int, seed: int = 0) -> np.ndarray:\n","    \"\"\"\n","    Farthest-Point Sampling:\n","      - points: (N,3) array of XYZ samples\n","      - k: number of keypoints to pick\n","    Returns an array of shape (k,3).\n","    \"\"\"\n","    np.random.seed(seed)\n","    N = points.shape[0]\n","    centroids = np.zeros((k,), dtype=np.int32)\n","    distances = np.full((N,), np.inf)\n","    farthest = np.random.randint(0, N)\n","    for i in range(k):\n","        centroids[i] = farthest\n","        centroid = points[farthest]\n","        dist = np.sum((points - centroid)**2, axis=1)\n","        distances = np.minimum(distances, dist)\n","        farthest = np.argmax(distances)\n","    return points[centroids]\n","\n","classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","for cls in classes:\n","    seq_dir = os.path.join(data_path, cls)\n","    mesh_path = os.path.join(seq_dir, \"mesh.ply\")\n","\n","    if not os.path.isfile(mesh_path):\n","        print(f\"  Skipping {cls}: mesh.ply not found at {mesh_path}\")\n","        continue\n","\n","    print(f\" Processing class {cls}\")\n","\n","    # Load point cloud\n","    pcd = o3d.io.read_point_cloud(mesh_path)\n","    pts = np.asarray(pcd.points)\n","\n","    if pts.shape[0] < 9:\n","        print(f\" Not enough points in {mesh_path} (only {pts.shape[0]}), skipping.\")\n","        continue\n","\n","    # Perform Farthest-Point Sampling\n","    keypoints = fps(pts, k=9, seed=42)\n","\n","    # Save output\n","    out_path = os.path.join(seq_dir, \"Outside9.npy\")\n","    np.save(out_path, keypoints)\n","    print(f\" Saved {out_path} (shape {keypoints.shape})\")\n","\n","    # Test load to confirm file is valid\n","    try:\n","        test = np.load(out_path)\n","        print(f\" Verified file: {out_path} loaded successfully with shape {test.shape}\\n\")\n","    except Exception as e:\n","        print(f\" Failed to load saved file: {e}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"AXbUbafiiMFD"},"source":["### It is only Possible to see the content of Outside9.npy using code below"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1748809920704,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"rrrof0ashbQ9","outputId":"e7c0a906-6452-4e02-cf63-39348669bc30","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Class 01 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/01/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -44.2630\n"," ├─ Max  : 41.4566\n"," ├─ Mean : -3.6767\n"," ├─ Std  : 23.3938\n"," └─ Sample Data:\n","[[-20.3664  19.3442  -7.6361]\n"," [ 35.6255 -16.9712 -44.263 ]\n"," [  7.5111  -4.4303  41.4566]\n"," [ -1.604  -33.9459  -5.4867]\n"," [-26.6571 -13.9838 -43.0416]\n"," [ 10.2731  20.1762 -40.7712]\n"," [ 13.9023  17.8718   9.8713]\n"," [-20.7606  -9.8099  16.1704]\n"," [ 30.0282 -21.3168 -10.4571]]\n","\n"," Class 02 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/02/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -107.0920\n"," ├─ Max  : 107.3880\n"," ├─ Mean : 0.5087\n"," ├─ Std  : 54.8957\n"," └─ Sample Data:\n","[[  12.3025  -17.8971   43.9721]\n"," [  90.5719   -1.9537 -107.092 ]\n"," [ -70.8535   22.383   -92.6023]\n"," [  53.2234   58.4881  -23.509 ]\n"," [   7.5829  -29.9273  -56.5234]\n"," [ -92.0255   -4.1247   -3.2999]\n"," [  66.5464    0.135   107.388 ]\n"," [  86.4643  -24.3331  -25.008 ]\n"," [ -11.3591   30.5359   -5.3499]]\n","\n"," Class 04 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/04/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -67.9932\n"," ├─ Max  : 67.0034\n"," ├─ Mean : -11.1607\n"," ├─ Std  : 39.5965\n"," └─ Sample Data:\n","[[-60.7914  62.0638 -19.3466]\n"," [ 67.0034 -10.3907  -2.8697]\n"," [-54.8219 -67.9932  19.5177]\n"," [-10.7869  13.9897  49.5203]\n"," [-19.0693 -15.506  -47.6997]\n"," [ 17.4433  49.3472 -20.1936]\n"," [-66.316   -0.8159   7.7016]\n"," [  4.4803 -41.5652  16.038 ]\n"," [-56.6145 -67.7992 -45.8652]]\n","\n"," Class 05 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/05/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -93.7359\n"," ├─ Max  : 96.3970\n"," ├─ Mean : -3.6729\n"," ├─ Std  : 47.5687\n"," └─ Sample Data:\n","[[ -8.0273  55.7049 -79.5385]\n"," [  1.2705   6.302   96.397 ]\n"," [ -6.0933 -70.6847 -20.2746]\n"," [ 47.7824  17.2083   1.3454]\n"," [-48.5596  10.6845  13.091 ]\n"," [ 35.0964 -27.4034 -93.7359]\n"," [  2.8314  88.323   -1.3848]\n"," [-42.4456 -17.0875 -69.874 ]\n"," [ 45.2718  25.511  -60.8797]]\n","\n"," Class 06 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/06/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -61.5181\n"," ├─ Max  : 55.2023\n"," ├─ Mean : -5.8993\n"," ├─ Std  : 34.8858\n"," └─ Sample Data:\n","[[ -9.8392  50.4696  31.5456]\n"," [ 21.0611 -61.5181 -54.2087]\n"," [-27.4429  20.7841 -56.9346]\n"," [  1.3448 -54.5936  25.5383]\n"," [ 18.5875   3.4214  -9.0781]\n"," [-32.6254 -37.7601 -54.0536]\n"," [ 27.5001  27.813  -57.959 ]\n"," [ 12.0627   7.1267  55.2023]\n"," [-26.0997   4.744   15.631 ]]\n","\n"," Class 08 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/08/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -102.5330\n"," ├─ Max  : 114.7380\n"," ├─ Mean : -1.7252\n"," ├─ Std  : 57.1376\n"," └─ Sample Data:\n","[[ -85.1721   27.1747   55.3571]\n"," [  41.817   -21.577  -102.533 ]\n"," [ 114.738    -3.9728   89.5442]\n"," [  18.887    -2.2339   10.7861]\n"," [ -64.6352    9.4542  -96.3116]\n"," [  12.7168   -1.2401  101.553 ]\n"," [ -52.2799   -5.5599  -20.5459]\n"," [ -37.3041  -31.165    53.1621]\n"," [   1.973    36.0232  -95.2365]]\n","\n"," Class 09 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/09/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -46.6694\n"," ├─ Max  : 46.7280\n"," ├─ Mean : -4.6406\n"," ├─ Std  : 27.9066\n"," └─ Sample Data:\n","[[ -2.0994  11.7574   0.0198]\n"," [ 46.728  -13.5908 -40.1141]\n"," [-35.4824 -23.6476 -38.1239]\n"," [ 44.0857  -6.8009  20.9951]\n"," [ 21.6419  31.9062 -40.411 ]\n"," [-30.3699  26.0363 -38.8646]\n"," [  7.4866 -33.6071 -20.1031]\n"," [  1.8624  -9.9197  40.7914]\n"," [-46.6694   3.9089  -2.7115]]\n","\n"," Class 10 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/10/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -73.4333\n"," ├─ Max  : 63.0991\n"," ├─ Mean : -0.9485\n"," ├─ Std  : 36.2279\n"," └─ Sample Data:\n","[[ 39.1349  11.601  -23.9408]\n"," [-73.4333 -30.9207  -0.2043]\n"," [-29.2001  45.2791  29.0616]\n"," [  4.209  -42.7445  30.9551]\n"," [-56.7928  29.7865 -33.7575]\n"," [ 63.0991 -46.6746  -1.2703]\n"," [-14.9766 -26.8483 -32.3674]\n"," [ 44.4272  40.0844  33.1058]\n"," [ -5.5014  42.452  -20.1714]]\n","\n"," Class 11 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/11/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -84.7123\n"," ├─ Max  : 85.9639\n"," ├─ Mean : -6.8638\n"," ├─ Std  : 34.8670\n"," └─ Sample Data:\n","[[ 15.494  -15.955   -7.3843]\n"," [  0.3111   5.3904  85.9639]\n"," [-12.4399  31.7775 -84.7123]\n"," [  7.3067 -37.177  -76.2814]\n"," [-11.4441  34.2681 -24.9297]\n"," [-10.3177  14.479   31.7019]\n"," [ 16.6566   7.0834 -54.5396]\n"," [-15.0613 -32.8293 -36.4945]\n"," [-11.6849 -26.7442  22.2407]]\n","\n"," Class 12 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/12/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -46.0149\n"," ├─ Max  : 49.0182\n"," ├─ Mean : -1.7436\n"," ├─ Std  : 34.0428\n"," └─ Sample Data:\n","[[  1.2851 -26.6466  13.895 ]\n"," [ 44.2851  48.1457 -37.705 ]\n"," [-43.2355  29.6682 -42.005 ]\n"," [ 42.1351  36.1182  40.9671]\n"," [-42.4672 -41.2818 -39.855 ]\n"," [ 48.7106 -21.9318 -37.705 ]\n"," [-15.9149  49.0182  13.9386]\n"," [-46.0149  -4.7318  -0.097 ]\n"," [  5.5851  10.3182 -31.5558]]\n","\n"," Class 13 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/13/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -127.0280\n"," ├─ Max  : 127.4950\n"," ├─ Mean : -7.1318\n"," ├─ Std  : 59.5631\n"," └─ Sample Data:\n","[[ -11.3506  -37.9226  -11.2949]\n"," [ 127.495    -0.2632  -67.9833]\n"," [-127.028    44.9615  -37.3693]\n"," [-107.902    -1.4071   69.7612]\n"," [  80.0632    0.7081   38.3604]\n"," [  28.8828   49.0412  -65.602 ]\n"," [ -99.7896  -47.943   -50.6508]\n"," [ -42.0713   43.7073  -13.5953]\n"," [ -21.6835    5.7878   62.5302]]\n","\n"," Class 14 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/14/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -101.7510\n"," ├─ Max  : 101.2500\n"," ├─ Mean : -6.5086\n"," ├─ Std  : 62.7174\n"," └─ Sample Data:\n","[[  43.2177   52.4112   54.0458]\n"," [ -99.9506  -36.8233 -101.751 ]\n"," [  32.046    34.081   -98.0899]\n"," [ -70.1905  -24.798    76.0068]\n"," [  82.3728  -56.4652   23.6938]\n"," [  -5.7534  -11.7776  -12.261 ]\n"," [ -63.4212   52.9445  -85.1847]\n"," [ 101.25     -3.7718   92.8678]\n"," [ -11.3597  -43.5055  -95.5663]]\n","\n"," Class 15 — Outside9.npy\n"," ├─ Path : /content/dataset/Linemod_preprocessed/data/15/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -87.9908\n"," ├─ Max  : 91.5867\n"," ├─ Mean : -14.2489\n"," ├─ Std  : 49.5074\n"," └─ Sample Data:\n","[[-33.3298 -51.9169 -81.7832]\n"," [-10.1223 -27.1229  91.5867]\n"," [ -3.8379  72.9849 -24.3855]\n"," [ 46.3641   7.2564 -79.3277]\n"," [ 10.0339 -60.7272   3.6516]\n"," [-37.3104  31.5644 -87.9908]\n"," [-39.8922   0.1733 -13.9607]\n"," [ 36.755  -63.5682 -71.5871]\n"," [ 19.6582  64.2833 -82.1688]]\n","\n"," All classes processed.\n"]}],"source":["# Ensure NumPy will print the full array\n","np.set_printoptions(precision=4, suppress=True, threshold=np.inf)\n","\n","for cls in classes:\n","    file_path = os.path.join(data_path, cls, \"Outside9.npy\")\n","    if not os.path.isfile(file_path):\n","        print(f\"  File not found for class {cls}: {file_path}\")\n","        continue\n","\n","    data = np.load(file_path)\n","    flat = data.flatten()\n","    # Compute some summary statistics\n","    shape   = data.shape\n","    dtype   = data.dtype\n","    minimum = flat.min()\n","    maximum = flat.max()\n","    mean    = flat.mean()\n","    std     = flat.std()\n","\n","    # Nicely formatted output\n","    print(f\"\\n Class {cls} — Outside9.npy\")\n","    print(f\" ├─ Path : {file_path}\")\n","    print(f\" ├─ Shape: {shape}\")\n","    print(f\" ├─ DType: {dtype}\")\n","    print(f\" ├─ Min  : {minimum:.4f}\")\n","    print(f\" ├─ Max  : {maximum:.4f}\")\n","    print(f\" ├─ Mean : {mean:.4f}\")\n","    print(f\" ├─ Std  : {std:.4f}\")\n","    # Print the full matrix\n","    print(\" └─ Sample Data:\")\n","    print(data)\n","\n","print(\"\\n All classes processed.\")"]},{"cell_type":"markdown","metadata":{"id":"PDddF5Sy-XbT"},"source":["### **3 - Generate Pose For Each Picture Using gt.yml**\n","\n","Purpose: This script generates pose files (Rotation & Translation matrices) for each image in the dataset.\n","\n","How it works:\n","1. Reads the depth image and removes the background using the mask.\n","2. Converts the depth into a point cloud (scene point cloud).\n","3. Loads the 3D model (`mesh.ply`) and samples it into a point cloud.\n","4. Uses ICP to align the model point cloud with the scene point cloud.\n","5. Saves the resulting [R|t] matrix as a `.npy` file named `poseXXXXXX.npy`.\n","\n","Output:\n","Each frame will have a file like `pose000123.npy` in the `pose/` folder.\n","It contains a 3×4 RT matrix with:\n","- R: rotation (3×3)\n","- t: translation (3×1)\n","\n","Use in RCVPose:\n","These pose files are used to:\n","- Project 3D keypoints onto 2D image space\n","- Generate 3D radius maps\n","- Serve as ground truth during training\n","\n","Requirements:\n","- `.dpt` depth images\n","- `.png` binary masks\n","- 3D model in `.ply` format\n","- Camera intrinsics `K`\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25017,"status":"ok","timestamp":1748809954427,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"Pj1gZdE7-9j4","outputId":"4e32d90d-7b0b-431a-b754-e44f1ad52555","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Processing class 01...\n"]},{"output_type":"stream","name":"stderr","text":["Class 01: 100%|██████████| 1236/1236 [00:00<00:00, 11372.56image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 02...\n"]},{"output_type":"stream","name":"stderr","text":["Class 02: 100%|██████████| 1214/1214 [00:00<00:00, 9925.76image/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Multiple objects in image 000000, using object 2.\n"," Multiple objects in image 000001, using object 2.\n"," Multiple objects in image 000002, using object 2.\n"," Multiple objects in image 000003, using object 2.\n"," Multiple objects in image 000004, using object 2.\n"," Multiple objects in image 000005, using object 2.\n"," Multiple objects in image 000006, using object 2.\n"," Multiple objects in image 000007, using object 2.\n"," Multiple objects in image 000008, using object 2.\n"," Multiple objects in image 000009, using object 2.\n"," Multiple objects in image 000010, using object 2.\n"," Multiple objects in image 000011, using object 2.\n"," Multiple objects in image 000012, using object 2.\n"," Multiple objects in image 000013, using object 2.\n"," Multiple objects in image 000014, using object 2.\n"," Multiple objects in image 000015, using object 2.\n"," Multiple objects in image 000016, using object 2.\n"," Multiple objects in image 000017, using object 2.\n"," Multiple objects in image 000018, using object 2.\n"," Multiple objects in image 000019, using object 2.\n"," Multiple objects in image 000020, using object 2.\n"," Multiple objects in image 000021, using object 2.\n"," Multiple objects in image 000022, using object 2.\n"," Multiple objects in image 000023, using object 2.\n"," Multiple objects in image 000024, using object 2.\n"," Multiple objects in image 000025, using object 2.\n"," Multiple objects in image 000026, using object 2.\n"," Multiple objects in image 000027, using object 2.\n"," Multiple objects in image 000028, using object 2.\n"," Multiple objects in image 000029, using object 2.\n"," Multiple objects in image 000030, using object 2.\n"," Multiple objects in image 000031, using object 2.\n"," Multiple objects in image 000032, using object 2.\n"," Multiple objects in image 000033, using object 2.\n"," Multiple objects in image 000034, using object 2.\n"," Multiple objects in image 000035, using object 2.\n"," Multiple objects in image 000036, using object 2.\n"," Multiple objects in image 000037, using object 2.\n"," Multiple objects in image 000038, using object 2.\n"," Multiple objects in image 000039, using object 2.\n"," Multiple objects in image 000040, using object 2.\n"," Multiple objects in image 000041, using object 2.\n"," Multiple objects in image 000042, using object 2.\n"," Multiple objects in image 000043, using object 2.\n"," Multiple objects in image 000044, using object 2.\n"," Multiple objects in image 000045, using object 2.\n"," Multiple objects in image 000046, using object 2.\n"," Multiple objects in image 000047, using object 2.\n"," Multiple objects in image 000048, using object 2.\n"," Multiple objects in image 000049, using object 2.\n"," Multiple objects in image 000050, using object 2.\n"," Multiple objects in image 000051, using object 2.\n"," Multiple objects in image 000052, using object 2.\n"," Multiple objects in image 000053, using object 2.\n"," Multiple objects in image 000054, using object 2.\n"," Multiple objects in image 000055, using object 2.\n"," Multiple objects in image 000056, using object 2.\n"," Multiple objects in image 000057, using object 2.\n"," Multiple objects in image 000058, using object 2.\n"," Multiple objects in image 000059, using object 2.\n"," Multiple objects in image 000060, using object 2.\n"," Multiple objects in image 000061, using object 2.\n"," Multiple objects in image 000062, using object 2.\n"," Multiple objects in image 000063, using object 2.\n"," Multiple objects in image 000064, using object 2.\n"," Multiple objects in image 000065, using object 2.\n"," Multiple objects in image 000066, using object 2.\n"," Multiple objects in image 000067, using object 2.\n"," Multiple objects in image 000068, using object 2.\n"," Multiple objects in image 000069, using object 2.\n"," Multiple objects in image 000070, using object 2.\n"," Multiple objects in image 000071, using object 2.\n"," Multiple objects in image 000072, using object 2.\n"," Multiple objects in image 000073, using object 2.\n"," Multiple objects in image 000074, using object 2.\n"," Multiple objects in image 000075, using object 2.\n"," Multiple objects in image 000076, using object 2.\n"," Multiple objects in image 000077, using object 2.\n"," Multiple objects in image 000078, using object 2.\n"," Multiple objects in image 000079, using object 2.\n"," Multiple objects in image 000080, using object 2.\n"," Multiple objects in image 000081, using object 2.\n"," Multiple objects in image 000082, using object 2.\n"," Multiple objects in image 000083, using object 2.\n"," Multiple objects in image 000084, using object 2.\n"," Multiple objects in image 000085, using object 2.\n"," Multiple objects in image 000086, using object 2.\n"," Multiple objects in image 000087, using object 2.\n"," Multiple objects in image 000088, using object 2.\n"," Multiple objects in image 000089, using object 2.\n"," Multiple objects in image 000090, using object 2.\n"," Multiple objects in image 000091, using object 2.\n"," Multiple objects in image 000092, using object 2.\n"," Multiple objects in image 000093, using object 2.\n"," Multiple objects in image 000094, using object 2.\n"," Multiple objects in image 000095, using object 2.\n"," Multiple objects in image 000096, using object 2.\n"," Multiple objects in image 000097, using object 2.\n"," Multiple objects in image 000098, using object 2.\n"," Multiple objects in image 000099, using object 2.\n"," Multiple objects in image 000100, using object 2.\n"," Multiple objects in image 000101, using object 2.\n"," Multiple objects in image 000102, using object 2.\n"," Multiple objects in image 000103, using object 2.\n"," Multiple objects in image 000104, using object 2.\n"," Multiple objects in image 000105, using object 2.\n"," Multiple objects in image 000106, using object 2.\n"," Multiple objects in image 000107, using object 2.\n"," Multiple objects in image 000108, using object 2.\n"," Multiple objects in image 000109, using object 2.\n"," Multiple objects in image 000110, using object 2.\n"," Multiple objects in image 000111, using object 2.\n"," Multiple objects in image 000112, using object 2.\n"," Multiple objects in image 000113, using object 2.\n"," Multiple objects in image 000114, using object 2.\n"," Multiple objects in image 000115, using object 2.\n"," Multiple objects in image 000116, using object 2.\n"," Multiple objects in image 000117, using object 2.\n"," Multiple objects in image 000118, using object 2.\n"," Multiple objects in image 000119, using object 2.\n"," Multiple objects in image 000120, using object 2.\n"," Multiple objects in image 000121, using object 2.\n"," Multiple objects in image 000122, using object 2.\n"," Multiple objects in image 000123, using object 2.\n"," Multiple objects in image 000124, using object 2.\n"," Multiple objects in image 000125, using object 2.\n"," Multiple objects in image 000126, using object 2.\n"," Multiple objects in image 000127, using object 2.\n"," Multiple objects in image 000128, using object 2.\n"," Multiple objects in image 000129, using object 2.\n"," Multiple objects in image 000130, using object 2.\n"," Multiple objects in image 000131, using object 2.\n"," Multiple objects in image 000132, using object 2.\n"," Multiple objects in image 000133, using object 2.\n"," Multiple objects in image 000134, using object 2.\n"," Multiple objects in image 000135, using object 2.\n"," Multiple objects in image 000136, using object 2.\n"," Multiple objects in image 000137, using object 2.\n"," Multiple objects in image 000138, using object 2.\n"," Multiple objects in image 000139, using object 2.\n"," Multiple objects in image 000140, using object 2.\n"," Multiple objects in image 000141, using object 2.\n"," Multiple objects in image 000142, using object 2.\n"," Multiple objects in image 000143, using object 2.\n"," Multiple objects in image 000144, using object 2.\n"," Multiple objects in image 000145, using object 2.\n"," Multiple objects in image 000146, using object 2.\n"," Multiple objects in image 000147, using object 2.\n"," Multiple objects in image 000148, using object 2.\n"," Multiple objects in image 000149, using object 2.\n"," Multiple objects in image 000150, using object 2.\n"," Multiple objects in image 000151, using object 2.\n"," Multiple objects in image 000152, using object 2.\n"," Multiple objects in image 000153, using object 2.\n"," Multiple objects in image 000154, using object 2.\n"," Multiple objects in image 000155, using object 2.\n"," Multiple objects in image 000156, using object 2.\n"," Multiple objects in image 000157, using object 2.\n"," Multiple objects in image 000158, using object 2.\n"," Multiple objects in image 000159, using object 2.\n"," Multiple objects in image 000160, using object 2.\n"," Multiple objects in image 000161, using object 2.\n"," Multiple objects in image 000162, using object 2.\n"," Multiple objects in image 000163, using object 2.\n"," Multiple objects in image 000164, using object 2.\n"," Multiple objects in image 000165, using object 2.\n"," Multiple objects in image 000166, using object 2.\n"," Multiple objects in image 000167, using object 2.\n"," Multiple objects in image 000168, using object 2.\n"," Multiple objects in image 000169, using object 2.\n"," Multiple objects in image 000170, using object 2.\n"," Multiple objects in image 000171, using object 2.\n"," Multiple objects in image 000172, using object 2.\n"," Multiple objects in image 000173, using object 2.\n"," Multiple objects in image 000174, using object 2.\n"," Multiple objects in image 000175, using object 2.\n"," Multiple objects in image 000176, using object 2.\n"," Multiple objects in image 000177, using object 2.\n"," Multiple objects in image 000178, using object 2.\n"," Multiple objects in image 000179, using object 2.\n"," Multiple objects in image 000180, using object 2.\n"," Multiple objects in image 000181, using object 2.\n"," Multiple objects in image 000182, using object 2.\n"," Multiple objects in image 000183, using object 2.\n"," Multiple objects in image 000184, using object 2.\n"," Multiple objects in image 000185, using object 2.\n"," Multiple objects in image 000186, using object 2.\n"," Multiple objects in image 000187, using object 2.\n"," Multiple objects in image 000188, using object 2.\n"," Multiple objects in image 000189, using object 2.\n"," Multiple objects in image 000190, using object 2.\n"," Multiple objects in image 000191, using object 2.\n"," Multiple objects in image 000192, using object 2.\n"," Multiple objects in image 000193, using object 2.\n"," Multiple objects in image 000194, using object 2.\n"," Multiple objects in image 000195, using object 2.\n"," Multiple objects in image 000196, using object 2.\n"," Multiple objects in image 000197, using object 2.\n"," Multiple objects in image 000198, using object 2.\n"," Multiple objects in image 000199, using object 2.\n"," Multiple objects in image 000200, using object 2.\n"," Multiple objects in image 000201, using object 2.\n"," Multiple objects in image 000202, using object 2.\n"," Multiple objects in image 000203, using object 2.\n"," Multiple objects in image 000204, using object 2.\n"," Multiple objects in image 000205, using object 2.\n"," Multiple objects in image 000206, using object 2.\n"," Multiple objects in image 000207, using object 2.\n"," Multiple objects in image 000208, using object 2.\n"," Multiple objects in image 000209, using object 2.\n"," Multiple objects in image 000210, using object 2.\n"," Multiple objects in image 000211, using object 2.\n"," Multiple objects in image 000212, using object 2.\n"," Multiple objects in image 000213, using object 2.\n"," Multiple objects in image 000214, using object 2.\n"," Multiple objects in image 000215, using object 2.\n"," Multiple objects in image 000216, using object 2.\n"," Multiple objects in image 000217, using object 2.\n"," Multiple objects in image 000218, using object 2.\n"," Multiple objects in image 000219, using object 2.\n"," Multiple objects in image 000220, using object 2.\n"," Multiple objects in image 000221, using object 2.\n"," Multiple objects in image 000222, using object 2.\n"," Multiple objects in image 000223, using object 2.\n"," Multiple objects in image 000224, using object 2.\n"," Multiple objects in image 000225, using object 2.\n"," Multiple objects in image 000226, using object 2.\n"," Multiple objects in image 000227, using object 2.\n"," Multiple objects in image 000228, using object 2.\n"," Multiple objects in image 000229, using object 2.\n"," Multiple objects in image 000230, using object 2.\n"," Multiple objects in image 000231, using object 2.\n"," Multiple objects in image 000232, using object 2.\n"," Multiple objects in image 000233, using object 2.\n"," Multiple objects in image 000234, using object 2.\n"," Multiple objects in image 000235, using object 2.\n"," Multiple objects in image 000236, using object 2.\n"," Multiple objects in image 000237, using object 2.\n"," Multiple objects in image 000238, using object 2.\n"," Multiple objects in image 000239, using object 2.\n"," Multiple objects in image 000240, using object 2.\n"," Multiple objects in image 000241, using object 2.\n"," Multiple objects in image 000242, using object 2.\n"," Multiple objects in image 000243, using object 2.\n"," Multiple objects in image 000244, using object 2.\n"," Multiple objects in image 000245, using object 2.\n"," Multiple objects in image 000246, using object 2.\n"," Multiple objects in image 000247, using object 2.\n"," Multiple objects in image 000248, using object 2.\n"," Multiple objects in image 000249, using object 2.\n"," Multiple objects in image 000250, using object 2.\n"," Multiple objects in image 000251, using object 2.\n"," Multiple objects in image 000252, using object 2.\n"," Multiple objects in image 000253, using object 2.\n"," Multiple objects in image 000254, using object 2.\n"," Multiple objects in image 000255, using object 2.\n"," Multiple objects in image 000256, using object 2.\n"," Multiple objects in image 000257, using object 2.\n"," Multiple objects in image 000258, using object 2.\n"," Multiple objects in image 000259, using object 2.\n"," Multiple objects in image 000260, using object 2.\n"," Multiple objects in image 000261, using object 2.\n"," Multiple objects in image 000262, using object 2.\n"," Multiple objects in image 000263, using object 2.\n"," Multiple objects in image 000264, using object 2.\n"," Multiple objects in image 000265, using object 2.\n"," Multiple objects in image 000266, using object 2.\n"," Multiple objects in image 000267, using object 2.\n"," Multiple objects in image 000268, using object 2.\n"," Multiple objects in image 000269, using object 2.\n"," Multiple objects in image 000270, using object 2.\n"," Multiple objects in image 000271, using object 2.\n"," Multiple objects in image 000272, using object 2.\n"," Multiple objects in image 000273, using object 2.\n"," Multiple objects in image 000274, using object 2.\n"," Multiple objects in image 000275, using object 2.\n"," Multiple objects in image 000276, using object 2.\n"," Multiple objects in image 000277, using object 2.\n"," Multiple objects in image 000278, using object 2.\n"," Multiple objects in image 000279, using object 2.\n"," Multiple objects in image 000280, using object 2.\n"," Multiple objects in image 000281, using object 2.\n"," Multiple objects in image 000282, using object 2.\n"," Multiple objects in image 000283, using object 2.\n"," Multiple objects in image 000284, using object 2.\n"," Multiple objects in image 000285, using object 2.\n"," Multiple objects in image 000286, using object 2.\n"," Multiple objects in image 000287, using object 2.\n"," Multiple objects in image 000288, using object 2.\n"," Multiple objects in image 000289, using object 2.\n"," Multiple objects in image 000290, using object 2.\n"," Multiple objects in image 000291, using object 2.\n"," Multiple objects in image 000292, using object 2.\n"," Multiple objects in image 000293, using object 2.\n"," Multiple objects in image 000294, using object 2.\n"," Multiple objects in image 000295, using object 2.\n"," Multiple objects in image 000296, using object 2.\n"," Multiple objects in image 000297, using object 2.\n"," Multiple objects in image 000298, using object 2.\n"," Multiple objects in image 000299, using object 2.\n"," Multiple objects in image 000300, using object 2.\n"," Multiple objects in image 000301, using object 2.\n"," Multiple objects in image 000302, using object 2.\n"," Multiple objects in image 000303, using object 2.\n"," Multiple objects in image 000304, using object 2.\n"," Multiple objects in image 000305, using object 2.\n"," Multiple objects in image 000306, using object 2.\n"," Multiple objects in image 000307, using object 2.\n"," Multiple objects in image 000308, using object 2.\n"," Multiple objects in image 000309, using object 2.\n"," Multiple objects in image 000310, using object 2.\n"," Multiple objects in image 000311, using object 2.\n"," Multiple objects in image 000312, using object 2.\n"," Multiple objects in image 000313, using object 2.\n"," Multiple objects in image 000314, using object 2.\n"," Multiple objects in image 000315, using object 2.\n"," Multiple objects in image 000316, using object 2.\n"," Multiple objects in image 000317, using object 2.\n"," Multiple objects in image 000318, using object 2.\n"," Multiple objects in image 000319, using object 2.\n"," Multiple objects in image 000320, using object 2.\n"," Multiple objects in image 000321, using object 2.\n"," Multiple objects in image 000322, using object 2.\n"," Multiple objects in image 000323, using object 2.\n"," Multiple objects in image 000324, using object 2.\n"," Multiple objects in image 000325, using object 2.\n"," Multiple objects in image 000326, using object 2.\n"," Multiple objects in image 000327, using object 2.\n"," Multiple objects in image 000328, using object 2.\n"," Multiple objects in image 000329, using object 2.\n"," Multiple objects in image 000330, using object 2.\n"," Multiple objects in image 000331, using object 2.\n"," Multiple objects in image 000332, using object 2.\n"," Multiple objects in image 000333, using object 2.\n"," Multiple objects in image 000334, using object 2.\n"," Multiple objects in image 000335, using object 2.\n"," Multiple objects in image 000336, using object 2.\n"," Multiple objects in image 000337, using object 2.\n"," Multiple objects in image 000338, using object 2.\n"," Multiple objects in image 000339, using object 2.\n"," Multiple objects in image 000340, using object 2.\n"," Multiple objects in image 000341, using object 2.\n"," Multiple objects in image 000342, using object 2.\n"," Multiple objects in image 000343, using object 2.\n"," Multiple objects in image 000344, using object 2.\n"," Multiple objects in image 000345, using object 2.\n"," Multiple objects in image 000346, using object 2.\n"," Multiple objects in image 000347, using object 2.\n"," Multiple objects in image 000348, using object 2.\n"," Multiple objects in image 000349, using object 2.\n"," Multiple objects in image 000350, using object 2.\n"," Multiple objects in image 000351, using object 2.\n"," Multiple objects in image 000352, using object 2.\n"," Multiple objects in image 000353, using object 2.\n"," Multiple objects in image 000354, using object 2.\n"," Multiple objects in image 000355, using object 2.\n"," Multiple objects in image 000356, using object 2.\n"," Multiple objects in image 000357, using object 2.\n"," Multiple objects in image 000358, using object 2.\n"," Multiple objects in image 000359, using object 2.\n"," Multiple objects in image 000360, using object 2.\n"," Multiple objects in image 000361, using object 2.\n"," Multiple objects in image 000362, using object 2.\n"," Multiple objects in image 000363, using object 2.\n"," Multiple objects in image 000364, using object 2.\n"," Multiple objects in image 000365, using object 2.\n"," Multiple objects in image 000366, using object 2.\n"," Multiple objects in image 000367, using object 2.\n"," Multiple objects in image 000368, using object 2.\n"," Multiple objects in image 000369, using object 2.\n"," Multiple objects in image 000370, using object 2.\n"," Multiple objects in image 000371, using object 2.\n"," Multiple objects in image 000372, using object 2.\n"," Multiple objects in image 000373, using object 2.\n"," Multiple objects in image 000374, using object 2.\n"," Multiple objects in image 000375, using object 2.\n"," Multiple objects in image 000376, using object 2.\n"," Multiple objects in image 000377, using object 2.\n"," Multiple objects in image 000378, using object 2.\n"," Multiple objects in image 000379, using object 2.\n"," Multiple objects in image 000380, using object 2.\n"," Multiple objects in image 000381, using object 2.\n"," Multiple objects in image 000382, using object 2.\n"," Multiple objects in image 000383, using object 2.\n"," Multiple objects in image 000384, using object 2.\n"," Multiple objects in image 000385, using object 2.\n"," Multiple objects in image 000386, using object 2.\n"," Multiple objects in image 000387, using object 2.\n"," Multiple objects in image 000388, using object 2.\n"," Multiple objects in image 000389, using object 2.\n"," Multiple objects in image 000390, using object 2.\n"," Multiple objects in image 000391, using object 2.\n"," Multiple objects in image 000392, using object 2.\n"," Multiple objects in image 000393, using object 2.\n"," Multiple objects in image 000394, using object 2.\n"," Multiple objects in image 000395, using object 2.\n"," Multiple objects in image 000396, using object 2.\n"," Multiple objects in image 000397, using object 2.\n"," Multiple objects in image 000398, using object 2.\n"," Multiple objects in image 000399, using object 2.\n"," Multiple objects in image 000400, using object 2.\n"," Multiple objects in image 000401, using object 2.\n"," Multiple objects in image 000402, using object 2.\n"," Multiple objects in image 000403, using object 2.\n"," Multiple objects in image 000404, using object 2.\n"," Multiple objects in image 000405, using object 2.\n"," Multiple objects in image 000406, using object 2.\n"," Multiple objects in image 000407, using object 2.\n"," Multiple objects in image 000408, using object 2.\n"," Multiple objects in image 000409, using object 2.\n"," Multiple objects in image 000410, using object 2.\n"," Multiple objects in image 000411, using object 2.\n"," Multiple objects in image 000412, using object 2.\n"," Multiple objects in image 000413, using object 2.\n"," Multiple objects in image 000414, using object 2.\n"," Multiple objects in image 000415, using object 2.\n"," Multiple objects in image 000416, using object 2.\n"," Multiple objects in image 000417, using object 2.\n"," Multiple objects in image 000418, using object 2.\n"," Multiple objects in image 000419, using object 2.\n"," Multiple objects in image 000420, using object 2.\n"," Multiple objects in image 000421, using object 2.\n"," Multiple objects in image 000422, using object 2.\n"," Multiple objects in image 000423, using object 2.\n"," Multiple objects in image 000424, using object 2.\n"," Multiple objects in image 000425, using object 2.\n"," Multiple objects in image 000426, using object 2.\n"," Multiple objects in image 000427, using object 2.\n"," Multiple objects in image 000428, using object 2.\n"," Multiple objects in image 000429, using object 2.\n"," Multiple objects in image 000430, using object 2.\n"," Multiple objects in image 000431, using object 2.\n"," Multiple objects in image 000432, using object 2.\n"," Multiple objects in image 000433, using object 2.\n"," Multiple objects in image 000434, using object 2.\n"," Multiple objects in image 000435, using object 2.\n"," Multiple objects in image 000436, using object 2.\n"," Multiple objects in image 000437, using object 2.\n"," Multiple objects in image 000438, using object 2.\n"," Multiple objects in image 000439, using object 2.\n"," Multiple objects in image 000440, using object 2.\n"," Multiple objects in image 000441, using object 2.\n"," Multiple objects in image 000442, using object 2.\n"," Multiple objects in image 000443, using object 2.\n"," Multiple objects in image 000444, using object 2.\n"," Multiple objects in image 000445, using object 2.\n"," Multiple objects in image 000446, using object 2.\n"," Multiple objects in image 000447, using object 2.\n"," Multiple objects in image 000448, using object 2.\n"," Multiple objects in image 000449, using object 2.\n"," Multiple objects in image 000450, using object 2.\n"," Multiple objects in image 000451, using object 2.\n"," Multiple objects in image 000452, using object 2.\n"," Multiple objects in image 000453, using object 2.\n"," Multiple objects in image 000454, using object 2.\n"," Multiple objects in image 000455, using object 2.\n"," Multiple objects in image 000456, using object 2.\n"," Multiple objects in image 000457, using object 2.\n"," Multiple objects in image 000458, using object 2.\n"," Multiple objects in image 000459, using object 2.\n"," Multiple objects in image 000460, using object 2.\n"," Multiple objects in image 000461, using object 2.\n"," Multiple objects in image 000462, using object 2.\n"," Multiple objects in image 000463, using object 2.\n"," Multiple objects in image 000464, using object 2.\n"," Multiple objects in image 000465, using object 2.\n"," Multiple objects in image 000466, using object 2.\n"," Multiple objects in image 000467, using object 2.\n"," Multiple objects in image 000468, using object 2.\n"," Multiple objects in image 000469, using object 2.\n"," Multiple objects in image 000470, using object 2.\n"," Multiple objects in image 000471, using object 2.\n"," Multiple objects in image 000472, using object 2.\n"," Multiple objects in image 000473, using object 2.\n"," Multiple objects in image 000474, using object 2.\n"," Multiple objects in image 000475, using object 2.\n"," Multiple objects in image 000476, using object 2.\n"," Multiple objects in image 000477, using object 2.\n"," Multiple objects in image 000478, using object 2.\n"," Multiple objects in image 000479, using object 2.\n"," Multiple objects in image 000480, using object 2.\n"," Multiple objects in image 000481, using object 2.\n"," Multiple objects in image 000482, using object 2.\n"," Multiple objects in image 000483, using object 2.\n"," Multiple objects in image 000484, using object 2.\n"," Multiple objects in image 000485, using object 2.\n"," Multiple objects in image 000486, using object 2.\n"," Multiple objects in image 000487, using object 2.\n"," Multiple objects in image 000488, using object 2.\n"," Multiple objects in image 000489, using object 2.\n"," Multiple objects in image 000490, using object 2.\n"," Multiple objects in image 000491, using object 2.\n"," Multiple objects in image 000492, using object 2.\n"," Multiple objects in image 000493, using object 2.\n"," Multiple objects in image 000494, using object 2.\n"," Multiple objects in image 000495, using object 2.\n"," Multiple objects in image 000496, using object 2.\n"," Multiple objects in image 000497, using object 2.\n"," Multiple objects in image 000498, using object 2.\n"," Multiple objects in image 000499, using object 2.\n"," Multiple objects in image 000500, using object 2.\n"," Multiple objects in image 000501, using object 2.\n"," Multiple objects in image 000502, using object 2.\n"," Multiple objects in image 000503, using object 2.\n"," Multiple objects in image 000504, using object 2.\n"," Multiple objects in image 000505, using object 2.\n"," Multiple objects in image 000506, using object 2.\n"," Multiple objects in image 000507, using object 2.\n"," Multiple objects in image 000508, using object 2.\n"," Multiple objects in image 000509, using object 2.\n"," Multiple objects in image 000510, using object 2.\n"," Multiple objects in image 000511, using object 2.\n"," Multiple objects in image 000512, using object 2.\n"," Multiple objects in image 000513, using object 2.\n"," Multiple objects in image 000514, using object 2.\n"," Multiple objects in image 000515, using object 2.\n"," Multiple objects in image 000516, using object 2.\n"," Multiple objects in image 000517, using object 2.\n"," Multiple objects in image 000518, using object 2.\n"," Multiple objects in image 000519, using object 2.\n"," Multiple objects in image 000520, using object 2.\n"," Multiple objects in image 000521, using object 2.\n"," Multiple objects in image 000522, using object 2.\n"," Multiple objects in image 000523, using object 2.\n"," Multiple objects in image 000524, using object 2.\n"," Multiple objects in image 000525, using object 2.\n"," Multiple objects in image 000526, using object 2.\n"," Multiple objects in image 000527, using object 2.\n"," Multiple objects in image 000528, using object 2.\n"," Multiple objects in image 000529, using object 2.\n"," Multiple objects in image 000530, using object 2.\n"," Multiple objects in image 000531, using object 2.\n"," Multiple objects in image 000532, using object 2.\n"," Multiple objects in image 000533, using object 2.\n"," Multiple objects in image 000534, using object 2.\n"," Multiple objects in image 000535, using object 2.\n"," Multiple objects in image 000536, using object 2.\n"," Multiple objects in image 000537, using object 2.\n"," Multiple objects in image 000538, using object 2.\n"," Multiple objects in image 000539, using object 2.\n"," Multiple objects in image 000540, using object 2.\n"," Multiple objects in image 000541, using object 2.\n"," Multiple objects in image 000542, using object 2.\n"," Multiple objects in image 000543, using object 2.\n"," Multiple objects in image 000544, using object 2.\n"," Multiple objects in image 000545, using object 2.\n"," Multiple objects in image 000546, using object 2.\n"," Multiple objects in image 000547, using object 2.\n"," Multiple objects in image 000548, using object 2.\n"," Multiple objects in image 000549, using object 2.\n"," Multiple objects in image 000550, using object 2.\n"," Multiple objects in image 000551, using object 2.\n"," Multiple objects in image 000552, using object 2.\n"," Multiple objects in image 000553, using object 2.\n"," Multiple objects in image 000554, using object 2.\n"," Multiple objects in image 000555, using object 2.\n"," Multiple objects in image 000556, using object 2.\n"," Multiple objects in image 000557, using object 2.\n"," Multiple objects in image 000558, using object 2.\n"," Multiple objects in image 000559, using object 2.\n"," Multiple objects in image 000560, using object 2.\n"," Multiple objects in image 000561, using object 2.\n"," Multiple objects in image 000562, using object 2.\n"," Multiple objects in image 000563, using object 2.\n"," Multiple objects in image 000564, using object 2.\n"," Multiple objects in image 000565, using object 2.\n"," Multiple objects in image 000566, using object 2.\n"," Multiple objects in image 000567, using object 2.\n"," Multiple objects in image 000568, using object 2.\n"," Multiple objects in image 000569, using object 2.\n"," Multiple objects in image 000570, using object 2.\n"," Multiple objects in image 000571, using object 2.\n"," Multiple objects in image 000572, using object 2.\n"," Multiple objects in image 000573, using object 2.\n"," Multiple objects in image 000574, using object 2.\n"," Multiple objects in image 000575, using object 2.\n"," Multiple objects in image 000576, using object 2.\n"," Multiple objects in image 000577, using object 2.\n"," Multiple objects in image 000578, using object 2.\n"," Multiple objects in image 000579, using object 2.\n"," Multiple objects in image 000580, using object 2.\n"," Multiple objects in image 000581, using object 2.\n"," Multiple objects in image 000582, using object 2.\n"," Multiple objects in image 000583, using object 2.\n"," Multiple objects in image 000584, using object 2.\n"," Multiple objects in image 000585, using object 2.\n"," Multiple objects in image 000586, using object 2.\n"," Multiple objects in image 000587, using object 2.\n"," Multiple objects in image 000588, using object 2.\n"," Multiple objects in image 000589, using object 2.\n"," Multiple objects in image 000590, using object 2.\n"," Multiple objects in image 000591, using object 2.\n"," Multiple objects in image 000592, using object 2.\n"," Multiple objects in image 000593, using object 2.\n"," Multiple objects in image 000594, using object 2.\n"," Multiple objects in image 000595, using object 2.\n"," Multiple objects in image 000596, using object 2.\n"," Multiple objects in image 000597, using object 2.\n"," Multiple objects in image 000598, using object 2.\n"," Multiple objects in image 000599, using object 2.\n"," Multiple objects in image 000600, using object 2.\n"," Multiple objects in image 000601, using object 2.\n"," Multiple objects in image 000602, using object 2.\n"," Multiple objects in image 000603, using object 2.\n"," Multiple objects in image 000604, using object 2.\n"," Multiple objects in image 000605, using object 2.\n"," Multiple objects in image 000606, using object 2.\n"," Multiple objects in image 000607, using object 2.\n"," Multiple objects in image 000608, using object 2.\n"," Multiple objects in image 000609, using object 2.\n"," Multiple objects in image 000610, using object 2.\n"," Multiple objects in image 000611, using object 2.\n"," Multiple objects in image 000612, using object 2.\n"," Multiple objects in image 000613, using object 2.\n"," Multiple objects in image 000614, using object 2.\n"," Multiple objects in image 000615, using object 2.\n"," Multiple objects in image 000616, using object 2.\n"," Multiple objects in image 000617, using object 2.\n"," Multiple objects in image 000618, using object 2.\n"," Multiple objects in image 000619, using object 2.\n"," Multiple objects in image 000620, using object 2.\n"," Multiple objects in image 000621, using object 2.\n"," Multiple objects in image 000622, using object 2.\n"," Multiple objects in image 000623, using object 2.\n"," Multiple objects in image 000624, using object 2.\n"," Multiple objects in image 000625, using object 2.\n"," Multiple objects in image 000626, using object 2.\n"," Multiple objects in image 000627, using object 2.\n"," Multiple objects in image 000628, using object 2.\n"," Multiple objects in image 000629, using object 2.\n"," Multiple objects in image 000630, using object 2.\n"," Multiple objects in image 000631, using object 2.\n"," Multiple objects in image 000632, using object 2.\n"," Multiple objects in image 000633, using object 2.\n"," Multiple objects in image 000634, using object 2.\n"," Multiple objects in image 000635, using object 2.\n"," Multiple objects in image 000636, using object 2.\n"," Multiple objects in image 000637, using object 2.\n"," Multiple objects in image 000638, using object 2.\n"," Multiple objects in image 000639, using object 2.\n"," Multiple objects in image 000640, using object 2.\n"," Multiple objects in image 000641, using object 2.\n"," Multiple objects in image 000642, using object 2.\n"," Multiple objects in image 000643, using object 2.\n"," Multiple objects in image 000644, using object 2.\n"," Multiple objects in image 000645, using object 2.\n"," Multiple objects in image 000646, using object 2.\n"," Multiple objects in image 000647, using object 2.\n"," Multiple objects in image 000648, using object 2.\n"," Multiple objects in image 000649, using object 2.\n"," Multiple objects in image 000650, using object 2.\n"," Multiple objects in image 000651, using object 2.\n"," Multiple objects in image 000652, using object 2.\n"," Multiple objects in image 000653, using object 2.\n"," Multiple objects in image 000654, using object 2.\n"," Multiple objects in image 000655, using object 2.\n"," Multiple objects in image 000656, using object 2.\n"," Multiple objects in image 000657, using object 2.\n"," Multiple objects in image 000658, using object 2.\n"," Multiple objects in image 000659, using object 2.\n"," Multiple objects in image 000660, using object 2.\n"," Multiple objects in image 000661, using object 2.\n"," Multiple objects in image 000662, using object 2.\n"," Multiple objects in image 000663, using object 2.\n"," Multiple objects in image 000664, using object 2.\n"," Multiple objects in image 000665, using object 2.\n"," Multiple objects in image 000666, using object 2.\n"," Multiple objects in image 000667, using object 2.\n"," Multiple objects in image 000668, using object 2.\n"," Multiple objects in image 000669, using object 2.\n"," Multiple objects in image 000670, using object 2.\n"," Multiple objects in image 000671, using object 2.\n"," Multiple objects in image 000672, using object 2.\n"," Multiple objects in image 000673, using object 2.\n"," Multiple objects in image 000674, using object 2.\n"," Multiple objects in image 000675, using object 2.\n"," Multiple objects in image 000676, using object 2.\n"," Multiple objects in image 000677, using object 2.\n"," Multiple objects in image 000678, using object 2.\n"," Multiple objects in image 000679, using object 2.\n"," Multiple objects in image 000680, using object 2.\n"," Multiple objects in image 000681, using object 2.\n"," Multiple objects in image 000682, using object 2.\n"," Multiple objects in image 000683, using object 2.\n"," Multiple objects in image 000684, using object 2.\n"," Multiple objects in image 000685, using object 2.\n"," Multiple objects in image 000686, using object 2.\n"," Multiple objects in image 000687, using object 2.\n"," Multiple objects in image 000688, using object 2.\n"," Multiple objects in image 000689, using object 2.\n"," Multiple objects in image 000690, using object 2.\n"," Multiple objects in image 000691, using object 2.\n"," Multiple objects in image 000692, using object 2.\n"," Multiple objects in image 000693, using object 2.\n"," Multiple objects in image 000694, using object 2.\n"," Multiple objects in image 000695, using object 2.\n"," Multiple objects in image 000696, using object 2.\n"," Multiple objects in image 000697, using object 2.\n"," Multiple objects in image 000698, using object 2.\n"," Multiple objects in image 000699, using object 2.\n"," Multiple objects in image 000700, using object 2.\n"," Multiple objects in image 000701, using object 2.\n"," Multiple objects in image 000702, using object 2.\n"," Multiple objects in image 000703, using object 2.\n"," Multiple objects in image 000704, using object 2.\n"," Multiple objects in image 000705, using object 2.\n"," Multiple objects in image 000706, using object 2.\n"," Multiple objects in image 000707, using object 2.\n"," Multiple objects in image 000708, using object 2.\n"," Multiple objects in image 000709, using object 2.\n"," Multiple objects in image 000710, using object 2.\n"," Multiple objects in image 000711, using object 2.\n"," Multiple objects in image 000712, using object 2.\n"," Multiple objects in image 000713, using object 2.\n"," Multiple objects in image 000714, using object 2.\n"," Multiple objects in image 000715, using object 2.\n"," Multiple objects in image 000716, using object 2.\n"," Multiple objects in image 000717, using object 2.\n"," Multiple objects in image 000718, using object 2.\n"," Multiple objects in image 000719, using object 2.\n"," Multiple objects in image 000720, using object 2.\n"," Multiple objects in image 000721, using object 2.\n"," Multiple objects in image 000722, using object 2.\n"," Multiple objects in image 000723, using object 2.\n"," Multiple objects in image 000724, using object 2.\n"," Multiple objects in image 000725, using object 2.\n"," Multiple objects in image 000726, using object 2.\n"," Multiple objects in image 000727, using object 2.\n"," Multiple objects in image 000728, using object 2.\n"," Multiple objects in image 000729, using object 2.\n"," Multiple objects in image 000730, using object 2.\n"," Multiple objects in image 000731, using object 2.\n"," Multiple objects in image 000732, using object 2.\n"," Multiple objects in image 000733, using object 2.\n"," Multiple objects in image 000734, using object 2.\n"," Multiple objects in image 000735, using object 2.\n"," Multiple objects in image 000736, using object 2.\n"," Multiple objects in image 000737, using object 2.\n"," Multiple objects in image 000738, using object 2.\n"," Multiple objects in image 000739, using object 2.\n"," Multiple objects in image 000740, using object 2.\n"," Multiple objects in image 000741, using object 2.\n"," Multiple objects in image 000742, using object 2.\n"," Multiple objects in image 000743, using object 2.\n"," Multiple objects in image 000744, using object 2.\n"," Multiple objects in image 000745, using object 2.\n"," Multiple objects in image 000746, using object 2.\n"," Multiple objects in image 000747, using object 2.\n"," Multiple objects in image 000748, using object 2.\n"," Multiple objects in image 000749, using object 2.\n"," Multiple objects in image 000750, using object 2.\n"," Multiple objects in image 000751, using object 2.\n"," Multiple objects in image 000752, using object 2.\n"," Multiple objects in image 000753, using object 2.\n"," Multiple objects in image 000754, using object 2.\n"," Multiple objects in image 000755, using object 2.\n"," Multiple objects in image 000756, using object 2.\n"," Multiple objects in image 000757, using object 2.\n"," Multiple objects in image 000758, using object 2.\n"," Multiple objects in image 000759, using object 2.\n"," Multiple objects in image 000760, using object 2.\n"," Multiple objects in image 000761, using object 2.\n"," Multiple objects in image 000762, using object 2.\n"," Multiple objects in image 000763, using object 2.\n"," Multiple objects in image 000764, using object 2.\n"," Multiple objects in image 000765, using object 2.\n"," Multiple objects in image 000766, using object 2.\n"," Multiple objects in image 000767, using object 2.\n"," Multiple objects in image 000768, using object 2.\n"," Multiple objects in image 000769, using object 2.\n"," Multiple objects in image 000770, using object 2.\n"," Multiple objects in image 000771, using object 2.\n"," Multiple objects in image 000772, using object 2.\n"," Multiple objects in image 000773, using object 2.\n"," Multiple objects in image 000774, using object 2.\n"," Multiple objects in image 000775, using object 2.\n"," Multiple objects in image 000776, using object 2.\n"," Multiple objects in image 000777, using object 2.\n"," Multiple objects in image 000778, using object 2.\n"," Multiple objects in image 000779, using object 2.\n"," Multiple objects in image 000780, using object 2.\n"," Multiple objects in image 000781, using object 2.\n"," Multiple objects in image 000782, using object 2.\n"," Multiple objects in image 000783, using object 2.\n"," Multiple objects in image 000784, using object 2.\n"," Multiple objects in image 000785, using object 2.\n"," Multiple objects in image 000786, using object 2.\n"," Multiple objects in image 000787, using object 2.\n"," Multiple objects in image 000788, using object 2.\n"," Multiple objects in image 000789, using object 2.\n"," Multiple objects in image 000790, using object 2.\n"," Multiple objects in image 000791, using object 2.\n"," Multiple objects in image 000792, using object 2.\n"," Multiple objects in image 000793, using object 2.\n"," Multiple objects in image 000794, using object 2.\n"," Multiple objects in image 000795, using object 2.\n"," Multiple objects in image 000796, using object 2.\n"," Multiple objects in image 000797, using object 2.\n"," Multiple objects in image 000798, using object 2.\n"," Multiple objects in image 000799, using object 2.\n"," Multiple objects in image 000800, using object 2.\n"," Multiple objects in image 000801, using object 2.\n"," Multiple objects in image 000802, using object 2.\n"," Multiple objects in image 000803, using object 2.\n"," Multiple objects in image 000804, using object 2.\n"," Multiple objects in image 000805, using object 2.\n"," Multiple objects in image 000806, using object 2.\n"," Multiple objects in image 000807, using object 2.\n"," Multiple objects in image 000808, using object 2.\n"," Multiple objects in image 000809, using object 2.\n"," Multiple objects in image 000810, using object 2.\n"," Multiple objects in image 000811, using object 2.\n"," Multiple objects in image 000812, using object 2.\n"," Multiple objects in image 000813, using object 2.\n"," Multiple objects in image 000814, using object 2.\n"," Multiple objects in image 000815, using object 2.\n"," Multiple objects in image 000816, using object 2.\n"," Multiple objects in image 000817, using object 2.\n"," Multiple objects in image 000818, using object 2.\n"," Multiple objects in image 000819, using object 2.\n"," Multiple objects in image 000820, using object 2.\n"," Multiple objects in image 000821, using object 2.\n"," Multiple objects in image 000822, using object 2.\n"," Multiple objects in image 000823, using object 2.\n"," Multiple objects in image 000824, using object 2.\n"," Multiple objects in image 000825, using object 2.\n"," Multiple objects in image 000826, using object 2.\n"," Multiple objects in image 000827, using object 2.\n"," Multiple objects in image 000828, using object 2.\n"," Multiple objects in image 000829, using object 2.\n"," Multiple objects in image 000830, using object 2.\n"," Multiple objects in image 000831, using object 2.\n"," Multiple objects in image 000832, using object 2.\n"," Multiple objects in image 000833, using object 2.\n"," Multiple objects in image 000834, using object 2.\n"," Multiple objects in image 000835, using object 2.\n"," Multiple objects in image 000836, using object 2.\n"," Multiple objects in image 000837, using object 2.\n"," Multiple objects in image 000838, using object 2.\n"," Multiple objects in image 000839, using object 2.\n"," Multiple objects in image 000840, using object 2.\n"," Multiple objects in image 000841, using object 2.\n"," Multiple objects in image 000842, using object 2.\n"," Multiple objects in image 000843, using object 2.\n"," Multiple objects in image 000844, using object 2.\n"," Multiple objects in image 000845, using object 2.\n"," Multiple objects in image 000846, using object 2.\n"," Multiple objects in image 000847, using object 2.\n"," Multiple objects in image 000848, using object 2.\n"," Multiple objects in image 000849, using object 2.\n"," Multiple objects in image 000850, using object 2.\n"," Multiple objects in image 000851, using object 2.\n"," Multiple objects in image 000852, using object 2.\n"," Multiple objects in image 000853, using object 2.\n"," Multiple objects in image 000854, using object 2.\n"," Multiple objects in image 000855, using object 2.\n"," Multiple objects in image 000856, using object 2.\n"," Multiple objects in image 000857, using object 2.\n"," Multiple objects in image 000858, using object 2.\n"," Multiple objects in image 000859, using object 2.\n"," Multiple objects in image 000860, using object 2.\n"," Multiple objects in image 000861, using object 2.\n"," Multiple objects in image 000862, using object 2.\n"," Multiple objects in image 000863, using object 2.\n"," Multiple objects in image 000864, using object 2.\n"," Multiple objects in image 000865, using object 2.\n"," Multiple objects in image 000866, using object 2.\n"," Multiple objects in image 000867, using object 2.\n"," Multiple objects in image 000868, using object 2.\n"," Multiple objects in image 000869, using object 2.\n"," Multiple objects in image 000870, using object 2.\n"," Multiple objects in image 000871, using object 2.\n"," Multiple objects in image 000872, using object 2.\n"," Multiple objects in image 000873, using object 2.\n"," Multiple objects in image 000874, using object 2.\n"," Multiple objects in image 000875, using object 2.\n"," Multiple objects in image 000876, using object 2.\n"," Multiple objects in image 000877, using object 2.\n"," Multiple objects in image 000878, using object 2.\n"," Multiple objects in image 000879, using object 2.\n"," Multiple objects in image 000880, using object 2.\n"," Multiple objects in image 000881, using object 2.\n"," Multiple objects in image 000882, using object 2.\n"," Multiple objects in image 000883, using object 2.\n"," Multiple objects in image 000884, using object 2.\n"," Multiple objects in image 000885, using object 2.\n"," Multiple objects in image 000886, using object 2.\n"," Multiple objects in image 000887, using object 2.\n"," Multiple objects in image 000888, using object 2.\n"," Multiple objects in image 000889, using object 2.\n"," Multiple objects in image 000890, using object 2.\n"," Multiple objects in image 000891, using object 2.\n"," Multiple objects in image 000892, using object 2.\n"," Multiple objects in image 000893, using object 2.\n"," Multiple objects in image 000894, using object 2.\n"," Multiple objects in image 000895, using object 2.\n"," Multiple objects in image 000896, using object 2.\n"," Multiple objects in image 000897, using object 2.\n"," Multiple objects in image 000898, using object 2.\n"," Multiple objects in image 000899, using object 2.\n"," Multiple objects in image 000900, using object 2.\n"," Multiple objects in image 000901, using object 2.\n"," Multiple objects in image 000902, using object 2.\n"," Multiple objects in image 000903, using object 2.\n"," Multiple objects in image 000904, using object 2.\n"," Multiple objects in image 000905, using object 2.\n"," Multiple objects in image 000906, using object 2.\n"," Multiple objects in image 000907, using object 2.\n"," Multiple objects in image 000908, using object 2.\n"," Multiple objects in image 000909, using object 2.\n"," Multiple objects in image 000910, using object 2.\n"," Multiple objects in image 000911, using object 2.\n"," Multiple objects in image 000912, using object 2.\n"," Multiple objects in image 000913, using object 2.\n"," Multiple objects in image 000914, using object 2.\n"," Multiple objects in image 000915, using object 2.\n"," Multiple objects in image 000916, using object 2.\n"," Multiple objects in image 000917, using object 2.\n"," Multiple objects in image 000918, using object 2.\n"," Multiple objects in image 000919, using object 2.\n"," Multiple objects in image 000920, using object 2.\n"," Multiple objects in image 000921, using object 2.\n"," Multiple objects in image 000922, using object 2.\n"," Multiple objects in image 000923, using object 2.\n"," Multiple objects in image 000924, using object 2.\n"," Multiple objects in image 000925, using object 2.\n"," Multiple objects in image 000926, using object 2.\n"," Multiple objects in image 000927, using object 2.\n"," Multiple objects in image 000928, using object 2.\n"," Multiple objects in image 000929, using object 2.\n"," Multiple objects in image 000930, using object 2.\n"," Multiple objects in image 000931, using object 2.\n"," Multiple objects in image 000932, using object 2.\n"," Multiple objects in image 000933, using object 2.\n"," Multiple objects in image 000934, using object 2.\n"," Multiple objects in image 000935, using object 2.\n"," Multiple objects in image 000936, using object 2.\n"," Multiple objects in image 000937, using object 2.\n"," Multiple objects in image 000938, using object 2.\n"," Multiple objects in image 000939, using object 2.\n"," Multiple objects in image 000940, using object 2.\n"," Multiple objects in image 000941, using object 2.\n"," Multiple objects in image 000942, using object 2.\n"," Multiple objects in image 000943, using object 2.\n"," Multiple objects in image 000944, using object 2.\n"," Multiple objects in image 000945, using object 2.\n"," Multiple objects in image 000946, using object 2.\n"," Multiple objects in image 000947, using object 2.\n"," Multiple objects in image 000948, using object 2.\n"," Multiple objects in image 000949, using object 2.\n"," Multiple objects in image 000950, using object 2.\n"," Multiple objects in image 000951, using object 2.\n"," Multiple objects in image 000952, using object 2.\n"," Multiple objects in image 000953, using object 2.\n"," Multiple objects in image 000954, using object 2.\n"," Multiple objects in image 000955, using object 2.\n"," Multiple objects in image 000956, using object 2.\n"," Multiple objects in image 000957, using object 2.\n"," Multiple objects in image 000958, using object 2.\n"," Multiple objects in image 000959, using object 2.\n"," Multiple objects in image 000960, using object 2.\n"," Multiple objects in image 000961, using object 2.\n"," Multiple objects in image 000962, using object 2.\n"," Multiple objects in image 000963, using object 2.\n"," Multiple objects in image 000964, using object 2.\n"," Multiple objects in image 000965, using object 2.\n"," Multiple objects in image 000966, using object 2.\n"," Multiple objects in image 000967, using object 2.\n"," Multiple objects in image 000968, using object 2.\n"," Multiple objects in image 000969, using object 2.\n"," Multiple objects in image 000970, using object 2.\n"," Multiple objects in image 000971, using object 2.\n"," Multiple objects in image 000972, using object 2.\n"," Multiple objects in image 000973, using object 2.\n"," Multiple objects in image 000974, using object 2.\n"," Multiple objects in image 000975, using object 2.\n"," Multiple objects in image 000976, using object 2.\n"," Multiple objects in image 000977, using object 2.\n"," Multiple objects in image 000978, using object 2.\n"," Multiple objects in image 000979, using object 2.\n"," Multiple objects in image 000980, using object 2.\n"," Multiple objects in image 000981, using object 2.\n"," Multiple objects in image 000982, using object 2.\n"," Multiple objects in image 000983, using object 2.\n"," Multiple objects in image 000984, using object 2.\n"," Multiple objects in image 000985, using object 2.\n"," Multiple objects in image 000986, using object 2.\n"," Multiple objects in image 000987, using object 2.\n"," Multiple objects in image 000988, using object 2.\n"," Multiple objects in image 000989, using object 2.\n"," Multiple objects in image 000990, using object 2.\n"," Multiple objects in image 000991, using object 2.\n"," Multiple objects in image 000992, using object 2.\n"," Multiple objects in image 000993, using object 2.\n"," Multiple objects in image 000994, using object 2.\n"," Multiple objects in image 000995, using object 2.\n"," Multiple objects in image 000996, using object 2.\n"," Multiple objects in image 000997, using object 2.\n"," Multiple objects in image 000998, using object 2.\n"," Multiple objects in image 000999, using object 2.\n"," Multiple objects in image 001000, using object 2.\n"," Multiple objects in image 001001, using object 2.\n"," Multiple objects in image 001002, using object 2.\n"," Multiple objects in image 001003, using object 2.\n"," Multiple objects in image 001004, using object 2.\n"," Multiple objects in image 001005, using object 2.\n"," Multiple objects in image 001006, using object 2.\n"," Multiple objects in image 001007, using object 2.\n"," Multiple objects in image 001008, using object 2.\n"," Multiple objects in image 001009, using object 2.\n"," Multiple objects in image 001010, using object 2.\n"," Multiple objects in image 001011, using object 2.\n"," Multiple objects in image 001012, using object 2.\n"," Multiple objects in image 001013, using object 2.\n"," Multiple objects in image 001014, using object 2.\n"," Multiple objects in image 001015, using object 2.\n"," Multiple objects in image 001016, using object 2.\n"," Multiple objects in image 001017, using object 2.\n"," Multiple objects in image 001018, using object 2.\n"," Multiple objects in image 001019, using object 2.\n"," Multiple objects in image 001020, using object 2.\n"," Multiple objects in image 001021, using object 2.\n"," Multiple objects in image 001022, using object 2.\n"," Multiple objects in image 001023, using object 2.\n"," Multiple objects in image 001024, using object 2.\n"," Multiple objects in image 001025, using object 2.\n"," Multiple objects in image 001026, using object 2.\n"," Multiple objects in image 001027, using object 2.\n"," Multiple objects in image 001028, using object 2.\n"," Multiple objects in image 001029, using object 2.\n"," Multiple objects in image 001030, using object 2.\n"," Multiple objects in image 001031, using object 2.\n"," Multiple objects in image 001032, using object 2.\n"," Multiple objects in image 001033, using object 2.\n"," Multiple objects in image 001034, using object 2.\n"," Multiple objects in image 001035, using object 2.\n"," Multiple objects in image 001036, using object 2.\n"," Multiple objects in image 001037, using object 2.\n"," Multiple objects in image 001038, using object 2.\n"," Multiple objects in image 001039, using object 2.\n"," Multiple objects in image 001040, using object 2.\n"," Multiple objects in image 001041, using object 2.\n"," Multiple objects in image 001042, using object 2.\n"," Multiple objects in image 001043, using object 2.\n"," Multiple objects in image 001044, using object 2.\n"," Multiple objects in image 001045, using object 2.\n"," Multiple objects in image 001046, using object 2.\n"," Multiple objects in image 001047, using object 2.\n"," Multiple objects in image 001048, using object 2.\n"," Multiple objects in image 001049, using object 2.\n"," Multiple objects in image 001050, using object 2.\n"," Multiple objects in image 001051, using object 2.\n"," Multiple objects in image 001052, using object 2.\n"," Multiple objects in image 001053, using object 2.\n"," Multiple objects in image 001054, using object 2.\n"," Multiple objects in image 001055, using object 2.\n"," Multiple objects in image 001056, using object 2.\n"," Multiple objects in image 001057, using object 2.\n"," Multiple objects in image 001058, using object 2.\n"," Multiple objects in image 001059, using object 2.\n"," Multiple objects in image 001060, using object 2.\n"," Multiple objects in image 001061, using object 2.\n"," Multiple objects in image 001062, using object 2.\n"," Multiple objects in image 001063, using object 2.\n"," Multiple objects in image 001064, using object 2.\n"," Multiple objects in image 001065, using object 2.\n"," Multiple objects in image 001066, using object 2.\n"," Multiple objects in image 001067, using object 2.\n"," Multiple objects in image 001068, using object 2.\n"," Multiple objects in image 001069, using object 2.\n"," Multiple objects in image 001070, using object 2.\n"," Multiple objects in image 001071, using object 2.\n"," Multiple objects in image 001072, using object 2.\n"," Multiple objects in image 001073, using object 2.\n"," Multiple objects in image 001074, using object 2.\n"," Multiple objects in image 001075, using object 2.\n"," Multiple objects in image 001076, using object 2.\n"," Multiple objects in image 001077, using object 2.\n"," Multiple objects in image 001078, using object 2.\n"," Multiple objects in image 001079, using object 2.\n"," Multiple objects in image 001080, using object 2.\n"," Multiple objects in image 001081, using object 2.\n"," Multiple objects in image 001082, using object 2.\n"," Multiple objects in image 001083, using object 2.\n"," Multiple objects in image 001084, using object 2.\n"," Multiple objects in image 001085, using object 2.\n"," Multiple objects in image 001086, using object 2.\n"," Multiple objects in image 001087, using object 2.\n"," Multiple objects in image 001088, using object 2.\n"," Multiple objects in image 001089, using object 2.\n"," Multiple objects in image 001090, using object 2.\n"," Multiple objects in image 001091, using object 2.\n"," Multiple objects in image 001092, using object 2.\n"," Multiple objects in image 001093, using object 2.\n"," Multiple objects in image 001094, using object 2.\n"," Multiple objects in image 001095, using object 2.\n"," Multiple objects in image 001096, using object 2.\n"," Multiple objects in image 001097, using object 2.\n"," Multiple objects in image 001098, using object 2.\n"," Multiple objects in image 001099, using object 2.\n"," Multiple objects in image 001100, using object 2.\n"," Multiple objects in image 001101, using object 2.\n"," Multiple objects in image 001102, using object 2.\n"," Multiple objects in image 001103, using object 2.\n"," Multiple objects in image 001104, using object 2.\n"," Multiple objects in image 001105, using object 2.\n"," Multiple objects in image 001106, using object 2.\n"," Multiple objects in image 001107, using object 2.\n"," Multiple objects in image 001108, using object 2.\n"," Multiple objects in image 001109, using object 2.\n"," Multiple objects in image 001110, using object 2.\n"," Multiple objects in image 001111, using object 2.\n"," Multiple objects in image 001112, using object 2.\n"," Multiple objects in image 001113, using object 2.\n"," Multiple objects in image 001114, using object 2.\n"," Multiple objects in image 001115, using object 2.\n"," Multiple objects in image 001116, using object 2.\n"," Multiple objects in image 001117, using object 2.\n"," Multiple objects in image 001118, using object 2.\n"," Multiple objects in image 001119, using object 2.\n"," Multiple objects in image 001120, using object 2.\n"," Multiple objects in image 001121, using object 2.\n"," Multiple objects in image 001122, using object 2.\n"," Multiple objects in image 001123, using object 2.\n"," Multiple objects in image 001124, using object 2.\n"," Multiple objects in image 001125, using object 2.\n"," Multiple objects in image 001126, using object 2.\n"," Multiple objects in image 001127, using object 2.\n"," Multiple objects in image 001128, using object 2.\n"," Multiple objects in image 001129, using object 2.\n"," Multiple objects in image 001130, using object 2.\n"," Multiple objects in image 001131, using object 2.\n"," Multiple objects in image 001132, using object 2.\n"," Multiple objects in image 001133, using object 2.\n"," Multiple objects in image 001134, using object 2.\n"," Multiple objects in image 001135, using object 2.\n"," Multiple objects in image 001136, using object 2.\n"," Multiple objects in image 001137, using object 2.\n"," Multiple objects in image 001138, using object 2.\n"," Multiple objects in image 001139, using object 2.\n"," Multiple objects in image 001140, using object 2.\n"," Multiple objects in image 001141, using object 2.\n"," Multiple objects in image 001142, using object 2.\n"," Multiple objects in image 001143, using object 2.\n"," Multiple objects in image 001144, using object 2.\n"," Multiple objects in image 001145, using object 2.\n"," Multiple objects in image 001146, using object 2.\n"," Multiple objects in image 001147, using object 2.\n"," Multiple objects in image 001148, using object 2.\n"," Multiple objects in image 001149, using object 2.\n"," Multiple objects in image 001150, using object 2.\n"," Multiple objects in image 001151, using object 2.\n"," Multiple objects in image 001152, using object 2.\n"," Multiple objects in image 001153, using object 2.\n"," Multiple objects in image 001154, using object 2.\n"," Multiple objects in image 001155, using object 2.\n"," Multiple objects in image 001156, using object 2.\n"," Multiple objects in image 001157, using object 2.\n"," Multiple objects in image 001158, using object 2.\n"," Multiple objects in image 001159, using object 2.\n"," Multiple objects in image 001160, using object 2.\n"," Multiple objects in image 001161, using object 2.\n"," Multiple objects in image 001162, using object 2.\n"," Multiple objects in image 001163, using object 2.\n"," Multiple objects in image 001164, using object 2.\n"," Multiple objects in image 001165, using object 2.\n"," Multiple objects in image 001166, using object 2.\n"," Multiple objects in image 001167, using object 2.\n"," Multiple objects in image 001168, using object 2.\n"," Multiple objects in image 001169, using object 2.\n"," Multiple objects in image 001170, using object 2.\n"," Multiple objects in image 001171, using object 2.\n"," Multiple objects in image 001172, using object 2.\n"," Multiple objects in image 001173, using object 2.\n"," Multiple objects in image 001174, using object 2.\n"," Multiple objects in image 001175, using object 2.\n"," Multiple objects in image 001176, using object 2.\n"," Multiple objects in image 001177, using object 2.\n"," Multiple objects in image 001178, using object 2.\n"," Multiple objects in image 001179, using object 2.\n"," Multiple objects in image 001180, using object 2.\n"," Multiple objects in image 001181, using object 2.\n"," Multiple objects in image 001182, using object 2.\n"," Multiple objects in image 001183, using object 2.\n"," Multiple objects in image 001184, using object 2.\n"," Multiple objects in image 001185, using object 2.\n"," Multiple objects in image 001186, using object 2.\n"," Multiple objects in image 001187, using object 2.\n"," Multiple objects in image 001188, using object 2.\n"," Multiple objects in image 001189, using object 2.\n"," Multiple objects in image 001190, using object 2.\n"," Multiple objects in image 001191, using object 2.\n"," Multiple objects in image 001192, using object 2.\n"," Multiple objects in image 001193, using object 2.\n"," Multiple objects in image 001194, using object 2.\n"," Multiple objects in image 001195, using object 2.\n"," Multiple objects in image 001196, using object 2.\n"," Multiple objects in image 001197, using object 2.\n"," Multiple objects in image 001198, using object 2.\n"," Multiple objects in image 001199, using object 2.\n"," Multiple objects in image 001200, using object 2.\n"," Multiple objects in image 001201, using object 2.\n"," Multiple objects in image 001202, using object 2.\n"," Multiple objects in image 001203, using object 2.\n"," Multiple objects in image 001204, using object 2.\n"," Multiple objects in image 001205, using object 2.\n"," Multiple objects in image 001206, using object 2.\n"," Multiple objects in image 001207, using object 2.\n"," Multiple objects in image 001208, using object 2.\n"," Multiple objects in image 001209, using object 2.\n"," Multiple objects in image 001210, using object 2.\n"," Multiple objects in image 001211, using object 2.\n"," Multiple objects in image 001212, using object 2.\n"," Multiple objects in image 001213, using object 2.\n","\n"," Processing class 04...\n"]},{"output_type":"stream","name":"stderr","text":["Class 04: 100%|██████████| 1201/1201 [00:00<00:00, 11503.92image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 05...\n"]},{"output_type":"stream","name":"stderr","text":["Class 05: 100%|██████████| 1196/1196 [00:00<00:00, 11421.60image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 06...\n"]},{"output_type":"stream","name":"stderr","text":["Class 06: 100%|██████████| 1179/1179 [00:00<00:00, 10864.83image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 08...\n"]},{"output_type":"stream","name":"stderr","text":["Class 08: 100%|██████████| 1188/1188 [00:00<00:00, 10984.19image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 09...\n"]},{"output_type":"stream","name":"stderr","text":["Class 09: 100%|██████████| 1254/1254 [00:00<00:00, 10508.47image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 10...\n"]},{"output_type":"stream","name":"stderr","text":["Class 10: 100%|██████████| 1253/1253 [00:00<00:00, 11008.76image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 11...\n"]},{"output_type":"stream","name":"stderr","text":["Class 11: 100%|██████████| 1220/1220 [00:00<00:00, 11654.75image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 12...\n"]},{"output_type":"stream","name":"stderr","text":["Class 12: 100%|██████████| 1237/1237 [00:00<00:00, 11095.92image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 13...\n"]},{"output_type":"stream","name":"stderr","text":["Class 13: 100%|██████████| 1152/1152 [00:00<00:00, 10964.70image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 14...\n"]},{"output_type":"stream","name":"stderr","text":["Class 14: 100%|██████████| 1227/1227 [00:00<00:00, 11423.67image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 15...\n"]},{"output_type":"stream","name":"stderr","text":["Class 15: 100%|██████████| 1243/1243 [00:00<00:00, 9860.81image/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," All poses extracted and saved from gt.yml files.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import yaml\n","\n","class_ids = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n","\n","for class_id in class_ids:\n","    print(f\"\\n Processing class {class_id}...\")\n","    class_path = os.path.join(data_path, class_id)\n","    gt_path    = os.path.join(class_path, \"gt.yml\")\n","    pose_dir   = os.path.join(class_path, \"pose\")\n","\n","    if not os.path.exists(gt_path):\n","        print(f\" gt.yml not found for class {class_id} — skipped.\")\n","        continue\n","    os.makedirs(pose_dir, exist_ok=True)\n","\n","    with open(gt_path, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    for img_id, pose_list in tqdm(gt_data.items(), desc=f\"Class {class_id}\", unit=\"image\"):\n","        idx = int(img_id)\n","\n","        # If multiple objects, pick the second one (index 1)\n","        if len(pose_list) > 1:\n","            print(f\" Multiple objects in image {idx:06d}, using object 2.\")\n","            selected_pose = pose_list[1]\n","        else:\n","            selected_pose = pose_list[0]\n","\n","        # Extract R, t and build 3x4 matrix\n","        R = np.array(selected_pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)\n","        t = np.array(selected_pose['cam_t_m2c'], dtype=np.float32).reshape(3, 1)  # mm\n","        RT = np.hstack([R, t])\n","\n","        save_path = os.path.join(pose_dir, f\"pose{idx:06d}.npy\")\n","        np.save(save_path, RT)\n","\n","print(\"\\n All poses extracted and saved from gt.yml files.\")"]},{"cell_type":"markdown","metadata":{"id":"yim04eDar2j1"},"source":["### 4 - Modify RGB, Mask, Depth Files Format From 4 Digits to 6 Digits - Radius Map Needs Files in 6-Digit Format"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1093,"status":"ok","timestamp":1748809963974,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"yV8C9kelWjbe","outputId":"b0fb362b-e7a7-4a59-83ed-b63befa6c88f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===================== Renaming class 01=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 02=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1214 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1215 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1214 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 04=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 05=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 06=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 08=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 09=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 10=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 11=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 12=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 13=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 14=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","===================== Renaming class 15=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1243 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1225 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1243 files\n"]},{"output_type":"stream","name":"stderr","text":["                                        "]},{"output_type":"stream","name":"stdout","text":["============================================================\n","             \n","  All files renamed successfully.             \n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["def rename_files_to_6_digit(data_path, classes):\n","    line_width = 60\n","    for cls in classes:\n","        header = f\" Renaming class {cls}\"\n","        print(\"\\n\" + header.center(line_width, \"=\"))\n","        print(f\"{'Folder':<10} │ {'Count':>6}\")\n","        print(\"-\" * line_width)\n","\n","        for folder_name in ['rgb', 'mask', 'depth']:\n","            emoji = {\n","                'rgb':   '🌈',\n","                'mask':  '🛡️ ',\n","                'depth': '🌊 '\n","            }[folder_name]\n","\n","            folder_path = os.path.join(data_path, cls, folder_name)\n","            if not os.path.isdir(folder_path):\n","                print(f\"{emoji} {folder_name:<8} │  {'—':>6}  (not found)\")\n","                continue\n","\n","            file_list = [\n","                f for f in os.listdir(folder_path)\n","                if os.path.splitext(f)[0].isdigit()\n","            ]\n","            print(f\"{emoji} {folder_name:<8} │ {len(file_list):>6} files\")\n","            for fname in tqdm(\n","                file_list,\n","                desc=f\"{cls}/{folder_name}\",\n","                unit=\"file\",\n","                leave=False,\n","                ncols=40\n","            ):\n","                name, ext = os.path.splitext(fname)\n","                idx = int(name)\n","                new_name = f\"{idx:06d}{ext}\"\n","                os.rename(\n","                    os.path.join(folder_path, fname),\n","                    os.path.join(folder_path, new_name)\n","                )\n","\n","        print(\"=\" * line_width)\n","    print(\"\\n  All files renamed successfully.\".center(line_width))\n","\n","\n","rename_files_to_6_digit(data_path, classes)"]},{"cell_type":"markdown","metadata":{"id":"WxdBZpOTodXW"},"source":["###5 -  Convert Depth files from .png to .dpt - Radius Map Needs Depth Files With .dpt Format"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68885,"status":"ok","timestamp":1748810039372,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"ZPZWinGpongz","outputId":"6d046681-1020-42c4-ee5e-ce0e7b498212"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Processing class 01...\n"]},{"output_type":"stream","name":"stderr","text":["Class 01: 100%|██████████| 1236/1236 [00:04<00:00, 285.05file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 02...\n"]},{"output_type":"stream","name":"stderr","text":["Class 02: 100%|██████████| 1214/1214 [00:06<00:00, 195.39file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 04...\n"]},{"output_type":"stream","name":"stderr","text":["Class 04: 100%|██████████| 1201/1201 [00:04<00:00, 279.72file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 05...\n"]},{"output_type":"stream","name":"stderr","text":["Class 05: 100%|██████████| 1196/1196 [00:05<00:00, 225.43file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 06...\n"]},{"output_type":"stream","name":"stderr","text":["Class 06: 100%|██████████| 1179/1179 [00:05<00:00, 219.16file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 08...\n"]},{"output_type":"stream","name":"stderr","text":["Class 08: 100%|██████████| 1188/1188 [00:05<00:00, 198.57file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 09...\n"]},{"output_type":"stream","name":"stderr","text":["Class 09: 100%|██████████| 1254/1254 [00:04<00:00, 297.87file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 10...\n"]},{"output_type":"stream","name":"stderr","text":["Class 10: 100%|██████████| 1253/1253 [00:05<00:00, 232.92file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 11...\n"]},{"output_type":"stream","name":"stderr","text":["Class 11: 100%|██████████| 1220/1220 [00:05<00:00, 208.55file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 12...\n"]},{"output_type":"stream","name":"stderr","text":["Class 12: 100%|██████████| 1237/1237 [00:06<00:00, 204.96file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 13...\n"]},{"output_type":"stream","name":"stderr","text":["Class 13: 100%|██████████| 1152/1152 [00:04<00:00, 284.29file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 14...\n"]},{"output_type":"stream","name":"stderr","text":["Class 14: 100%|██████████| 1227/1227 [00:04<00:00, 288.74file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Processing class 15...\n"]},{"output_type":"stream","name":"stderr","text":["Class 15: 100%|██████████| 1243/1243 [00:07<00:00, 165.22file/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," All PNG files have been converted to .dpt format.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import cv2\n","from PIL import Image\n","import struct\n","\n","def save_dpt_file(png_path, dpt_path, scale=1.0):\n","    \"\"\"\n","    Converts a depth .png file to a .dpt file with proper binary format:\n","    [uint32 height][uint32 width][uint16 depth values]\n","    Then deletes the original .png file.\n","    \"\"\"\n","    # Read PNG (16-bit depth) safely\n","    depth_img = cv2.imread(png_path, cv2.IMREAD_UNCHANGED)  # preserves uint16\n","    if depth_img is None:\n","        raise ValueError(f\"Failed to read depth PNG: {png_path}\")\n","    depth_img = (depth_img.astype(np.float32) * scale).astype(np.uint16)\n","\n","    h, w = depth_img.shape\n","\n","    with open(dpt_path, 'wb') as f:\n","        f.write(struct.pack('I', h))\n","        f.write(struct.pack('I', w))\n","        f.write(depth_img.tobytes())\n","\n","    # Delete the original .png file after successful conversion\n","    os.remove(png_path)\n","\n","def convert_all_png_to_dpt_in_classes(data_path, classes, scale=1.0):\n","    \"\"\"\n","    Processes all PNG files in the 'depth' folder of each class and creates corresponding .dpt files.\n","    \"\"\"\n","    for cls in classes:\n","        depth_dir = os.path.join(data_path, cls, \"depth\")\n","        if not os.path.exists(depth_dir):\n","            print(f\" Skipping class {cls}: no depth folder found.\")\n","            continue\n","\n","        print(f\"\\n Processing class {cls}...\")\n","\n","        png_files = sorted([f for f in os.listdir(depth_dir) if f.endswith(\".png\")])\n","        for fname in tqdm(png_files, desc=f\"Class {cls}\", unit=\"file\"):\n","            png_path = os.path.join(depth_dir, fname)\n","            dpt_path = os.path.join(depth_dir, os.path.splitext(fname)[0] + \".dpt\")\n","            save_dpt_file(png_path, dpt_path, scale)\n","\n","    print(\"\\n All PNG files have been converted to .dpt format.\")\n","\n","\n","\n","# PNG values are already in mm → no scaling needed\n","scale = 1.0\n","\n","convert_all_png_to_dpt_in_classes(data_path, classes, scale)"]},{"cell_type":"markdown","metadata":{"id":"za-C--7N2egf"},"source":["### 3 - Generate Radius Map\n","\n","We compute the mesh and keypoints to:\n","\n","* Mesh, accurately represent an object’s 3D shape.\n","* Reduce complexity by sampling a few representative points.\n","* Enable fast registration and matching across views.\n","* Support precise pose estimation via 3D–2D point correspondences.\n","* Improve efficiency in real-time or learning-based applications.\n"]},{"cell_type":"markdown","metadata":{"id":"9ppcG2yKdz_s"},"source":["### **Generate Radius Map**"]},{"cell_type":"code","source":["from PIL import Image\n","import open3d as o3d\n","import os\n","from numba import jit, prange\n","import argparse\n","from typing import List\n","import numpy as np # Make sure numpy is imported\n","import cv2 # For reading depth images if not .dpt\n","from tqdm import tqdm # For progress bars\n","\n","# Try to import Google Colab drive module\n","try:\n","    from google.colab import drive\n","    COLAB_ENV = True\n","except ImportError:\n","    COLAB_ENV = False\n","\n","# List of Linemod classes to process\n","# classes = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15'] # Moved to argparse\n","\n","# Camera intrinsics (will cast to float64 later for max precision)\n","linemod_K = np.array([\n","    [572.4114,   0.0,      325.2611],\n","    [  0.0,      573.57043, 242.04899],\n","    [  0.0,        0.0,       1.0    ]\n","], dtype=np.float64)  # changed to float64 for higher precision\n","\n","# Default Paths (can be overridden by args)\n","DEFAULT_DATA_PATH = \"/content/dataset/Linemod_preprocessed/data/\"\n","DEFAULT_GDRIVE_OUTPUT_PATH = \"/content/drive/My Drive/Linemod_Radius_Maps_Output/\" # Default GDrive output\n","\n","# Depth scale: multiply raw depth value (read from PNG or DPT) by this factor to get millimetres.\n","# If PNG already mm use 1.0; if stored in metres*1000 choose accordingly.\n","# DEPTH_SCALE = 1.0 # Moved to argparse\n","\n","# Argument parser so users can adjust paths / scale easily\n","parser = argparse.ArgumentParser(description=\"Generate per-pixel radius maps for LINEMOD dataset with high-precision float64 computation and save to Google Drive.\")\n","parser.add_argument(\"--dataset_path\", type=str, default=DEFAULT_DATA_PATH,\n","                    help=f\"Root path to LINEMOD-Preprocessed data directory (default: {DEFAULT_DATA_PATH}).\")\n","parser.add_argument(\"--gdrive_output_path\", type=str, default=DEFAULT_GDRIVE_OUTPUT_PATH,\n","                    help=f\"Root path in Google Drive to save output folders (default: {DEFAULT_GDRIVE_OUTPUT_PATH}). Ensure Drive is mounted.\")\n","parser.add_argument(\"--classes\", type=str, nargs=\"*\", default=['12', '13', '14', '15'],\n","                    help=\"Space-separated list of class IDs to process, e.g. 01 02 05 (default: ['01'])\")\n","parser.add_argument(\"--depth_scale\", type=float, default=1.0,\n","                    help=\"Scale factor that converts stored depth units to millimetres. Use 1.0 if depth already in mm; 1000 if depth in metres, etc. (default: 1.0)\")\n","\n","# Robust to unknown extra args (e.g., from Jupyter/Colab)\n","# In a script, you might use: args = parser.parse_args()\n","# For Colab/Jupyter, parse_known_args is safer if other cells define args\n","args, _ = parser.parse_known_args()\n","\n","# After parsing, override globals/set variables\n","data_path: str = args.dataset_path\n","gdrive_output_root_path: str = args.gdrive_output_path # Path for Google Drive output\n","classes: List[str] = args.classes\n","DEPTH_SCALE: float = args.depth_scale\n","\n","# Mount Google Drive if in Colab environment\n","if COLAB_ENV:\n","    print(f\"Attempting to mount Google Drive...\")\n","    drive.mount('/content/drive')\n","    print(f\"Google Drive mounted. Output will be saved to: {gdrive_output_root_path}\")\n","else:\n","    print(\"Not running in Google Colab. Output will be relative to the script's location or the specified gdrive_output_path if it's a local absolute path.\")\n","    # If not in Colab, gdrive_output_root_path will be treated as a local path.\n","    # You might want to add a check here if gdrive_output_path must be a GDrive path.\n","\n","# END argparse section\n","\n","def project(xyz: np.ndarray, K: np.ndarray, RT: np.ndarray):\n","    \"\"\"Project 3D points into the image plane (float64 precision).\"\"\"\n","    xyz_cam = xyz @ RT[:, :3].T + RT[:, 3:].T\n","    proj    = xyz_cam @ K.T\n","    xy      = proj[:, :2] / proj[:, 2:3]\n","    return xy, xyz_cam\n","\n","def read_depth(path):\n","    \"\"\"Read .dpt or fallback PNG depth.\"\"\"\n","    if path.endswith(\".dpt\"):\n","        with open(path, \"rb\") as f:\n","            h, w  = np.fromfile(f, dtype=np.uint32, count=2)\n","            data  = np.fromfile(f, dtype=np.uint16, count=h*w)\n","        return data.reshape((h, w)).astype(np.float64)\n","    else:\n","        # Use cv2 to preserve original bit depth (e.g., 16-bit PNG)\n","        d = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","        if d is None:\n","            raise IOError(f\"Cannot read depth image: {path}\")\n","        return d.astype(np.float64)\n","\n","# INSERT kernel definition HERE\n","@jit(nopython=True, parallel=True, fastmath=True)\n","def compute_radius_map(depth: np.ndarray, kp_cam: np.ndarray, K: np.ndarray) -> np.ndarray:\n","    \"\"\"Compute per-pixel Euclidean distance (in mm) to kp_cam with float64 precision.\"\"\"\n","    H, W = depth.shape\n","    radius_map = np.zeros((H, W), dtype=np.float64)\n","    fx = K[0, 0]\n","    fy = K[1, 1]\n","    cx = K[0, 2]\n","    cy = K[1, 2]\n","    for v in prange(H):\n","        for u in range(W):\n","            z = depth[v, u]\n","            if z == 0.0:\n","                continue\n","            x = (u - cx) * z / fx\n","            y = (v - cy) * z / fy\n","            dx = x - kp_cam[0]\n","            dy = y - kp_cam[1]\n","            dz = z - kp_cam[2]\n","            radius_map[v, u] = (dx * dx + dy * dy + dz * dz) ** 0.5\n","    return radius_map\n","# END kernel\n","\n","# Track last folder per class\n","last_folder = {}\n","\n","for cls in classes:\n","    depth_dir = os.path.join(data_path, cls, \"depth\")\n","    if not os.path.isdir(depth_dir):\n","        print(f\"Warning: Depth directory not found for class {cls}: {depth_dir}. Skipping.\")\n","        continue\n","\n","    dpt_files = [f for f in os.listdir(depth_dir) if f.endswith(\".dpt\") or f.endswith(\".png\")] # Allow PNG depth too\n","    if not dpt_files:\n","        print(f\"Warning: No depth files (.dpt or .png) found in {depth_dir} for class {cls}. Skipping.\")\n","        continue\n","\n","    keypts_path = os.path.join(data_path, cls, \"Outside9.npy\")\n","    if not os.path.isfile(keypts_path):\n","        print(f\"Warning: Keypoints file not found for class {cls}: {keypts_path}. Skipping class.\")\n","        continue\n","    keypts = np.load(keypts_path)\n","\n","    # Print header\n","    print(\"\\n\" + f\" Class {cls}\".center(80, \"─\"))\n","\n","    # One progress bar for this entire class\n","    total_steps = len(keypts) * len(dpt_files)\n","    pbar = tqdm(\n","        total=total_steps,\n","        ncols=80,\n","        bar_format=\"{desc} │{bar}| {n_fmt}/{total_fmt}\"\n","    )\n","\n","    # Preload mesh (optional, consider error handling if file not found)\n","    mesh_path = os.path.join(data_path, cls, \"mesh.ply\")\n","    if os.path.isfile(mesh_path):\n","        _ = o3d.io.read_point_cloud(mesh_path)\n","    else:\n","        print(f\"Warning: Mesh file not found for class {cls}: {mesh_path}\")\n","\n","\n","    # Process each keypoint folder\n","    for idx_pt, kp in enumerate(keypts, start=1):\n","        folder_name = f\"Out_pt{idx_pt}_dm\"\n","        # THIS IS THE MODIFIED OUTPUT PATH for Google Drive\n","        out_folder  = os.path.join(gdrive_output_root_path, cls, folder_name)\n","\n","        try:\n","            os.makedirs(out_folder, exist_ok=True)\n","        except OSError as e:\n","            print(f\"Error creating directory {out_folder}: {e}. Skipping keypoint {idx_pt} for class {cls}.\")\n","            # Update pbar for all files related to this skipped keypoint\n","            pbar.update(len(dpt_files))\n","            continue # Skip to the next keypoint\n","\n","        last_folder[cls] = os.path.join(cls, folder_name) # Store relative path for summary\n","\n","        for fname in dpt_files:\n","            # Update description to show current folder\n","            pbar.set_description(f\" Class {cls} | 🔑 {folder_name}\")\n","\n","            # Compose related file paths\n","            base = os.path.splitext(fname)[0]\n","            depth_path = os.path.join(depth_dir, fname) # fname already includes extension\n","\n","            # Determine mask file extension (could be .png for .dpt or .png depth)\n","            mask_fname = base + \".png\" # Assume mask is always png\n","            mask_path  = os.path.join(data_path, cls, \"mask\", mask_fname)\n","            pose_path  = os.path.join(data_path, cls, \"pose\", f\"pose{base}.npy\")\n","\n","            # Check existence\n","            if not os.path.isfile(depth_path): # Should always exist due to dpt_files list comp\n","                print(f\" Skipped {cls}/{fname}: missing depth file {depth_path}\")\n","                pbar.update(1)\n","                continue\n","            if not os.path.isfile(pose_path):\n","                print(f\" Skipped {cls}/{fname}: missing pose file {pose_path}\")\n","                pbar.update(1)\n","                continue\n","            if not os.path.isfile(mask_path):\n","                print(f\" Skipped {cls}/{fname}: missing mask file {mask_path}\")\n","                pbar.update(1)\n","                continue\n","\n","            # Read & mask depth\n","            try:\n","                depth = read_depth(depth_path) * DEPTH_SCALE  # now float64\n","                mask_img = Image.open(mask_path)\n","                # Ensure mask is L mode (grayscale) before converting to numpy array\n","                if mask_img.mode != 'L':\n","                    mask_img = mask_img.convert(\"L\")\n","                mask  = np.asarray(mask_img)\n","                depth[mask == 0] = 0.0\n","            except Exception as e:\n","                print(f\" Error processing depth/mask for {cls}/{fname}: {e}\")\n","                pbar.update(1)\n","                continue\n","\n","\n","            # Load pose matrix (ensure float64)\n","            RT = np.load(pose_path).astype(np.float64)\n","\n","            # Compute keypoint position in camera space (float64)\n","            _, kp_cam_xyz = project(np.array([kp], dtype=np.float64), linemod_K, RT)\n","            kp_cam = kp_cam_xyz[0]\n","\n","            # Build & save radius map using new kernel\n","            radius_map = compute_radius_map(depth, kp_cam, linemod_K)\n","\n","            output_npy_path = os.path.join(out_folder, base + \".npy\")\n","            try:\n","                np.save(output_npy_path, radius_map)\n","            except Exception as e:\n","                print(f\"Error saving radius map to {output_npy_path}: {e}\")\n","\n","            pbar.update(1)\n","\n","    # Close this class's bar\n","    pbar.close()\n","\n","# Final summary\n","print(\"\\n\" + \" Summary of Output Locations (relative to GDrive output path)\".center(80, \"─\"))\n","print(f\"Google Drive Output Root: {gdrive_output_root_path}\")\n","for cls_processed in classes: # Iterate over initially requested classes\n","    if cls_processed in last_folder:\n","        folder = last_folder.get(cls_processed)\n","        print(f\"Class {cls_processed} \\t→ \\t{folder}\")\n","    else:\n","        # Check if class was skipped early due to missing dirs/files\n","        class_data_path = os.path.join(data_path, cls_processed)\n","        if not os.path.isdir(class_data_path):\n","             print(f\"Class {cls_processed} \\t→ \\tSkipped (data directory not found)\")\n","        elif not os.path.isfile(os.path.join(class_data_path, \"Outside9.npy\")):\n","             print(f\"Class {cls_processed} \\t→ \\tSkipped (keypoints file not found)\")\n","        elif not any(f.endswith((\".dpt\", \".png\")) for f in os.listdir(os.path.join(class_data_path, \"depth\"))):\n","             print(f\"Class {cls_processed} \\t→ \\tSkipped (no depth files found)\")\n","        else:\n","             print(f\"Class {cls_processed} \\t→ \\t— (No output generated, check warnings)\")\n","\n","\n","print(\" All processing complete\".center(80, \"─\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APk0T30mQ5NN","executionInfo":{"status":"ok","timestamp":1748811065312,"user_tz":-120,"elapsed":995589,"user":{"displayName":"ml6d","userId":"04537147647663067774"}},"outputId":"1b24c35b-7c08-41da-85b1-b4972436593e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to mount Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Google Drive mounted. Output will be saved to: /content/drive/My Drive/Linemod_Radius_Maps_Output/\n","\n","─────────────────────────────────── Class 12────────────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":[" Class 12 | 🔑 Out_pt9_dm:  │██████████████████████████████████████| 11133/11133\n"]},{"output_type":"stream","name":"stdout","text":["\n","─────────────────────────────────── Class 13────────────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":[" Class 13 | 🔑 Out_pt9_dm:  │██████████████████████████████████████| 10368/10368\n"]},{"output_type":"stream","name":"stdout","text":["\n","─────────────────────────────────── Class 14────────────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":[" Class 14 | 🔑 Out_pt9_dm:  │██████████████████████████████████████| 11043/11043\n"]},{"output_type":"stream","name":"stdout","text":["\n","─────────────────────────────────── Class 15────────────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │                                          | 8/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │                                         | 28/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │█▏                                      | 321/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │█▍                                      | 390/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │█▌                                      | 421/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │█▌                                      | 451/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │██▏                                     | 625/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │██▎                                     | 635/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │██▍                                     | 690/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │███▎                                    | 914/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"," Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │███▍                                    | 944/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │███▉                                   | 1142/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt1_dm:  │████                                   | 1166/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │████▍                                  | 1262/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"," Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │█████▍                                 | 1565/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │█████▋                                 | 1637/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │█████▊                                 | 1668/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │█████▉                                 | 1697/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │██████▌                                | 1871/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │██████▋                                | 1936/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │███████▌                               | 2160/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"," Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │███████▌                               | 2187/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │████████▎                              | 2386/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt2_dm:  │████████▍                              | 2412/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │████████▋                              | 2505/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"," Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │█████████▊                             | 2812/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │██████████                             | 2880/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │██████████▏                            | 2908/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │██████████▏                            | 2936/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │██████████▊                            | 3116/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │███████████                            | 3178/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │███████████▊                           | 3400/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"," Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │███████████▉                           | 3429/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │████████████▋                          | 3627/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt3_dm:  │████████████▋                          | 3654/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │█████████████                          | 3744/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │█████████████                          | 3760/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │██████████████▏                        | 4053/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │██████████████▍                        | 4124/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │██████████████▍                        | 4144/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │██████████████▌                        | 4178/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │███████████████▏                       | 4356/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │███████████████▍                       | 4423/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │████████████████▏                      | 4641/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │████████████████▏                      | 4655/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │████████████████▎                      | 4676/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │█████████████████                      | 4881/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt4_dm:  │█████████████████                      | 4899/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │█████████████████▍                     | 4992/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"," Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │██████████████████▍                    | 5295/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │██████████████████▋                    | 5366/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │██████████████████▊                    | 5397/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │██████████████████▉                    | 5425/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │███████████████████▌                   | 5602/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │███████████████████▋                   | 5664/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │████████████████████▌                  | 5886/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"," Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │████████████████████▋                  | 5918/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │█████████████████████▎                 | 6122/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │█████████████████████▎                 | 6128/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt5_dm:  │█████████████████████▍                 | 6142/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │█████████████████████▋                 | 6235/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"," Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │██████████████████████▊                | 6533/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │███████████████████████                | 6608/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │███████████████████████▏               | 6640/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │███████████████████████▏               | 6656/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │███████████████████████▎               | 6670/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │███████████████████████▊               | 6846/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │████████████████████████               | 6909/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │████████████████████████▊              | 7131/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"," Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │████████████████████████▉              | 7163/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │█████████████████████████▋             | 7366/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt6_dm:  │█████████████████████████▋             | 7382/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │██████████████████████████             | 7477/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"," Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │███████████████████████████▏           | 7782/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │███████████████████████████▎           | 7850/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │███████████████████████████▍           | 7882/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │███████████████████████████▌           | 7910/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │████████████████████████████▏          | 8087/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │████████████████████████████▍          | 8153/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │█████████████████████████████▏         | 8371/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │█████████████████████████████▏         | 8382/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │█████████████████████████████▎         | 8405/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │██████████████████████████████         | 8610/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt7_dm:  │██████████████████████████████         | 8628/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │██████████████████████████████▍        | 8717/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │██████████████████████████████▍        | 8732/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │███████████████████████████████▍       | 9023/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │███████████████████████████████▋       | 9094/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │███████████████████████████████▊       | 9121/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │███████████████████████████████▉       | 9152/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │████████████████████████████████▌      | 9331/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │████████████████████████████████▊      | 9396/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │█████████████████████████████████▍     | 9605/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │█████████████████████████████████▌     | 9627/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │█████████████████████████████████▋     | 9648/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │██████████████████████████████████▎    | 9843/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt8_dm:  │██████████████████████████████████▍    | 9870/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │██████████████████████████████████▋    | 9963/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001232.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001232.png\n"," Skipped 15/001236.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001236.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │██████████████████████████████████▉   | 10268/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001234.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001234.png\n"," Skipped 15/001242.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001242.png\n"," Skipped 15/001228.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001228.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │███████████████████████████████████   | 10338/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001231.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001231.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │███████████████████████████████████▏  | 10356/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001241.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001241.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │███████████████████████████████████▎  | 10396/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001233.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001233.png\n"," Skipped 15/001225.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001225.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │███████████████████████████████████▉  | 10572/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001239.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001239.png\n"," Skipped 15/001230.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001230.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │████████████████████████████████████  | 10629/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001227.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001227.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │████████████████████████████████████▉ | 10862/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001229.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001229.png\n"," Skipped 15/001235.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001235.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │████████████████████████████████████▉ | 10890/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001238.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001238.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │█████████████████████████████████████▋| 11094/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001226.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001226.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │█████████████████████████████████████▋| 11111/11187"]},{"output_type":"stream","name":"stdout","text":[" Skipped 15/001240.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001240.png\n"," Skipped 15/001237.dpt: missing mask file /content/dataset/Linemod_preprocessed/data/15/mask/001237.png\n"]},{"output_type":"stream","name":"stderr","text":[" Class 15 | 🔑 Out_pt9_dm:  │██████████████████████████████████████| 11187/11187"]},{"output_type":"stream","name":"stdout","text":["\n","───────── Summary of Output Locations (relative to GDrive output path)──────────\n","Google Drive Output Root: /content/drive/My Drive/Linemod_Radius_Maps_Output/\n","Class 12 \t→ \t12/Out_pt9_dm\n","Class 13 \t→ \t13/Out_pt9_dm\n","Class 14 \t→ \t14/Out_pt9_dm\n","Class 15 \t→ \t15/Out_pt9_dm\n","──────────────────────────── All processing complete────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"qx11kUkndrmA"},"source":["### **Train Model**"]},{"cell_type":"markdown","metadata":{"id":"bT_d0-U-gkpW"},"source":["### Split Data: %70: Train - %20 Validation - %10 Test"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23398,"status":"ok","timestamp":1748812921232,"user":{"displayName":"ml6d","userId":"04537147647663067774"},"user_tz":-120},"id":"_MLHwzbAvO4m","outputId":"e76f3c87-ffa0-4954-c0ad-4d116f0516e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Splitting RGB frames for class 01 ---\n"," Class 01: train=865, val=247, test=124\n","\n","--- Splitting RGB frames for class 02 ---\n"," Class 02: train=849, val=243, test=122\n","\n","--- Splitting RGB frames for class 04 ---\n"," Class 04: train=840, val=240, test=121\n","\n","--- Splitting RGB frames for class 05 ---\n"," Class 05: train=837, val=239, test=120\n","\n","--- Splitting RGB frames for class 06 ---\n"," Class 06: train=825, val=236, test=118\n","\n","--- Splitting RGB frames for class 08 ---\n"," Class 08: train=831, val=238, test=119\n","\n","--- Splitting RGB frames for class 09 ---\n"," Class 09: train=877, val=251, test=126\n","\n","--- Splitting RGB frames for class 10 ---\n"," Class 10: train=877, val=250, test=126\n","\n","--- Splitting RGB frames for class 11 ---\n"," Class 11: train=854, val=244, test=122\n","\n","--- Splitting RGB frames for class 12 ---\n"," Class 12: train=865, val=248, test=124\n","\n","--- Splitting RGB frames for class 13 ---\n"," Class 13: train=806, val=230, test=116\n","\n","--- Splitting RGB frames for class 14 ---\n"," Class 14: train=858, val=246, test=123\n","\n","--- Splitting RGB frames for class 15 ---\n"," Class 15: train=870, val=248, test=125\n","\n","All classes processed successfully.\n"]}],"source":["#Split RGB Files to Train Validate Test\n","import os\n","import glob\n","import yaml\n","import random\n","\n","# Base directory containing class subfolders\n","data_path = '/content/dataset/Linemod_preprocessed/data'\n","base_dir = os.path.join(data_path)\n","classes = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n","# Fixed seed for reproducibility in splitting\n","random.seed(42)\n","\n","for cls in classes:\n","    print(f\"\\n--- Splitting RGB frames for class {cls} ---\")\n","\n","    # 1) Gather all RGB image indices (filenames without extension)\n","    rgb_dir = os.path.join(base_dir, cls, \"rgb\")\n","    rgb_paths = sorted(glob.glob(os.path.join(rgb_dir, \"*.png\")))\n","    rgb_idx = [os.path.splitext(os.path.basename(p))[0] for p in rgb_paths]\n","\n","    # 2) Load GT keys from gt.yml, zero-pad them to six digits\n","    gt_path = os.path.join(base_dir, cls, \"gt.yml\")\n","    if not os.path.isfile(gt_path):\n","        print(f\"WARNING: gt.yml not found for class {cls}, skipping.\")\n","        continue\n","\n","    with open(gt_path, \"r\") as f:\n","        gt = yaml.safe_load(f)\n","    gt_keys = {f\"{int(k):06d}\" for k in gt.keys()}\n","\n","    # 3) Keep only indices present in both RGB and GT\n","    valid_idx = [i for i in rgb_idx if i in gt_keys]\n","    if not valid_idx:\n","        print(f\"WARNING: No valid frames for class {cls}, skipping.\")\n","        continue\n","\n","    # 4) Shuffle and split 70% train / 20% val / 10% test\n","    random.shuffle(valid_idx)\n","    n = len(valid_idx)\n","    cut1 = int(0.7 * n)\n","    cut2 = int(0.9 * n)\n","    train_idx = valid_idx[:cut1]\n","    val_idx   = valid_idx[cut1:cut2]\n","    test_idx  = valid_idx[cut2:]\n","\n","    # 5) Write splits to text files under a new \"Split\" folder\n","    split_dir = os.path.join(base_dir, cls, \"Split\")\n","    os.makedirs(split_dir, exist_ok=True)\n","    with open(os.path.join(split_dir, \"train.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(train_idx))\n","    with open(os.path.join(split_dir, \"val.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(val_idx))\n","    with open(os.path.join(split_dir, \"test.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(test_idx))\n","\n","    print(f\" Class {cls}: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n","\n","print(\"\\nAll classes processed successfully.\")"]},{"cell_type":"markdown","source":["### Normalization over Dataset"],"metadata":{"id":"Dptivg10sXLy"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import argparse\n","from tqdm import tqdm\n","\n","# Try to import Google Colab drive module\n","try:\n","    from google.colab import drive\n","    COLAB_ENV = True\n","except ImportError:\n","    COLAB_ENV = False\n","\n","# ===================== Argument parser =====================\n","parser = argparse.ArgumentParser(\n","    description=\"Dataset sanitation & normalization (RGB, depth, mask, pose, radius maps) for LINEMOD-Preprocessed.\")\n","parser.add_argument(\"--dataset_path\", type=str, default=\"/content/dataset/Linemod_preprocessed/data\",\n","                    help=\"Root path to the main dataset (RGB, original depth, masks, poses, etc.) (default: Colab location)\")\n","parser.add_argument(\"--radius_maps_gdrive_path\", type=str, default=\"/content/drive/My Drive/Linemod_Radius_Maps_Output\",\n","                    help=\"Root path in Google Drive where the generated radius map .npy files are stored (default: /content/drive/My Drive/Linemod_Radius_Maps_Output/)\")\n","parser.add_argument(\"--classes\", type=str, nargs=\"*\", default=None,\n","                    help=\"Classes to process (e.g. 01 02 05). Default: all numeric folders in dataset_path\")\n","parser.add_argument(\"--depth_unit_scale\", type=float, default=1000.0,\n","                    help=\"Factor to convert original depth from millimetres to metres (default: 1000 for .dpt files assumed to be in mm)\")\n","parser.add_argument(\"--radius_unit_scale\", type=float, default=1000.0, # This applies to radius maps\n","                    help=\"Factor to convert radius map values from millimetres to metres if they are not already. (default: 1000)\")\n","parser.add_argument(\"--depth_max_m\", type=float, default=10.0,\n","                    help=\"Maximum valid depth in metres for clipping & scaling original depth (default: 10.0)\")\n","args, _ = parser.parse_known_args()\n","\n","ROOT_DATASET = args.dataset_path # Path for original dataset (RGB, depth, mask, pose)\n","ROOT_RADIUS_MAPS_GDRIVE = args.radius_maps_gdrive_path # Path for radius maps on Google Drive\n","\n","# Mount Google Drive if in Colab environment and GDrive path is specified\n","if COLAB_ENV and ROOT_RADIUS_MAPS_GDRIVE.startswith(\"/content/drive/\"):\n","    print(f\"Attempting to mount Google Drive for radius maps...\")\n","    drive.mount('/content/drive')\n","    print(f\"Google Drive mounted. Radius maps will be accessed from/saved to: {ROOT_RADIUS_MAPS_GDRIVE}\")\n","elif not COLAB_ENV and ROOT_RADIUS_MAPS_GDRIVE.startswith(\"/content/drive/\"):\n","    print(f\"Warning: Script not in Colab, but GDrive path '{ROOT_RADIUS_MAPS_GDRIVE}' looks like a Colab GDrive path. Ensure it's accessible or change the path.\")\n","else:\n","    print(f\"Radius maps will be accessed from/saved to: {ROOT_RADIUS_MAPS_GDRIVE}\")\n","    if COLAB_ENV:\n","         print(\"Note: Not a /content/drive/ path, Google Drive mounting not strictly needed for this path unless it's a symlink.\")\n","\n","\n","# --------------------- derive class list (from main dataset path) -------------------\n","if args.classes:\n","    CLASSES = args.classes\n","else:\n","    # all directories with two-digit names in the main dataset path\n","    if not os.path.isdir(ROOT_DATASET):\n","        print(f\"Error: Main dataset path not found: {ROOT_DATASET}. Cannot derive classes. Exiting.\")\n","        exit()\n","    CLASSES = sorted([d for d in os.listdir(ROOT_DATASET) if os.path.isdir(os.path.join(ROOT_DATASET, d)) and d.isdigit()])\n","    if not CLASSES:\n","        print(f\"Warning: No numeric class folders found in {ROOT_DATASET}. Check --dataset_path.\")\n","\n","\n","OUT_PTS  = [f\"Out_pt{i}_dm\" for i in range(1,10)] # Corresponds to subdirs for radius maps\n","MEAN_RGB = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","STD_RGB  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","DEPTH_MAX = args.depth_max_m # For original depth files, not radius maps directly\n","\n","def read_depth_dpt_auto(path: str) -> np.ndarray:\n","    \"\"\"Read .dpt that can be either uint16 (mm) or float32 (m) based on file size.\"\"\"\n","    if not os.path.isfile(path):\n","        raise FileNotFoundError(f\"Depth file not found: {path}\")\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        f.seek(0, os.SEEK_END)\n","        file_size = f.tell()\n","        data_bytes = file_size - 8 # minus header (h, w)\n","        f.seek(8) # go back to data start\n","\n","        expected_u16_bytes = h * w * np.dtype(np.uint16).itemsize\n","        expected_f32_bytes = h * w * np.dtype(np.float32).itemsize\n","\n","        if data_bytes == expected_f32_bytes:\n","            data = np.fromfile(f, dtype=np.float32, count=h*w)\n","            # print(f\"DEBUG: Read {path} as float32 (metres)\")\n","        elif data_bytes == expected_u16_bytes:\n","            data = np.fromfile(f, dtype=np.uint16, count=h*w).astype(np.float32)\n","            # print(f\"DEBUG: Read {path} as uint16 (mm), converted to float32\")\n","        else:\n","            raise ValueError(f\"Unexpected data size in {path}. Expected {expected_u16_bytes} (for uint16) or {expected_f32_bytes} (for float32) bytes of data, got {data_bytes}.\")\n","    return data.reshape(h, w)\n","\n","def sep(char=\"─\"):\n","    print(char * 80)\n","\n","if not CLASSES:\n","    print(\"No classes to process. Exiting.\")\n","    exit()\n","\n","for cls in CLASSES:\n","    # cls_dir refers to the main dataset directory for this class (for RGB, mask, original depth, pose)\n","    cls_data_dir = os.path.join(ROOT_DATASET, cls)\n","    if not os.path.isdir(cls_data_dir):\n","        sep()\n","        print(f\"🚫 Main data folder for class missing: {cls_data_dir} (skipped for RGB, depth, etc.)\")\n","        # We might still want to process radius maps if they exist in GDrive for this class\n","        # So, we continue to the radius map processing part.\n","    else:\n","         print(f\"\\n📂 Main data for Class {cls} at: {cls_data_dir} \".center(80, \"─\"))\n","\n","\n","    # ---------- 1) Normalize radius-maps (from/to Google Drive path) ----------\n","    sep()\n","    print(f\"🌐 Radius maps for Class {cls} (from/to GDrive: {ROOT_RADIUS_MAPS_GDRIVE})\".center(80,\" \"))\n","\n","    # The class directory for radius maps is under the GDrive root\n","    cls_radius_gdrive_dir = os.path.join(ROOT_RADIUS_MAPS_GDRIVE, cls)\n","    if not os.path.isdir(cls_radius_gdrive_dir):\n","        print(f\"ℹ️  No radius map directory found for class {cls} in GDrive: {cls_radius_gdrive_dir}\")\n","    else:\n","        for out_subdir_name in OUT_PTS: # e.g., \"Out_pt1_dm\"\n","            # Path to the specific keypoint's radius map directory within the class folder on GDrive\n","            radius_map_kp_dir = os.path.join(cls_radius_gdrive_dir, out_subdir_name)\n","\n","            if not os.path.isdir(radius_map_kp_dir):\n","                # This is normal if a class doesn't have all 9 keypoint outputs\n","                # print(f\"  ℹ️  Radius map subdir missing: {radius_map_kp_dir}\")\n","                continue\n","\n","            files = [f for f in os.listdir(radius_map_kp_dir) if f.endswith(\".npy\")]\n","            if not files:\n","                # print(f\"  ℹ️  No .npy files found in {radius_map_kp_dir}\")\n","                continue\n","\n","            print(f\"   normalizing: {cls}/{out_subdir_name}\")\n","            for fn in tqdm(files, desc=f\"  {out_subdir_name}\", ncols=80, leave=False):\n","                path = os.path.join(radius_map_kp_dir, fn) # Full path to the .npy file on GDrive\n","                try:\n","                    arr = np.load(path)\n","                    if arr.size == 0:\n","                        raise ValueError(\"empty array loaded\")\n","\n","                    # Check if already scaled (e.g., max value is small, likely metres)\n","                    # The threshold 20.0 is heuristic; if max radius is > 20m, it's likely in mm.\n","                    if arr.max() > 20.0 and args.radius_unit_scale != 1.0 : # Check scale factor to avoid re-scaling if not needed\n","                        arr_scaled = arr / args.radius_unit_scale  # Convert mm -> m\n","                        np.save(path, arr_scaled.astype(np.float32)) # Save back to the same GDrive path\n","                        # print(f\"    Scaled {fn}: max {arr.max():.2f} -> {arr_scaled.max():.2f}\")\n","                    # else:\n","                        # print(f\"    Skipped scaling {fn}: max {arr.max():.2f} (already in metres or scale factor is 1)\")\n","\n","                except Exception as e:\n","                    print(f\"   ⚠️  [SKIP RADIUS MAP] {cls}/{out_subdir_name}/{fn} | {str(e)}\")\n","    # End of radius map normalization for class cls\n","\n","    # If the main data directory for the class doesn't exist, skip the rest\n","    if not os.path.isdir(cls_data_dir):\n","        print(f\"Skipping RGB, Depth, Mask, Pose for class {cls} as main data directory {cls_data_dir} not found.\")\n","        continue\n","\n","    # ---------- 2) Check & normalise train/val/test splits (using ROOT_DATASET) ----------\n","    split_dir = os.path.join(cls_data_dir, \"Split\") # Splits are in the main dataset\n","    if not os.path.isdir(split_dir):\n","        print(f\"ℹ️  Split directory not found for class {cls}: {split_dir}\")\n","    else:\n","        for split_file in (\"train.txt\", \"val.txt\", \"test.txt\"):\n","            split_path = os.path.join(split_dir, split_file)\n","            if not os.path.isfile(split_path):\n","                print(f\"ℹ️  No {split_file} present in {split_dir}\")\n","                continue\n","\n","            ids = []\n","            try:\n","                with open(split_path, 'r') as f_split:\n","                    ids = [int(line.strip()) for line in f_split if line.strip().isdigit()]\n","            except Exception as e:\n","                print(f\"Error reading split file {split_path}: {e}\")\n","                continue\n","\n","            if not ids:\n","                print(f\"ℹ️  No valid IDs found in {split_file}\")\n","                continue\n","\n","            sep(\".\")\n","            print(f\"🗂️  Split: {split_file:<10} │ {len(ids)} images (from {cls_data_dir})\")\n","\n","            # ---- RGB check (mean/std) ----\n","            print(f\"  Checking RGB files for {split_file}...\")\n","            for idx in tqdm(ids, desc=\"  RGB  \", ncols=80, leave=False):\n","                p = os.path.join(cls_data_dir, \"rgb\", f\"{idx:06d}.png\")\n","                try:\n","                    if not os.path.isfile(p): raise FileNotFoundError(\"RGB file missing\")\n","                    im = Image.open(p).convert(\"RGB\")\n","                    _  = (np.asarray(im, np.float32) / 255.0 - MEAN_RGB) / STD_RGB\n","                except Exception as e:\n","                    print(f\"   🚧 [SKIP RGB {cls}/rgb/{idx:06d}.png] │ {str(e)}\")\n","\n","            # ---- Original Depth normalisation (to 0-1 for inspection, from .dpt) ----\n","            print(f\"  Normalizing original Depth files for {split_file}...\")\n","            for idx in tqdm(ids, desc=\"  Depth\", ncols=80, leave=False):\n","                p = os.path.join(cls_data_dir, \"depth\", f\"{idx:06d}.dpt\")\n","                try:\n","                    dp = read_depth_dpt_auto(p) # Reads from cls_data_dir\n","                    # If max > 20, assume it's in mm (or some large unit), convert to metres\n","                    if dp.max() > 20.0 and args.depth_unit_scale != 1.0:\n","                        dp = dp / args.depth_unit_scale  # mm or other unit -> metres\n","\n","                    # Clip to DEPTH_MAX (in metres) and normalize to 0-1 for potential inspection/visualization\n","                    # Note: For actual use in models, raw depth in metres is often preferred over 0-1 normalized.\n","                    # This step might be more for dataset validation/preview.\n","                    # dp_normalized_for_inspection = np.clip(dp, 0, DEPTH_MAX) / DEPTH_MAX\n","                    # If you need to save this normalized depth, you would do it here.\n","                    # For now, it's just a check.\n","                    _ = np.clip(dp, 0, DEPTH_MAX) / DEPTH_MAX\n","\n","                except FileNotFoundError:\n","                    print(f\"   🚧 [SKIP DEPTH {cls}/depth/{idx:06d}.dpt] │ File not found\")\n","                except Exception as e:\n","                    print(f\"   🚧 [SKIP DEPTH {cls}/depth/{idx:06d}.dpt] │ {str(e)}\")\n","\n","            # ---- Mask check ----\n","            print(f\"  Checking Mask files for {split_file}...\")\n","            for idx in tqdm(ids, desc=\"  Mask \", ncols=80, leave=False):\n","                p = os.path.join(cls_data_dir, \"mask\", f\"{idx:06d}.png\")\n","                try:\n","                    if not os.path.isfile(p): raise FileNotFoundError(\"Mask file missing\")\n","                    _ = np.asarray(Image.open(p).convert(\"L\"), np.uint8)\n","                except Exception as e:\n","                    print(f\"   🚧 [SKIP MASK {cls}/mask/{idx:06d}.png] │ {str(e)}\")\n","\n","            # ---- Pose translation fix (if in mm, convert to m) ----\n","            print(f\"  Checking Pose files for {split_file}...\")\n","            for idx in tqdm(ids, desc=\"  Pose \", ncols=80, leave=False):\n","                p = os.path.join(cls_data_dir, \"pose\", f\"pose{idx:06d}.npy\")\n","                try:\n","                    if not os.path.isfile(p): raise FileNotFoundError(\"Pose file missing\")\n","                    pose = np.load(p).astype(np.float32)\n","                    t    = pose[:3,3]\n","                    # If norm of translation vector > 20 (likely mm), convert to m\n","                    if np.linalg.norm(t) > 20.0: # Heuristic for mm vs m\n","                        # print(f\"    Converting pose {idx:06d} from mm to m (translation: {t})\")\n","                        pose[:3,3] = t / 1000.0  # Assuming original was mm\n","                        np.save(p, pose) # Save back to the main dataset path\n","                except Exception as e:\n","                    print(f\"   🚧 [SKIP POSE {cls}/pose/pose{idx:06d}.npy] │ {str(e)}\")\n","    sep()\n","    print(f\"  Done Class {cls} \".center(80, \"=\"))\n","    sep()\n","\n","print(\"\\nAll specified classes processed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCouNlsqbwpJ","executionInfo":{"status":"ok","timestamp":1748823565352,"user_tz":-120,"elapsed":10580115,"user":{"displayName":"ml6d","userId":"04537147647663067774"}},"outputId":"c906f11b-aca4-4fb6-a3eb-07a69fc9441d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to mount Google Drive for radius maps...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Google Drive mounted. Radius maps will be accessed from/saved to: /content/drive/My Drive/Linemod_Radius_Maps_Output\n","──\n","📂 Main data for Class 01 at: /content/dataset/Linemod_preprocessed/data/01 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 01 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 01/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 01/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 865 images (from /content/dataset/Linemod_preprocessed/data/01)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 247 images (from /content/dataset/Linemod_preprocessed/data/01)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 124 images (from /content/dataset/Linemod_preprocessed/data/01)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 01 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 02 at: /content/dataset/Linemod_preprocessed/data/02 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 02 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 02/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 02/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 849 images (from /content/dataset/Linemod_preprocessed/data/02)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 243 images (from /content/dataset/Linemod_preprocessed/data/02)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 122 images (from /content/dataset/Linemod_preprocessed/data/02)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 02 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 04 at: /content/dataset/Linemod_preprocessed/data/04 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 04 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 04/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 04/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 840 images (from /content/dataset/Linemod_preprocessed/data/04)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 240 images (from /content/dataset/Linemod_preprocessed/data/04)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 121 images (from /content/dataset/Linemod_preprocessed/data/04)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 04 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 05 at: /content/dataset/Linemod_preprocessed/data/05 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 05 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 05/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 05/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 837 images (from /content/dataset/Linemod_preprocessed/data/05)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 239 images (from /content/dataset/Linemod_preprocessed/data/05)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 120 images (from /content/dataset/Linemod_preprocessed/data/05)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 05 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 06 at: /content/dataset/Linemod_preprocessed/data/06 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 06 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 06/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 06/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 825 images (from /content/dataset/Linemod_preprocessed/data/06)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 236 images (from /content/dataset/Linemod_preprocessed/data/06)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 118 images (from /content/dataset/Linemod_preprocessed/data/06)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 06 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 08 at: /content/dataset/Linemod_preprocessed/data/08 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 08 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 08/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 08/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 831 images (from /content/dataset/Linemod_preprocessed/data/08)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 238 images (from /content/dataset/Linemod_preprocessed/data/08)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 119 images (from /content/dataset/Linemod_preprocessed/data/08)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 08 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 09 at: /content/dataset/Linemod_preprocessed/data/09 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 09 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 09/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 09/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 877 images (from /content/dataset/Linemod_preprocessed/data/09)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 251 images (from /content/dataset/Linemod_preprocessed/data/09)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 126 images (from /content/dataset/Linemod_preprocessed/data/09)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 09 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 10 at: /content/dataset/Linemod_preprocessed/data/10 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 10 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 10/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 10/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 877 images (from /content/dataset/Linemod_preprocessed/data/10)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 250 images (from /content/dataset/Linemod_preprocessed/data/10)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 126 images (from /content/dataset/Linemod_preprocessed/data/10)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 10 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 11 at: /content/dataset/Linemod_preprocessed/data/11 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 11 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 11/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 11/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 854 images (from /content/dataset/Linemod_preprocessed/data/11)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 244 images (from /content/dataset/Linemod_preprocessed/data/11)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 122 images (from /content/dataset/Linemod_preprocessed/data/11)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 11 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 12 at: /content/dataset/Linemod_preprocessed/data/12 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 12 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 12/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 12/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 865 images (from /content/dataset/Linemod_preprocessed/data/12)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 248 images (from /content/dataset/Linemod_preprocessed/data/12)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 124 images (from /content/dataset/Linemod_preprocessed/data/12)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 12 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 13 at: /content/dataset/Linemod_preprocessed/data/13 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 13 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 13/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 13/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 806 images (from /content/dataset/Linemod_preprocessed/data/13)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 230 images (from /content/dataset/Linemod_preprocessed/data/13)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 116 images (from /content/dataset/Linemod_preprocessed/data/13)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 13 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 14 at: /content/dataset/Linemod_preprocessed/data/14 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 14 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 14/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 14/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 858 images (from /content/dataset/Linemod_preprocessed/data/14)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 246 images (from /content/dataset/Linemod_preprocessed/data/14)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 123 images (from /content/dataset/Linemod_preprocessed/data/14)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 14 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","──\n","📂 Main data for Class 15 at: /content/dataset/Linemod_preprocessed/data/15 ──\n","────────────────────────────────────────────────────────────────────────────────\n","🌐 Radius maps for Class 15 (from/to GDrive: /content/drive/My Drive/Linemod_Radius_Maps_Output)\n","   normalizing: 15/Out_pt1_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt2_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt3_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt4_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt5_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt6_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt7_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt8_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   normalizing: 15/Out_pt9_dm\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: train.txt  │ 870 images (from /content/dataset/Linemod_preprocessed/data/15)\n","  Checking RGB files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :   3%|█                               | 30/870 [00:00<00:02, 295.87it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001226.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001228.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  17%|█████▎                         | 148/870 [00:00<00:02, 289.24it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001236.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001234.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001238.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  31%|█████████▌                     | 268/870 [00:00<00:02, 292.89it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001230.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001242.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  38%|███████████▊                   | 331/870 [00:01<00:01, 299.05it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001227.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  56%|█████████████████▍             | 489/870 [00:01<00:01, 311.76it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001241.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001232.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001235.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  74%|███████████████████████        | 647/870 [00:02<00:00, 307.25it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001229.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001225.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  96%|█████████████████████████████▋ | 833/870 [00:02<00:00, 297.64it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001231.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for train.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: val.txt    │ 248 images (from /content/dataset/Linemod_preprocessed/data/15)\n","  Checking RGB files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  63%|███████████████████▋           | 157/248 [00:00<00:00, 309.00it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001237.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":["  Mask :  89%|███████████████████████████▌   | 220/248 [00:00<00:00, 306.89it/s]"]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001239.png] │ Mask file missing\n","   🚧 [SKIP MASK 15/mask/001233.png] │ Mask file missing\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Pose files for val.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["................................................................................\n","🗂️  Split: test.txt   │ 125 images (from /content/dataset/Linemod_preprocessed/data/15)\n","  Checking RGB files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Normalizing original Depth files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Checking Mask files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   🚧 [SKIP MASK 15/mask/001240.png] │ Mask file missing\n","  Checking Pose files for test.txt...\n"]},{"output_type":"stream","name":"stderr","text":["                                                                                "]},{"output_type":"stream","name":"stdout","text":["────────────────────────────────────────────────────────────────────────────────\n","================================  Done Class 15 ================================\n","────────────────────────────────────────────────────────────────────────────────\n","\n","All specified classes processed.\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"markdown","source":["# **Train**"],"metadata":{"id":"2KsgE4ZxQxhJ"}},{"cell_type":"code","source":["!pip install colorama"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-sn2AhNkYBw","executionInfo":{"status":"ok","timestamp":1748823572772,"user_tz":-120,"elapsed":2710,"user":{"displayName":"ml6d","userId":"04537147647663067774"}},"outputId":"30ef215b-d2ab-4d0d-bd95-772573c36247"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.6\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Enhanced RCVPose: Deep Learning Model for 6D Pose Estimation (Unified Multi-Object Training)\n","\n","This script is designed to train a SINGLE 6D pose estimation model capable of handling\n","multiple objects, using RGB images, depth maps, masks, pose data, and radius maps.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import cv2\n","import os\n","from tqdm import tqdm\n","# import logging # logging module not explicitly used, TrainingLogger is custom\n","from datetime import datetime\n","from google.colab import drive\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.nn import functional as F\n","import time\n","from colorama import init, Fore, Style\n","from torch.cuda.amp import autocast, GradScaler\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","\n","# Initialize colorama for colored terminal output\n","init(autoreset=True)\n","\n","# =========================\n","# Configuration Section\n","# =========================\n","CONFIG = {\n","    # Base directory containing all object folders (01, 02, ...) for RGB, depth, mask, pose\n","    'BASE_DIR': '/content/dataset/Linemod_preprocessed/data',\n","    # Path for radius maps stored on Google Drive\n","    'RADIUS_MAPS_GDRIVE_PATH': '/content/drive/My Drive/Linemod_Radius_Maps_Output',\n","    # List of object IDs to train (all these will be part of the single model)\n","    'OBJECT_IDS': ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15'],\n","    # Subdirectory names inside each object folder\n","    'RGB_DIR': 'rgb',\n","    'DEPTH_DIR': 'depth',\n","    'MASK_DIR': 'mask',\n","    'POSE_DIR': 'pose',\n","    'RADIUS_PREFIX': 'Out_pt',\n","    'RADIUS_SUFFIX': '_dm',\n","    'NUM_RADIUS_POINTS': 9,\n","    'SPLIT_DIR': 'Split',\n","    'TRAIN_SPLIT': 'train.txt',\n","    'VAL_SPLIT': 'val.txt',\n","    'TEST_SPLIT': 'test.txt', # Not used in this training script but defined\n","    # Training parameters\n","    'BATCH_SIZE': 8,\n","    'NUM_WORKERS': 2,\n","    'NUM_EPOCHS': 100,\n","    'LEARNING_RATE': 0.001,\n","    'ACCUMULATION_STEPS': 4,\n","    # Model saving (local path, primarily for GDrive now)\n","    'MODELS_DIR': '/content/models_unified_temp', # Temp local dir if needed\n","    'GDRIVE_UNIFIED_MODELS_DIR': '/content/drive/MyDrive/models_unified' # Primary save location\n","}\n","\n","# =========================\n","# Logger for Training Output\n","# =========================\n","class TrainingLogger:\n","    def __init__(self):\n","        self.start_time = time.time()\n","        self.separator = \"=\" * 100\n","        self.sub_separator = \"-\" * 100\n","    def log_section(self, message):\n","        print(f\"\\n{Fore.CYAN}{self.separator}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}🚀 {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{self.separator}{Style.RESET_ALL}\\n\")\n","    def log_subsection(self, message, no_lines=False):\n","        if no_lines:\n","            print(f\"{Fore.YELLOW}📌 {message}{Style.RESET_ALL}\")\n","        else:\n","            print(f\"\\n{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\")\n","            print(f\"{Fore.YELLOW}📌 {message}{Style.RESET_ALL}\")\n","            print(f\"{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\\n\")\n","    def log_info(self, message):\n","        print(f\"{Fore.GREEN}ℹ️  {message}{Style.RESET_ALL}\")\n","    def log_warning(self, message):\n","        print(f\"{Fore.YELLOW}⚠️  {message}{Style.RESET_ALL}\")\n","    def log_error(self, message):\n","        print(f\"{Fore.RED}❌ {message}{Style.RESET_ALL}\")\n","    def log_success(self, message):\n","        print(f\"{Fore.GREEN}✅ {message}{Style.RESET_ALL}\")\n","    def log_metrics(self, metrics, phase=\"Training\", no_lines=False):\n","        header = f\"📊 {phase} Metrics:\"\n","        if no_lines:\n","            print(f\"{Fore.MAGENTA}{header}{Style.RESET_ALL}\")\n","        else:\n","            print(f\"\\n{Fore.MAGENTA}{header}{Style.RESET_ALL}\")\n","        for key, value in metrics.items():\n","            print(f\"{Fore.MAGENTA}   {key}: {value:.4f}{Style.RESET_ALL}\")\n","        print(f\"{Fore.MAGENTA}{'-' * 30}{Style.RESET_ALL}\")\n","    def log_model_save(self, path):\n","        print(f\"{Fore.BLUE}💾 Model saved at: {path}{Style.RESET_ALL}\")\n","    def log_time(self, epoch, total_epochs, current_epoch_duration=None):\n","        elapsed_total = time.time() - self.start_time\n","        avg_epoch_time_so_far = elapsed_total / (epoch + 1)\n","        remaining_epochs = total_epochs - (epoch + 1)\n","        estimated_remaining_time = avg_epoch_time_so_far * remaining_epochs\n","\n","        print(f\"\\n{Fore.CYAN}{'-' * 50}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}⏱️  Time Information (End of Epoch {epoch + 1}):{Style.RESET_ALL}\")\n","        if current_epoch_duration:\n","             print(f\"{Fore.CYAN}   Current Epoch Duration: {self.format_time(current_epoch_duration)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Total Elapsed Time:   {self.format_time(elapsed_total)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Avg. Time Per Epoch:  {self.format_time(avg_epoch_time_so_far)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Est. Remaining Time:  {self.format_time(estimated_remaining_time)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{'-' * 50}{Style.RESET_ALL}\")\n","    def log_epoch_start(self, epoch, total_epochs):\n","        print(f\"\\n{Fore.CYAN}{self.separator}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}🔄 Starting Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{self.separator}{Style.RESET_ALL}\\n\")\n","    def log_epoch_end(self, epoch, total_epochs, metrics):\n","        print(f\"\\n{Fore.GREEN}{self.separator}{Style.RESET_ALL}\")\n","        print(f\"{Fore.GREEN}🏁 Completed Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n","        self.log_metrics(metrics, f\"Epoch {epoch + 1} Summary\", no_lines=True)\n","        # self.log_time(epoch, total_epochs) # Moved time logging to be called explicitly with epoch duration\n","        print(f\"{Fore.GREEN}{self.separator}{Style.RESET_ALL}\\n\")\n","    @staticmethod\n","    def format_time(seconds):\n","        hours = int(seconds // 3600)\n","        minutes = int((seconds % 3600) // 60)\n","        seconds = int(seconds % 60)\n","        return f\"{hours:02d}h:{minutes:02d}m:{seconds:02d}s\"\n","\n","# =========================\n","# Helper Functions\n","# =========================\n","def safe_read_depth(path):\n","    if not os.path.exists(path):\n","        raise FileNotFoundError(f\"Depth image not found: {path}\")\n","    if path.endswith('.dpt'):\n","        try:\n","            with open(path, 'rb') as f:\n","                h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","                data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","                if data.size != h*w:\n","                    raise ValueError(f\"Depth data size mismatch in file: {path}\")\n","                return data.reshape(h, w).astype(np.float32)\n","        except Exception as e:\n","            raise IOError(f\"Failed to read .dpt file: {path}. Error: {e}\")\n","    depth_img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n","    if depth_img is None:\n","        raise IOError(f\"Failed to read depth image: {path}\")\n","    return depth_img.astype(np.float32)\n","\n","def load_split_file(split_file_path, logger_instance=None):\n","    if not os.path.exists(split_file_path):\n","        if logger_instance: logger_instance.log_warning(f\"Split file not found: {split_file_path}\")\n","        else: print(f\"Warning: Split file not found: {split_file_path}\")\n","        return []\n","    with open(split_file_path, 'r') as f:\n","        filenames = [line.strip() for line in f.readlines() if line.strip()]\n","    return filenames\n","\n","# =========================\n","# Unified Dataset Definition\n","# =========================\n","class UnifiedLinemodDataset(Dataset):\n","    def __init__(self, config, object_ids_to_load, split_type,\n","                 transform_rgb=None, transform_depth=None, transform_mask=None,\n","                 augmentation=None, logger=None):\n","        self.config = config\n","        self.object_ids_to_load = object_ids_to_load\n","        self.split_type = split_type\n","        self.transform_rgb = transform_rgb\n","        self.transform_depth = transform_depth\n","        self.transform_mask = transform_mask\n","        self.augmentation = augmentation\n","        self.logger = logger if logger else TrainingLogger()\n","\n","        self.samples = []\n","        self._load_samples()\n","\n","    def _check_radius_map_files_exist(self, obj_specific_radius_base_dir, base_name, obj_id):\n","        \"\"\"Checks if all 9 .npy radius map files exist for a given sample.\"\"\"\n","        for pt_idx in range(1, self.config['NUM_RADIUS_POINTS'] + 1):\n","            radius_folder_name = f\"{self.config['RADIUS_PREFIX']}{pt_idx}{self.config['RADIUS_SUFFIX']}\"\n","            radius_file_path = os.path.join(obj_specific_radius_base_dir, radius_folder_name, f'{base_name}.npy')\n","            if not os.path.exists(radius_file_path):\n","                self.logger.log_warning(f\"Radius map file missing for obj {obj_id}, sample {base_name}: {radius_file_path}\")\n","                return False\n","        return True\n","\n","    def _load_samples(self):\n","        base_data_dir = self.config['BASE_DIR']\n","        gdrive_radius_root = self.config['RADIUS_MAPS_GDRIVE_PATH']\n","\n","        if self.split_type == 'train': split_file_key = 'TRAIN_SPLIT'\n","        elif self.split_type == 'val': split_file_key = 'VAL_SPLIT'\n","        else:\n","            self.logger.log_error(f\"Invalid split_type for dataset: {self.split_type}\")\n","            raise ValueError(f\"Invalid split_type: {self.split_type}\")\n","\n","        self.logger.log_info(f\"Loading {self.split_type} samples for objects: {self.object_ids_to_load}...\")\n","        total_samples_found_for_split = 0\n","\n","        for obj_id in self.object_ids_to_load:\n","            obj_main_dir = os.path.join(base_data_dir, obj_id)\n","            obj_rgb_dir = os.path.join(obj_main_dir, self.config['RGB_DIR'])\n","            obj_depth_dir = os.path.join(obj_main_dir, self.config['DEPTH_DIR'])\n","            obj_mask_dir = os.path.join(obj_main_dir, self.config['MASK_DIR'])\n","            obj_pose_dir = os.path.join(obj_main_dir, self.config['POSE_DIR'])\n","            obj_specific_radius_base_dir = os.path.join(gdrive_radius_root, obj_id)\n","\n","            if not os.path.isdir(obj_specific_radius_base_dir):\n","                self.logger.log_warning(f\"GDrive Radius map base directory not found for object {obj_id}: {obj_specific_radius_base_dir}. Skipping this object for {self.split_type} split.\")\n","                continue\n","\n","            # Check if top-level radius point folders (Out_pt1_dm etc.) exist for this object\n","            # This is a general check for the object, not per-sample yet.\n","            all_base_radius_folders_exist = True\n","            for pt_idx in range(1, self.config['NUM_RADIUS_POINTS'] + 1):\n","                radius_folder_name = f\"{self.config['RADIUS_PREFIX']}{pt_idx}{self.config['RADIUS_SUFFIX']}\"\n","                if not os.path.isdir(os.path.join(obj_specific_radius_base_dir, radius_folder_name)):\n","                    self.logger.log_warning(f\"Base radius folder '{radius_folder_name}' missing for object {obj_id} at {obj_specific_radius_base_dir}.\")\n","                    all_base_radius_folders_exist = False\n","                    break\n","            if not all_base_radius_folders_exist:\n","                self.logger.log_warning(f\"Skipping object {obj_id} for {self.split_type} split due to missing base radius map folders.\")\n","                continue\n","\n","\n","            split_file_path = os.path.join(obj_main_dir, self.config['SPLIT_DIR'], self.config[split_file_key])\n","            base_filenames_for_obj = load_split_file(split_file_path, self.logger)\n","\n","            if not base_filenames_for_obj:\n","                self.logger.log_info(f\"No samples listed in {split_file_path} for object {obj_id}.\")\n","                continue\n","\n","            obj_samples_added = 0\n","            for base_name in base_filenames_for_obj:\n","                rgb_check_path = os.path.join(obj_rgb_dir, f'{base_name}.png')\n","                depth_check_path = os.path.join(obj_depth_dir, f'{base_name}.dpt')\n","                mask_check_path = os.path.join(obj_mask_dir, f'{base_name}.png')\n","                pose_check_path = os.path.join(obj_pose_dir, f'pose{base_name}.npy')\n","\n","                if not all(os.path.exists(p) for p in [rgb_check_path, depth_check_path, mask_check_path, pose_check_path]):\n","                    self.logger.log_warning(f\"Core file(s) missing for obj {obj_id}, sample {base_name}. Skipping.\")\n","                    continue\n","\n","                if not self._check_radius_map_files_exist(obj_specific_radius_base_dir, base_name, obj_id):\n","                    # Warning already logged by _check_radius_map_files_exist\n","                    continue\n","\n","                self.samples.append({\n","                    'obj_id': obj_id, 'base_name': base_name,\n","                    'rgb_dir': obj_rgb_dir, 'depth_dir': obj_depth_dir,\n","                    'mask_dir': obj_mask_dir, 'pose_dir': obj_pose_dir,\n","                    'radius_base_dir_obj': obj_specific_radius_base_dir\n","                })\n","                obj_samples_added += 1\n","            if obj_samples_added > 0:\n","                self.logger.log_info(f\"Added {obj_samples_added} samples for object {obj_id} from {split_file_path}.\")\n","            total_samples_found_for_split += obj_samples_added\n","\n","        if not self.samples:\n","            self.logger.log_error(f\"CRITICAL: No valid samples loaded for {self.split_type} split across all objects. DataLoader will be empty.\")\n","        else:\n","            self.logger.log_success(f\"Successfully loaded a total of {len(self.samples)} samples for {self.split_type} split from {len(self.object_ids_to_load)} objects.\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample_info = self.samples[idx]\n","        obj_id = sample_info['obj_id']\n","        base_name = sample_info['base_name']\n","\n","        rgb_path = os.path.join(sample_info['rgb_dir'], f'{base_name}.png')\n","        rgb_img = cv2.imread(rgb_path)\n","        if rgb_img is None: raise FileNotFoundError(f\"Failed to load RGB: {rgb_path}\")\n","        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","        rgb_img = Image.fromarray(rgb_img)\n","\n","        depth_path = os.path.join(sample_info['depth_dir'], f'{base_name}.dpt')\n","        depth_img_np = safe_read_depth(depth_path).astype(np.float32) / 1000.0 # to meters\n","        # depth_img_np = np.expand_dims(depth_img_np, axis=2) # ToTensor will add channel for 2D image\n","        depth_img = Image.fromarray(depth_img_np, mode='F') # PIL 'F' mode for float32\n","\n","        mask_path = os.path.join(sample_info['mask_dir'], f'{base_name}.png')\n","        mask_img_np = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","        if mask_img_np is None: raise FileNotFoundError(f\"Failed to load Mask: {mask_path}\")\n","        # mask_img_np = np.expand_dims(mask_img_np.astype(np.float32) / 255.0, axis=2) # Normalization handled by ToTensor\n","        # mask_img_np = (mask_img_np * 255).astype(np.uint8) # ToTensor expects 0-255 for L mode\n","        mask_img = Image.fromarray(mask_img_np, mode='L')\n","\n","\n","        pose_path = os.path.join(sample_info['pose_dir'], f'pose{base_name}.npy')\n","        pose = np.load(pose_path)\n","        if pose.shape == (3, 4):\n","            rot_matrix = pose[:, :3]\n","            trans_vector = pose[:, 3]\n","        elif pose.shape == (4, 4):\n","            rot_matrix = pose[:3, :3]\n","            trans_vector = pose[:3, 3]\n","        else: # Assuming already (7,) [tx,ty,tz, qx,qy,qz,qw]\n","            pass\n","\n","        if 'rot_matrix' in locals(): # Convert if we parsed a matrix\n","            try:\n","                rot_quat = R.from_matrix(rot_matrix).as_quat() # x, y, z, w\n","                pose = np.concatenate([trans_vector, rot_quat])\n","            except ValueError as e:\n","                self.logger.log_error(f\"Error converting rotation matrix to quaternion for {obj_id}/{base_name}: {e}. Matrix:\\n{rot_matrix}\")\n","                # Handle error, e.g. by returning a dummy pose or raising exception\n","                # For now, re-raise to stop training if data is corrupt\n","                raise\n","\n","\n","        radius_maps_list = []\n","        obj_specific_radius_dir = sample_info['radius_base_dir_obj']\n","        for pt_idx in range(1, self.config['NUM_RADIUS_POINTS'] + 1):\n","            radius_folder = f\"{self.config['RADIUS_PREFIX']}{pt_idx}{self.config['RADIUS_SUFFIX']}\"\n","            radius_path = os.path.join(obj_specific_radius_dir, radius_folder, f'{base_name}.npy')\n","            if not os.path.exists(radius_path):\n","                 self.logger.log_error(f\"FATAL in __getitem__: Radius map file not found: {radius_path}\")\n","                 raise FileNotFoundError(f\"Radius map not found during __getitem__: {radius_path}\")\n","            radius_map = np.load(radius_path)\n","            radius_maps_list.append(radius_map)\n","        radius_maps_np = np.array(radius_maps_list, dtype=np.float32)\n","\n","        if self.augmentation:\n","            rgb_img, depth_img, mask_img = self.augmentation(rgb_img, depth_img, mask_img)\n","\n","        # Apply transforms\n","        final_rgb = self.transform_rgb(rgb_img) if self.transform_rgb else torch.from_numpy(np.array(rgb_img)).float()\n","        final_depth = self.transform_depth(depth_img) if self.transform_depth else torch.from_numpy(np.array(depth_img)).float().unsqueeze(0)\n","        final_mask = self.transform_mask(mask_img) if self.transform_mask else torch.from_numpy(np.array(mask_img)).float().unsqueeze(0)\n","\n","        return {\n","            'rgb': final_rgb,\n","            'depth': final_depth,\n","            'mask': final_mask,\n","            'pose': torch.FloatTensor(pose),\n","            'radius_maps': torch.FloatTensor(radius_maps_np)\n","        }\n","\n","# =========================\n","# Model Components & Definition (Assumed largely unchanged, ensure ResNet50 weights are handled)\n","# =========================\n","class AdvancedAugmentation:\n","    def __init__(self):\n","        self.color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n","    def __call__(self, rgb, depth, mask):\n","        rgb = self.color_jitter(rgb)\n","        angle = np.random.uniform(-10, 10)\n","        rgb = transforms.functional.rotate(rgb, angle, expand=False, fill=0) # fill with 0 for black\n","        depth = transforms.functional.rotate(depth, angle, expand=False, fill=0.0) # fill with 0.0 for depth\n","        mask = transforms.functional.rotate(mask, angle, expand=False, fill=0) # fill with 0 for mask\n","\n","        depth_np = np.array(depth).astype(np.float32) # mode 'F'\n","        noise = np.random.normal(scale=0.01 * depth_np.max(), size=depth_np.shape).astype(np.float32) # Noise relative to max depth\n","        depth_np = np.clip(depth_np + noise, 0, None) # Ensure depth remains non-negative\n","        depth = Image.fromarray(depth_np, mode='F')\n","        return rgb, depth, mask\n","\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    def __init__(self, in_channels_list, out_channels=256): # Added out_channels\n","        super(FeaturePyramidNetwork, self).__init__()\n","        self.out_channels = out_channels\n","        self.lateral_convs = nn.ModuleList()\n","        self.fpn_convs = nn.ModuleList()\n","        for in_ch in in_channels_list:\n","            self.lateral_convs.append(nn.Conv2d(in_ch, self.out_channels, 1))\n","            self.fpn_convs.append(nn.Conv2d(self.out_channels, self.out_channels, 3, padding=1))\n","    def forward(self, features):\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        for i in range(len(laterals) - 1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:], mode='nearest')\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    def __init__(self, fpn_out_channels=256): # Pass fpn_out_channels\n","        super(EnhancedRCVPose, self).__init__()\n","        self.fpn_out_channels = fpn_out_channels\n","        resnet_rgb = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","        self.rgb_conv1 = resnet_rgb.conv1\n","        self.rgb_bn1 = resnet_rgb.bn1\n","        self.rgb_relu = resnet_rgb.relu\n","        self.rgb_maxpool = resnet_rgb.maxpool\n","        self.rgb_layer1 = resnet_rgb.layer1 # C2, channels: 256\n","        self.rgb_layer2 = resnet_rgb.layer2 # C3, channels: 512\n","        self.rgb_layer3 = resnet_rgb.layer3 # C4, channels: 1024\n","        self.rgb_layer4 = resnet_rgb.layer4 # C5, channels: 2048\n","\n","        resnet_depth = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) # Re-init for depth\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        with torch.no_grad():\n","            resnet_depth.conv1.weight.copy_(resnet_rgb.conv1.weight.mean(dim=1, keepdim=True))\n","        self.depth_conv1 = resnet_depth.conv1\n","        self.depth_bn1 = resnet_depth.bn1\n","        self.depth_relu = resnet_depth.relu\n","        self.depth_maxpool = resnet_depth.maxpool\n","        self.depth_layer1 = resnet_depth.layer1\n","        self.depth_layer2 = resnet_depth.layer2\n","        self.depth_layer3 = resnet_depth.layer3\n","        self.depth_layer4 = resnet_depth.layer4\n","\n","        # For ResNet50, layer1 is C2 (256 ch), layer2 is C3 (512 ch), layer3 is C4 (1024 ch), layer4 is C5 (2048 ch)\n","        # FPN typically takes C3, C4, C5 as input\n","        fpn_input_channels = [512, 1024, 2048]\n","        self.rgb_fpn = FeaturePyramidNetwork(fpn_input_channels, out_channels=self.fpn_out_channels)\n","        self.depth_fpn = FeaturePyramidNetwork(fpn_input_channels, out_channels=self.fpn_out_channels)\n","\n","        self.rgb_attention = AttentionModule(self.fpn_out_channels)\n","        self.depth_attention = AttentionModule(self.fpn_out_channels)\n","\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(self.fpn_out_channels * 2, self.fpn_out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n","            nn.Conv2d(self.fpn_out_channels, self.fpn_out_channels, kernel_size=3, padding=1), nn.ReLU(inplace=True)\n","        )\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(self.fpn_out_channels, 128), nn.ReLU(inplace=True),\n","            nn.Dropout(0.5), nn.Linear(128, 7)\n","        )\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(self.fpn_out_channels, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n","            nn.Conv2d(128, CONFIG['NUM_RADIUS_POINTS'], kernel_size=1)\n","        )\n","    def forward(self, rgb, depth):\n","        # RGB Stream\n","        r = self.rgb_conv1(rgb)\n","        r = self.rgb_bn1(r)\n","        r = self.rgb_relu(r)\n","        r_pool = self.rgb_maxpool(r) # C1 output\n","        r_c2 = self.rgb_layer1(r_pool)\n","        r_c3 = self.rgb_layer2(r_c2)\n","        r_c4 = self.rgb_layer3(r_c3)\n","        r_c5 = self.rgb_layer4(r_c4)\n","        rgb_fpn_features = self.rgb_fpn([r_c3, r_c4, r_c5]) # P3, P4, P5 from FPN\n","\n","        # Depth Stream\n","        d = self.depth_conv1(depth)\n","        d = self.depth_bn1(d)\n","        d = self.depth_relu(d)\n","        d_pool = self.depth_maxpool(d)\n","        d_c2 = self.depth_layer1(d_pool)\n","        d_c3 = self.depth_layer2(d_c2)\n","        d_c4 = self.depth_layer3(d_c3)\n","        d_c5 = self.depth_layer4(d_c4)\n","        depth_fpn_features = self.depth_fpn([d_c3, d_c4, d_c5])\n","\n","        # Use P3 (highest resolution from FPN list [P3, P4, P5]) for attention and fusion\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])     # P3 from RGB FPN\n","        depth_attended = self.depth_attention(depth_fpn_features[0]) # P3 from Depth FPN\n","\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)\n","\n","        pooled = self.global_pool(fused)\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","\n","        outside9 = self.outside9_head(fused)\n","        target_size = (rgb.shape[2], rgb.shape[3])\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","        return pose, outside9\n","\n","class GeometricLoss(nn.Module):\n","    def __init__(self):\n","        super(GeometricLoss, self).__init__()\n","    def forward(self, pred_pose, target_pose, pred_outside9, target_outside9):\n","        trans_loss = F.mse_loss(pred_pose[:, :3], target_pose[:, :3])\n","        pred_rot = F.normalize(pred_pose[:, 3:], dim=1, p=2)\n","        target_rot = F.normalize(target_pose[:, 3:], dim=1, p=2)\n","        dot = torch.sum(pred_rot * target_rot, dim=1).clamp(-1.0 + 1e-7, 1.0 - 1e-7)\n","        rot_error = 2.0 * torch.acos(torch.abs(dot))\n","        rot_loss = rot_error.mean()\n","        points_loss = F.mse_loss(pred_outside9, target_outside9)\n","        total_loss = trans_loss + 0.5 * rot_loss + 0.3 * points_loss # Weights can be tuned\n","        return total_loss, trans_loss, rot_loss, points_loss\n","\n","# =========================\n","# Training Function\n","# =========================\n","def train_model(model, train_loader, val_loader, num_epochs, learning_rate, logger_instance=None, config_dict=None):\n","    logger = logger_instance if logger_instance else TrainingLogger()\n","    cfg = config_dict if config_dict else CONFIG # Use passed config or global\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    logger.log_info(f\"Initializing training on device: {device}\")\n","    model = model.to(device)\n","\n","    os.makedirs(cfg['GDRIVE_UNIFIED_MODELS_DIR'], exist_ok=True)\n","    logger.log_info(f\"Models will be saved to: {cfg['GDRIVE_UNIFIED_MODELS_DIR']}\")\n","\n","    criterion = GeometricLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n","\n","    accumulation_steps = cfg.get('ACCUMULATION_STEPS', 1)\n","    steps_per_epoch_for_scheduler = len(train_loader) // accumulation_steps\n","    if len(train_loader) % accumulation_steps != 0:\n","        steps_per_epoch_for_scheduler += 1\n","\n","    scheduler = optim.lr_scheduler.OneCycleLR(\n","        optimizer, max_lr=learning_rate, epochs=num_epochs,\n","        steps_per_epoch=steps_per_epoch_for_scheduler\n","    )\n","    scaler = GradScaler(enabled=torch.cuda.is_available()) # Enable only if cuda is available\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        epoch_start_time = time.time()\n","        logger.log_epoch_start(epoch, num_epochs)\n","        torch.cuda.empty_cache()\n","\n","        model.train()\n","        train_running_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n","        optimizer.zero_grad()\n","\n","        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} Training', unit='batch')\n","        for i, batch in enumerate(progress_bar):\n","            rgb = batch['rgb'].to(device, non_blocking=True)\n","            depth = batch['depth'].to(device, non_blocking=True)\n","            pose_target = batch['pose'].to(device, non_blocking=True)\n","            outside9_target = batch['radius_maps'].to(device, non_blocking=True)\n","\n","            with autocast(enabled=torch.cuda.is_available()):\n","                pose_pred, outside9_pred = model(rgb, depth)\n","                total_loss, trans_loss, rot_loss, points_loss = criterion(\n","                    pose_pred, pose_target, outside9_pred, outside9_target\n","                )\n","\n","            scaler.scale(total_loss / accumulation_steps).backward()\n","\n","            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                if scheduler: scheduler.step()\n","\n","            train_running_losses['total'] += total_loss.item()\n","            train_running_losses['trans'] += trans_loss.item()\n","            train_running_losses['rot'] += rot_loss.item()\n","            train_running_losses['points'] += points_loss.item()\n","            progress_bar.set_postfix(loss=total_loss.item() / accumulation_steps)\n","\n","        avg_train_losses = {k: v / len(train_loader) for k, v in train_running_losses.items()}\n","        logger.log_metrics(avg_train_losses, \"Training Avg.\", no_lines=True)\n","\n","        model.eval()\n","        val_running_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n","        progress_bar_val = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} Validation', unit='batch')\n","        with torch.no_grad():\n","            for batch in progress_bar_val:\n","                rgb = batch['rgb'].to(device, non_blocking=True)\n","                depth = batch['depth'].to(device, non_blocking=True)\n","                pose_target = batch['pose'].to(device, non_blocking=True)\n","                outside9_target = batch['radius_maps'].to(device, non_blocking=True)\n","                with autocast(enabled=torch.cuda.is_available()):\n","                    pose_pred, outside9_pred = model(rgb, depth)\n","                    total_loss, trans_loss, rot_loss, points_loss = criterion(\n","                        pose_pred, pose_target, outside9_pred, outside9_target\n","                    )\n","                val_running_losses['total'] += total_loss.item()\n","                val_running_losses['trans'] += trans_loss.item()\n","                val_running_losses['rot'] += rot_loss.item()\n","                val_running_losses['points'] += points_loss.item()\n","                progress_bar_val.set_postfix(loss=total_loss.item())\n","\n","        avg_val_losses = {k: v / len(val_loader) for k, v in val_running_losses.items()}\n","        logger.log_metrics(avg_val_losses, \"Validation Avg.\", no_lines=True)\n","\n","        epoch_duration = time.time() - epoch_start_time\n","        logger.log_epoch_end(epoch, num_epochs, {'train_loss': avg_train_losses['total'], 'val_loss': avg_val_losses['total']})\n","        logger.log_time(epoch, num_epochs, current_epoch_duration=epoch_duration)\n","\n","\n","        if avg_val_losses['total'] < best_val_loss:\n","            best_val_loss = avg_val_losses['total']\n","            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","            drive_save_path = os.path.join(cfg['GDRIVE_UNIFIED_MODELS_DIR'], f'unified_best_model_epoch{epoch+1}_{timestamp}.pth')\n","            checkpoint = {\n","                'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","                'scaler_state_dict': scaler.state_dict(),\n","                'val_loss': best_val_loss, 'val_metrics': avg_val_losses, 'config': cfg\n","            }\n","            torch.save(checkpoint, drive_save_path)\n","            logger.log_success(f\"Unified best model saved to Google Drive (Epoch {epoch+1}, Val Loss: {best_val_loss:.4f})\")\n","            logger.log_model_save(drive_save_path)\n","\n","    logger.log_section(\"Unified Model Training Completed\")\n","    logger.log_success(f\"Best validation loss achieved for unified model: {best_val_loss:.4f}\")\n","\n","# =========================\n","# Main Execution\n","# =========================\n","def main():\n","    logger = TrainingLogger()\n","    logger.log_section(\"Starting RCVPose Training for a Unified Multi-Object Model\")\n","\n","    try:\n","        logger.log_info(\"Attempting to mount Google Drive...\")\n","        drive.mount('/content/drive', force_remount=False) # Set force_remount=True if needed\n","        logger.log_success(\"Google Drive mounted.\")\n","    except Exception as e:\n","        logger.log_error(f\"Failed to mount Google Drive: {e}. Ensure you are in a Colab environment or GDrive is accessible.\")\n","        return\n","\n","    object_ids_for_training = CONFIG['OBJECT_IDS']\n","    logger.log_info(f\"Unified model will be trained on objects: {object_ids_for_training}\")\n","\n","    logger.log_subsection(\"Setting up data transforms and augmentation\")\n","    transform_rgb = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    # For depth, ToTensor handles H,W -> C,H,W if input is PIL Image.\n","    # Input to ToTensor should be PIL Image or HWC numpy. Our depth is HW numpy.\n","    transform_depth = transforms.Compose([\n","        transforms.ToTensor() # Converts HWC or HW (if mode L or F) PIL to CHW tensor\n","    ])\n","    transform_mask = transforms.Compose([transforms.ToTensor()])\n","    augmentation = AdvancedAugmentation()\n","\n","    logger.log_subsection(f\"Creating unified training dataset\")\n","    train_dataset = UnifiedLinemodDataset(\n","        config=CONFIG, object_ids_to_load=object_ids_for_training, split_type='train',\n","        transform_rgb=transform_rgb, transform_depth=transform_depth,\n","        transform_mask=transform_mask, augmentation=augmentation, logger=logger\n","    )\n","\n","    logger.log_subsection(f\"Creating unified validation dataset\")\n","    val_dataset = UnifiedLinemodDataset(\n","        config=CONFIG, object_ids_to_load=object_ids_for_training, split_type='val',\n","        transform_rgb=transform_rgb, transform_depth=transform_depth,\n","        transform_mask=transform_mask, augmentation=None, logger=logger # No augmentation for validation\n","    )\n","\n","    if len(train_dataset) == 0:\n","        logger.log_error(\"Training dataset is empty. Cannot proceed. Check data paths, GDrive mount, and split files.\")\n","        return\n","    if len(val_dataset) == 0:\n","        logger.log_warning(\"Validation dataset is empty. Training will proceed without validation if you choose to continue, but this is not recommended.\")\n","        # return # Or allow to proceed with warning\n","\n","    logger.log_subsection(\"Creating data loaders for unified dataset\")\n","    train_loader = DataLoader(\n","        train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True,\n","        num_workers=CONFIG['NUM_WORKERS'], pin_memory=True, drop_last=True, # drop_last for stable batch sizes\n","        persistent_workers=(CONFIG['NUM_WORKERS'] > 0),\n","        prefetch_factor=2 if CONFIG['NUM_WORKERS'] > 0 else None\n","    )\n","    val_loader = DataLoader(\n","        val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False,\n","        num_workers=CONFIG['NUM_WORKERS'], pin_memory=True, drop_last=False,\n","        persistent_workers=(CONFIG['NUM_WORKERS'] > 0),\n","        prefetch_factor=2 if CONFIG['NUM_WORKERS'] > 0 else None\n","    )\n","    logger.log_info(f\"Train DataLoader: {len(train_loader)} batches. Val DataLoader: {len(val_loader)} batches.\")\n","\n","    logger.log_subsection(\"Initializing single EnhancedRCVPose model\")\n","    model = EnhancedRCVPose()\n","\n","    logger.log_info(\"Freezing BatchNorm layers in the model...\")\n","    for module_name, module in model.named_modules():\n","        if isinstance(module, nn.BatchNorm2d):\n","            module.eval() # Put BatchNorm in eval mode\n","            for param in module.parameters():\n","                param.requires_grad = False # Freeze parameters\n","            # logger.log_info(f\"Froze BatchNorm: {module_name}\")\n","\n","\n","    logger.log_section(\"Starting Unified Training Process\")\n","    train_model(\n","        model, train_loader, val_loader,\n","        num_epochs=CONFIG['NUM_EPOCHS'],\n","        learning_rate=CONFIG['LEARNING_RATE'],\n","        logger_instance=logger,\n","        config_dict=CONFIG\n","    )\n","\n","    logger.log_section(\"All Training Operations Completed for Unified Model\")\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BPjhiytw3HkQ","executionInfo":{"status":"error","timestamp":1748825751171,"user_tz":-120,"elapsed":858041,"user":{"displayName":"ml6d","userId":"04537147647663067774"}},"outputId":"2598d8a4-a4b4-4f78-c3e8-386683c75f80"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","====================================================================================================\n","🚀 Starting RCVPose Training for a Unified Multi-Object Model\n","====================================================================================================\n","\n","ℹ️  Attempting to mount Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Google Drive mounted.\n","ℹ️  Unified model will be trained on objects: ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n","\n","----------------------------------------------------------------------------------------------------\n","📌 Setting up data transforms and augmentation\n","----------------------------------------------------------------------------------------------------\n","\n","\n","----------------------------------------------------------------------------------------------------\n","📌 Creating unified training dataset\n","----------------------------------------------------------------------------------------------------\n","\n","ℹ️  Loading train samples for objects: ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']...\n","ℹ️  Added 865 samples for object 01 from /content/dataset/Linemod_preprocessed/data/01/Split/train.txt.\n","ℹ️  Added 849 samples for object 02 from /content/dataset/Linemod_preprocessed/data/02/Split/train.txt.\n","ℹ️  Added 840 samples for object 04 from /content/dataset/Linemod_preprocessed/data/04/Split/train.txt.\n","ℹ️  Added 837 samples for object 05 from /content/dataset/Linemod_preprocessed/data/05/Split/train.txt.\n","ℹ️  Added 825 samples for object 06 from /content/dataset/Linemod_preprocessed/data/06/Split/train.txt.\n","ℹ️  Added 831 samples for object 08 from /content/dataset/Linemod_preprocessed/data/08/Split/train.txt.\n","ℹ️  Added 877 samples for object 09 from /content/dataset/Linemod_preprocessed/data/09/Split/train.txt.\n","ℹ️  Added 877 samples for object 10 from /content/dataset/Linemod_preprocessed/data/10/Split/train.txt.\n","ℹ️  Added 854 samples for object 11 from /content/dataset/Linemod_preprocessed/data/11/Split/train.txt.\n","ℹ️  Added 865 samples for object 12 from /content/dataset/Linemod_preprocessed/data/12/Split/train.txt.\n","ℹ️  Added 806 samples for object 13 from /content/dataset/Linemod_preprocessed/data/13/Split/train.txt.\n","ℹ️  Added 858 samples for object 14 from /content/dataset/Linemod_preprocessed/data/14/Split/train.txt.\n","⚠️  Core file(s) missing for obj 15, sample 001226. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001228. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001236. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001234. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001238. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001230. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001242. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001227. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001241. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001232. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001235. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001229. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001225. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001231. Skipping.\n","ℹ️  Added 856 samples for object 15 from /content/dataset/Linemod_preprocessed/data/15/Split/train.txt.\n","✅ Successfully loaded a total of 11040 samples for train split from 13 objects.\n","\n","----------------------------------------------------------------------------------------------------\n","📌 Creating unified validation dataset\n","----------------------------------------------------------------------------------------------------\n","\n","ℹ️  Loading val samples for objects: ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']...\n","ℹ️  Added 247 samples for object 01 from /content/dataset/Linemod_preprocessed/data/01/Split/val.txt.\n","ℹ️  Added 243 samples for object 02 from /content/dataset/Linemod_preprocessed/data/02/Split/val.txt.\n","ℹ️  Added 240 samples for object 04 from /content/dataset/Linemod_preprocessed/data/04/Split/val.txt.\n","ℹ️  Added 239 samples for object 05 from /content/dataset/Linemod_preprocessed/data/05/Split/val.txt.\n","ℹ️  Added 236 samples for object 06 from /content/dataset/Linemod_preprocessed/data/06/Split/val.txt.\n","ℹ️  Added 238 samples for object 08 from /content/dataset/Linemod_preprocessed/data/08/Split/val.txt.\n","ℹ️  Added 251 samples for object 09 from /content/dataset/Linemod_preprocessed/data/09/Split/val.txt.\n","ℹ️  Added 250 samples for object 10 from /content/dataset/Linemod_preprocessed/data/10/Split/val.txt.\n","ℹ️  Added 244 samples for object 11 from /content/dataset/Linemod_preprocessed/data/11/Split/val.txt.\n","ℹ️  Added 248 samples for object 12 from /content/dataset/Linemod_preprocessed/data/12/Split/val.txt.\n","ℹ️  Added 230 samples for object 13 from /content/dataset/Linemod_preprocessed/data/13/Split/val.txt.\n","ℹ️  Added 246 samples for object 14 from /content/dataset/Linemod_preprocessed/data/14/Split/val.txt.\n","⚠️  Core file(s) missing for obj 15, sample 001237. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001239. Skipping.\n","⚠️  Core file(s) missing for obj 15, sample 001233. Skipping.\n","ℹ️  Added 245 samples for object 15 from /content/dataset/Linemod_preprocessed/data/15/Split/val.txt.\n","✅ Successfully loaded a total of 3157 samples for val split from 13 objects.\n","\n","----------------------------------------------------------------------------------------------------\n","📌 Creating data loaders for unified dataset\n","----------------------------------------------------------------------------------------------------\n","\n","ℹ️  Train DataLoader: 1380 batches. Val DataLoader: 395 batches.\n","\n","----------------------------------------------------------------------------------------------------\n","📌 Initializing single EnhancedRCVPose model\n","----------------------------------------------------------------------------------------------------\n","\n","ℹ️  Freezing BatchNorm layers in the model...\n","\n","====================================================================================================\n","🚀 Starting Unified Training Process\n","====================================================================================================\n","\n","ℹ️  Initializing training on device: cuda\n","ℹ️  Models will be saved to: /content/drive/MyDrive/models_unified\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-6903212cb5d7>:525: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler(enabled=torch.cuda.is_available()) # Enable only if cuda is available\n"]},{"output_type":"stream","name":"stdout","text":["\n","====================================================================================================\n","🔄 Starting Epoch 1/100\n","====================================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/100 Training:   0%|          | 0/1380 [00:00<?, ?batch/s]<ipython-input-4-6903212cb5d7>:544: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(enabled=torch.cuda.is_available()):\n","Epoch 1/100 Training:   0%|          | 3/1380 [00:26<3:23:23,  8.86s/batch, loss=0.26]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n","Epoch 1/100 Training:  11%|█         | 152/1380 [13:55<1:52:31,  5.50s/batch, loss=0.252]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-6903212cb5d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-6903212cb5d7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Unified Training Process\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     train_model(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_EPOCHS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-6903212cb5d7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, logger_instance, config_dict)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{num_epochs} Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rgb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# **Validation**"],"metadata":{"id":"BbAqYcV9HLro"}},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","import cv2\n","from tqdm import tqdm\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.cuda.amp import autocast\n","\n","# =========================\n","# CONFIGURATION SECTION\n","# =========================\n","CONFIG = {\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',  # Root directory containing all object folders\n","    'OBJECT_ID': '01',  # Object ID to validate (e.g., '01')\n","    'BATCH_SIZE': 1,  # For validation, use batch size 1 for accurate metrics\n","    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n","    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'MODELS_DIR': '/content/models',  # Directory containing saved models\n","    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n","    'ADD_THRESHOLD_MM': 10.0,  # Success if ADD < 10 mm\n","    'NUM_WORKERS': 2,\n","}\n","\n","RGB_TRANSFORM = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# For depth we keep lambda to tensor float32 (metres)\n","DEPTH_TRANSFORM = lambda img: torch.from_numpy(np.array(img, dtype=np.float32)).unsqueeze(0)\n","\n","# =========================\n","# DEPTH READER FOR .dpt FILES\n","# =========================\n","def read_depth_dpt(path):\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","    return data.reshape(h, w).astype(np.float32)\n","\n","# =========================\n","# MODEL ARCHITECTURE\n","# =========================\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","\n","    def forward(self, features):\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        # Use ResNet50 as backbone\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])   # Output: 256\n","        self.rgb_layer2 = list(resnet.children())[5]                    # Output: 512\n","        self.rgb_layer3 = list(resnet.children())[6]                    # Output: 1024\n","        self.rgb_layer4 = list(resnet.children())[7]                    # Output: 2048\n","\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","\n","        # Fusion block\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Pose head\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","\n","        # Outside9 head\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)\n","        )\n","\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)\n","\n","        # Pose prediction\n","        pooled = self.global_pool(fused)\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","\n","        # Outside9 prediction\n","        outside9 = self.outside9_head(fused)\n","        target_size = (rgb.shape[2], rgb.shape[3])\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","\n","        return pose, outside9\n","\n","# =========================\n","# DATASET CLASS\n","# =========================\n","class ValidationDataset(Dataset):\n","    def __init__(self, base_dir, object_id, num_radius_points=9):\n","        self.base_dir = base_dir\n","        self.object_id = object_id\n","        self.num_radius_points = num_radius_points\n","\n","        # Set up paths\n","        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n","        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n","        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n","        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n","        self.radius_base_dir = os.path.join(base_dir, object_id)\n","\n","        # Load validation split\n","        split_file = os.path.join(base_dir, object_id, 'Split', 'val.txt')\n","        with open(split_file, 'r') as f:\n","            self.filenames = [line.strip() for line in f.readlines()]\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            filename = self.filenames[idx]\n","            base_name = filename.split('.')[0]\n","            # Load RGB image\n","            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","            rgb_img = cv2.imread(rgb_path)\n","            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","            rgb_img = RGB_TRANSFORM(Image.fromarray(rgb_img))\n","            # Load depth image (.dpt files need special reader)\n","            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","            if not os.path.exists(depth_path):\n","                raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n","            depth_np = read_depth_dpt(depth_path)\n","            if depth_np is None:\n","                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n","            depth_np = depth_np / 1000.0  # to metres (consistent with training)\n","            depth_img = DEPTH_TRANSFORM(Image.fromarray(depth_np, mode='F'))\n","            # Load pose\n","            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","            pose = np.load(pose_path)\n","            if pose.shape == (3, 4):\n","                rot = R.from_matrix(pose[:, :3]).as_quat()\n","                trans = pose[:, 3]\n","                pose = np.concatenate([trans, rot])\n","            elif pose.shape == (4, 4):\n","                rot = R.from_matrix(pose[:3, :3]).as_quat()\n","                trans = pose[:3, 3]\n","                pose = np.concatenate([trans, rot])\n","            # Load radius maps\n","            radius_maps = []\n","            for pt_idx in range(1, self.num_radius_points + 1):\n","                radius_folder = f\"Out_pt{pt_idx}_dm\"\n","                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","                radius_map = np.load(radius_path)\n","                radius_maps.append(radius_map)\n","            radius_maps = np.array(radius_maps)\n","            return {\n","                'rgb': rgb_img,\n","                'depth': depth_img,\n","                'pose': torch.FloatTensor(pose),\n","                'radius_maps': torch.FloatTensor(radius_maps)\n","            }\n","        except Exception as e:\n","            print(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n","            return None\n","\n","def safe_collate(batch):\n","    batch = [b for b in batch if b is not None]\n","    if len(batch) == 0:\n","        return None\n","    return torch.utils.data.dataloader.default_collate(batch)\n","\n","# =========================\n","# METRIC FUNCTIONS\n","# =========================\n","def translation_rmse(pred, gt):\n","    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n","\n","def rotation_error(pred, gt, deg=True):\n","    pred_q = pred[3:] / np.linalg.norm(pred[3:])\n","    gt_q = gt[3:] / np.linalg.norm(gt[3:])\n","    dot = np.clip(np.abs(np.dot(pred_q, gt_q)), -1.0 + 1e-7, 1.0 - 1e-7)\n","    angle = 2.0 * np.arccos(dot)  # radians\n","    return np.degrees(angle) if deg else angle\n","\n","def add_metric(pred_pose, gt_pose, mesh_points):\n","    if mesh_points is None:\n","        return 0.0\n","\n","    # Convert quaternions to rotation matrices\n","    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n","    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n","\n","    # Transform mesh points\n","    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n","    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n","\n","    # Compute mean distance\n","    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n","    return mean_dist  # mm\n","\n","def get_latest_model(models_dir):\n","    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n","    if not model_files:\n","        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n","    return max(model_files, key=os.path.getctime)\n","\n","# =========================\n","# VALIDATION PIPELINE\n","# =========================\n","def validate():\n","    print(\"\\n================= RCVPose Validation (Colab) =================\")\n","\n","    # Find and load latest model\n","    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n","    print(f\"Using latest model: {model_path}\")\n","\n","    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n","    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    # Load validation data\n","    dataset = ValidationDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n","    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=CONFIG['NUM_WORKERS'], collate_fn=safe_collate, pin_memory=True)\n","\n","    # Load mesh for ADD metric\n","    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n","    mesh_points = None\n","    try:\n","        if os.path.exists(mesh_path):\n","            import open3d as o3d\n","            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n","    except ImportError:\n","        print(\"open3d is not installed. Please run: !pip install open3d\")\n","\n","    # Metrics accumulators\n","    trans_errors = []\n","    rot_errors = []\n","    points_errors = []\n","    add_errors = []\n","    add_success = 0\n","\n","    # Validation loop (skip samples with missing/corrupt files)\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Validating\"):\n","            if batch is None:\n","                continue  # all samples in this worker were None\n","            rgb = batch['rgb'].to(CONFIG['DEVICE'])\n","            depth = batch['depth'].to(CONFIG['DEVICE'])\n","            pose_gt = batch['pose'].to(CONFIG['DEVICE'])\n","            radius_maps_gt = batch['radius_maps'].to(CONFIG['DEVICE'])\n","            # Forward pass with AMP for speed\n","            with autocast():\n","                pose_pred, radius_maps_pred = model(rgb, depth)\n","            # Move predictions to CPU for metric computation\n","            pose_pred = pose_pred.cpu().numpy()\n","            pose_gt = pose_gt.cpu().numpy()\n","            radius_maps_pred = radius_maps_pred.cpu().numpy()\n","            radius_maps_gt = radius_maps_gt.cpu().numpy()\n","            # Compute metrics\n","            for j in range(len(pose_pred)):\n","                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n","                trans_errors.append(trans_error)\n","                rot_errors.append(rotation_error(pose_pred[j], pose_gt[j]))\n","                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n","                points_errors.append(points_error)\n","                if mesh_points is not None:\n","                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n","                    add_errors.append(add_error)\n","                    if add_error < CONFIG['ADD_THRESHOLD_MM']:\n","                        add_success += 1\n","\n","    # Compute and print final metrics\n","    print(\"\\nValidation Results:\")\n","    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n","    print(f\"Rotation Error (deg): {np.mean(rot_errors):.2f}\")\n","    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n","    if mesh_points is not None:\n","        print(f\"ADD (mm): {np.mean(add_errors):.3f}\")\n","        print(f\"ADD Success (<{CONFIG['ADD_THRESHOLD_MM']}mm): {add_success/len(trans_errors)*100:.2f}%\")\n","\n","    return {\n","        'trans_rmse': np.mean(trans_errors),\n","        'rot_error': np.mean(rot_errors),\n","        'points_mse': np.mean(points_errors),\n","        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n","        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n","    }\n","\n","if __name__ == \"__main__\":\n","    validate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QoKBaCBCHLHI","executionInfo":{"status":"ok","timestamp":1748568272335,"user_tz":-120,"elapsed":12120,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"89644d96-3ead-4103-f504-e9549f76067a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================= RCVPose Validation (Colab) =================\n","Using latest model: /content/models/best_model_20250530_010248.pth\n"]},{"output_type":"stream","name":"stderr","text":["\rValidating:   0%|          | 0/247 [00:00<?, ?it/s]<ipython-input-23-ae9bd51244bd>:320: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","Validating: 100%|██████████| 247/247 [00:10<00:00, 23.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Validation Results:\n","Translation RMSE: 0.0710 mm\n","Rotation Error (deg): 10.34\n","Points MSE: 0.0001\n","ADD (mm): 5.309\n","ADD Success (<10.0mm): 95.55%\n"]}]},{"cell_type":"markdown","source":["# **Test**"],"metadata":{"id":"q1BtA8hCbrYK"}},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","import cv2\n","from tqdm import tqdm\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.cuda.amp import autocast\n","\n","# =========================\n","# CONFIGURATION SECTION\n","# =========================\n","CONFIG = {\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',  # Root directory containing all object folders\n","    'OBJECT_ID': '01',  # Object ID to test (e.g., '01')\n","    'BATCH_SIZE': 1,  # For test, use batch size 1 for accurate metrics\n","    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n","    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'MODELS_DIR': '/content/models',  # Directory containing saved models\n","    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n","    'ADD_THRESHOLD_MM': 10.0,\n","    'NUM_WORKERS': 2,\n","}\n","\n","RGB_TRANSFORM = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","DEPTH_TRANSFORM = lambda img: torch.from_numpy(np.array(img, dtype=np.float32)).unsqueeze(0)\n","\n","# =========================\n","# DEPTH READER FOR .dpt FILES\n","# =========================\n","def read_depth_dpt(path):\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","    return data.reshape(h, w).astype(np.float32)\n","\n","# =========================\n","# MODEL ARCHITECTURE\n","# =========================\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","    def forward(self, features):\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])\n","        self.rgb_layer2 = list(resnet.children())[5]\n","        self.rgb_layer3 = list(resnet.children())[6]\n","        self.rgb_layer4 = list(resnet.children())[7]\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)\n","        )\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)\n","        pooled = self.global_pool(fused)\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","        outside9 = self.outside9_head(fused)\n","        target_size = (rgb.shape[2], rgb.shape[3])\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","        return pose, outside9\n","\n","# =========================\n","# DATASET CLASS\n","# =========================\n","class TestDataset(Dataset):\n","    def __init__(self, base_dir, object_id, num_radius_points=9):\n","        self.base_dir = base_dir\n","        self.object_id = object_id\n","        self.num_radius_points = num_radius_points\n","        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n","        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n","        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n","        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n","        self.radius_base_dir = os.path.join(base_dir, object_id)\n","        split_file = os.path.join(base_dir, object_id, 'Split', 'test.txt')\n","        with open(split_file, 'r') as f:\n","            self.filenames = [line.strip() for line in f.readlines()]\n","    def __len__(self):\n","        return len(self.filenames)\n","    def __getitem__(self, idx):\n","        try:\n","            filename = self.filenames[idx]\n","            base_name = filename.split('.')[0]\n","            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","            rgb_img = cv2.imread(rgb_path)\n","            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","            rgb_img = RGB_TRANSFORM(Image.fromarray(rgb_img))\n","            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","            depth_np = read_depth_dpt(depth_path)\n","            if depth_np is None:\n","                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n","            depth_np = depth_np / 1000.0  # metres\n","            depth_img = DEPTH_TRANSFORM(Image.fromarray(depth_np, mode='F'))\n","            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","            pose = np.load(pose_path)\n","            if pose.shape == (3, 4):\n","                rot = R.from_matrix(pose[:, :3]).as_quat()\n","                trans = pose[:, 3]\n","                pose = np.concatenate([trans, rot])\n","            elif pose.shape == (4, 4):\n","                rot = R.from_matrix(pose[:3, :3]).as_quat()\n","                trans = pose[:3, 3]\n","                pose = np.concatenate([trans, rot])\n","            radius_maps = []\n","            for pt_idx in range(1, self.num_radius_points + 1):\n","                radius_folder = f\"Out_pt{pt_idx}_dm\"\n","                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","                radius_map = np.load(radius_path)\n","                radius_maps.append(radius_map)\n","            radius_maps = np.array(radius_maps)\n","            return {\n","                'rgb': rgb_img,\n","                'depth': depth_img,\n","                'pose': torch.FloatTensor(pose),\n","                'radius_maps': torch.FloatTensor(radius_maps)\n","            }\n","        except Exception as e:\n","            print(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n","            return None\n","\n","def safe_collate(batch):\n","    batch = [b for b in batch if b is not None]\n","    if len(batch) == 0:\n","        return None\n","    return torch.utils.data.dataloader.default_collate(batch)\n","\n","# =========================\n","# METRIC FUNCTIONS\n","# =========================\n","def translation_rmse(pred, gt):\n","    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n","\n","def rotation_error(pred, gt, deg=True):\n","    pred_q = pred[3:] / np.linalg.norm(pred[3:])\n","    gt_q = gt[3:] / np.linalg.norm(gt[3:])\n","    dot = np.clip(np.abs(np.dot(pred_q, gt_q)), -1.0 + 1e-7, 1.0 - 1e-7)\n","    angle = 2.0 * np.arccos(dot)\n","    return np.degrees(angle) if deg else angle\n","\n","def add_metric(pred_pose, gt_pose, mesh_points):\n","    if mesh_points is None:\n","        return 0.0\n","    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n","    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n","    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n","    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n","    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n","    return mean_dist  # mm\n","\n","def get_latest_model(models_dir):\n","    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n","    if not model_files:\n","        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n","    return max(model_files, key=os.path.getctime)\n","\n","# =========================\n","# TEST PIPELINE\n","# =========================\n","def test():\n","    print(\"\\n================= RCVPose Test (Colab) =================\")\n","    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n","    print(f\"Using latest model: {model_path}\")\n","    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n","    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","    dataset = TestDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n","    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=CONFIG['NUM_WORKERS'], collate_fn=safe_collate, pin_memory=True)\n","    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n","    mesh_points = None\n","    try:\n","        if os.path.exists(mesh_path):\n","            import open3d as o3d\n","            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n","    except ImportError:\n","        print(\"open3d is not installed. Please run: !pip install open3d\")\n","    trans_errors = []\n","    rot_errors = []\n","    points_errors = []\n","    add_errors = []\n","    add_success = 0\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Testing\"):\n","            if batch is None:\n","                continue\n","            rgb = batch['rgb'].to(CONFIG['DEVICE'])\n","            depth = batch['depth'].to(CONFIG['DEVICE'])\n","            pose_gt = batch['pose'].to(CONFIG['DEVICE'])\n","            radius_maps_gt = batch['radius_maps'].to(CONFIG['DEVICE'])\n","            with autocast():\n","                pose_pred, radius_maps_pred = model(rgb, depth)\n","            pose_pred = pose_pred.cpu().numpy()\n","            pose_gt = pose_gt.cpu().numpy()\n","            radius_maps_pred = radius_maps_pred.cpu().numpy()\n","            radius_maps_gt = radius_maps_gt.cpu().numpy()\n","            for j in range(len(pose_pred)):\n","                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n","                trans_errors.append(trans_error)\n","                rot_error = rotation_error(pose_pred[j], pose_gt[j])\n","                rot_errors.append(rot_error)\n","                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n","                points_errors.append(points_error)\n","                if mesh_points is not None:\n","                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n","                    add_errors.append(add_error)\n","                    if add_error < CONFIG['ADD_THRESHOLD_MM']:\n","                        add_success += 1\n","    print(\"\\nTest Results:\")\n","    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n","    print(f\"Rotation Error (deg): {np.mean(rot_errors):.2f}\")\n","    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n","    if mesh_points is not None:\n","        print(f\"ADD (mm): {np.mean(add_errors):.3f}\")\n","        print(f\"ADD Success (<{CONFIG['ADD_THRESHOLD_MM']}mm): {add_success/len(trans_errors)*100:.2f}%\")\n","    return {\n","        'trans_rmse': np.mean(trans_errors),\n","        'rot_error': np.mean(rot_errors),\n","        'points_mse': np.mean(points_errors),\n","        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n","        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n","    }\n","\n","if __name__ == \"__main__\":\n","    test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4Mhrqi_bu5q","executionInfo":{"status":"ok","timestamp":1748568292155,"user_tz":-120,"elapsed":7313,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"f182669c-ef20-4415-f20d-4b09f83493c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================= RCVPose Test (Colab) =================\n","Using latest model: /content/models/best_model_20250530_010248.pth\n"]},{"output_type":"stream","name":"stderr","text":["\rTesting:   0%|          | 0/124 [00:00<?, ?it/s]<ipython-input-24-2b2162de47cc>:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","Testing: 100%|██████████| 124/124 [00:05<00:00, 21.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Results:\n","Translation RMSE: 0.0693 mm\n","Rotation Error (deg): 10.81\n","Points MSE: 0.0001\n","ADD (mm): 5.466\n","ADD Success (<10.0mm): 94.35%\n"]}]},{"cell_type":"markdown","metadata":{"id":"8yar8tY6XPh1"},"source":["# **All data are in millimeter scale.**"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1C6CXJWlRC5Mqfws-zB9kqh4-OeMSd4YM","timestamp":1747998293619},{"file_id":"1LEXC3BfIpeRC4cTCPIhH1QZ05N01bh1k","timestamp":1747955566766},{"file_id":"1NxmXsK5njA3LFwUTTPIei7t7qWxkHZim","timestamp":1747877518212},{"file_id":"1Ed8hajlaGmwSAwDFUftKdDTn5ReDDJjw","timestamp":1747775767352}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}