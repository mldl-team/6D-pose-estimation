{"cells":[{"cell_type":"markdown","metadata":{"id":"SMWItJ1CUp56"},"source":["# **Configure Google Colab Settings**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22187,"status":"ok","timestamp":1748442555073,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"St4iy9xVUvr8","outputId":"d9054cdc-01c9-4d6e-c637-0621689ac04b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Connect to personal Google Drive space\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1748442557321,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"6e80OV_qUcfQ","outputId":"28dc8f04-9585-453b-d359-a87f4f56bade"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","mkdir: cannot create directory ‘/content/dataset/’: File exists\n","mkdir: cannot create directory ‘/content/dataset/linemod/’: File exists\n"]}],"source":["#Change directory\n","%cd /content/\n","#Create Paths\n","!mkdir /content/dataset/\n","!mkdir /content/dataset/linemod/\n","!mkdir /content/dataset/linemod/Linemod_preprocessed/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":157720,"status":"ok","timestamp":1748442719566,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"7kIVCfM8UwgR","outputId":"efed263c-bd3f-4f82-f2b2-e437e71f5684"},"outputs":[{"output_type":"stream","name":"stdout","text":["📦 Extracting 62142 files...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Unzipping: 100%|██████████| 62142/62142 [02:33<00:00, 404.22file/s]  "]},{"output_type":"stream","name":"stdout","text":["\n","✅ Extraction complete.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import zipfile\n","import os\n","from tqdm import tqdm\n","\n","zip_path = \"/content/drive/MyDrive/Linemod_preprocessed.zip\"\n","extract_to = \"/content/dataset/linemod/\"\n","\n","# Create the output directory if it doesn't exist\n","os.makedirs(extract_to, exist_ok=True)\n","\n","# Open the zip file\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    # Retrieve the list of files in the archive\n","    file_list = zip_ref.infolist()\n","\n","    print(f\"📦 Extracting {len(file_list)} files...\\n\")\n","    for file in tqdm(file_list, desc=\"Unzipping\", unit=\"file\"):\n","        # Extract each file to the target directory\n","        zip_ref.extract(file, extract_to)\n","\n","print(\"\\n✅ Extraction complete.\")"]},{"cell_type":"markdown","metadata":{"id":"Rio2kCCbU0rl"},"source":["# **Configure Github Settings**"]},{"cell_type":"markdown","metadata":{"id":"zFNfdbV7BQgc"},"source":["### Install Git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5346,"status":"ok","timestamp":1747997208088,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"WskY0S65VGM9","outputId":"b3ce1e85-2142-44b0-c69c-52621ef9a260"},"outputs":[{"output_type":"stream","name":"stdout","text":["shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.12).\n","0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.12).\n","Calculating upgrade... Done\n","The following packages have been kept back:\n","  libcudnn9-cuda-12 libcudnn9-dev-cuda-12 libldap-2.5-0 libnccl-dev libnccl2\n","The following packages will be upgraded:\n","  base-files binutils binutils-common binutils-x86-64-linux-gnu\n","  cuda-toolkit-12-config-common cuda-toolkit-config-common e2fsprogs\n","  libbinutils libc-bin libcap2 libctf-nobfd0 libctf0 libext2fs2 libgnutls30\n","  libpam-modules libpam-modules-bin libpam-runtime libpam0g libperl5.34\n","  libseccomp2 libss2 libtasn1-6 libudev1 linux-libc-dev logsave openssl perl\n","  perl-base perl-modules-5.34\n","29 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n","Need to get 1,298 kB/18.9 MB of archives.\n","After this operation, 133 kB of additional disk space will be used.\n","Ign:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 linux-libc-dev amd64 5.15.0-139.149\n","Err:1 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 linux-libc-dev amd64 5.15.0-139.149\n","  404  Not Found [IP: 185.125.190.81 80]\n","\u001b[1;31mE: \u001b[0mFailed to fetch http://security.ubuntu.com/ubuntu/pool/main/l/linux/linux-libc-dev_5.15.0-139.149_amd64.deb  404  Not Found [IP: 185.125.190.81 80]\u001b[0m\n","\u001b[1;31mE: \u001b[0mUnable to fetch some archives, maybe run apt-get update or try with --fix-missing?\u001b[0m\n"]}],"source":["#Install/Upgrade Git\n","!apt-get install git\n","!apt upgrade git"]},{"cell_type":"markdown","metadata":{"id":"BIXMgN0jVTDp"},"source":["### Clone project from Github\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15392,"status":"ok","timestamp":1747997291029,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"GlWFu5IEiZHW","outputId":"27859655-bfe7-44d6-9ed1-4656856adc5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into '6D-pose-estimation'...\n","remote: Enumerating objects: 155, done.\u001b[K\n","remote: Counting objects: 100% (2/2), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 155 (delta 0), reused 0 (delta 0), pack-reused 153 (from 3)\u001b[K\n","Receiving objects: 100% (155/155), 205.82 MiB | 15.24 MiB/s, done.\n","Resolving deltas: 100% (24/24), done.\n"]}],"source":["%cd /content\n","!git clone https://github.com/mldl-team/6D-pose-estimation.git"]},{"cell_type":"markdown","metadata":{"id":"IRP4-CsXVbZL"},"source":["### Copy modified code into the cloned project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":913,"status":"ok","timestamp":1747997757749,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"MLzANWzrVLv_","outputId":"dbbc3f9f-459e-4e1a-e807-9a2d70243245"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/6D-pose-estimation\n"]}],"source":["#Copy the file into cloned project\n","%cd /content/6D-pose-estimation\n","!cp -r \"/content/drive/MyDrive/Backups/6 - Backup/rcvpose\" \"/content/6D-pose-estimation/RCVPose\"\n","!cp -r \"/content/drive/MyDrive/Backups/6 - Backup/6D-Pose-Estimation.ipynb\" \"/content/6D-pose-estimation\""]},{"cell_type":"markdown","metadata":{"id":"cjH76VX3BTow"},"source":["### Login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2MnT90rh5SV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747997796542,"user_tz":-120,"elapsed":1226,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"88f7d525-c79b-46d1-fca8-cb857a2c6af7"},"outputs":[{"output_type":"stream","name":"stdout","text":["From https://github.com/mldl-team/6D-pose-estimation\n"," * branch            sina       -> FETCH_HEAD\n","Branch 'sina' set up to track remote branch 'sina' from 'origin'.\n","Switched to a new branch 'sina'\n"]}],"source":["#Config Git Before Push\n","!git config --global user.email \"sina.ghiabi1@gmail.com\"\n","!git config --global user.name \"sina-ghiabi\"\n","\n","#Authentication\n","!git remote set-url origin https://sina-ghiabi:<SSH Key>@github.com/mldl-team/6D-pose-estimation.git\n","!git fetch origin sina\n","!git checkout sina"]},{"cell_type":"markdown","metadata":{"id":"hDNLbiT4Vnkb"},"source":["### Push the modified project into Github"]},{"cell_type":"code","source":["# Step 1: Navigate to the project directory\n","%cd /content/6D-pose-estimation\n","\n","# Step 2: Remove internal Git folder from rcvpose (if it exists)\n","!rm -rf RCVPose/rcvpose/.git\n","\n","# Step 3: Remove submodule from Git index (not the actual files)\n","!git rm --cached -r RCVPose/rcvpose || echo \"rcvpose was not tracked or already clean\"\n","\n","# Step 4: Re-add the folder as a normal directory\n","!git add .\n","\n","# Step 5: Commit with a clear message\n","!git commit -m \"RCVPose - Sixth Version (re-added rcvpose as regular folder)\"\n","\n","# Step 6: Push to the 'sina' branch\n","!git push origin sina"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"6S3sD5j3fe7m","executionInfo":{"status":"ok","timestamp":1747998162516,"user_tz":-120,"elapsed":1152,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"1968d4ac-ec87-4788-85e2-585f1f52a10b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/6D-pose-estimation\n","rm 'RCVPose/rcvpose/.gitignore'\n","rm 'RCVPose/rcvpose/3DRadius_lm.py'\n","rm 'RCVPose/rcvpose/3DRadius_ycb.py'\n","rm 'RCVPose/rcvpose/AccumulatorSpace.py'\n","rm 'RCVPose/rcvpose/LICENSE'\n","rm 'RCVPose/rcvpose/README.md'\n","rm 'RCVPose/rcvpose/data_loader.py'\n","rm 'RCVPose/rcvpose/doc/teaser_code.gif'\n","rm 'RCVPose/rcvpose/main.py'\n","rm 'RCVPose/rcvpose/models/fcnresnet.py'\n","rm 'RCVPose/rcvpose/rcvpose.yml'\n","rm 'RCVPose/rcvpose/rmap_dataset.py'\n","rm 'RCVPose/rcvpose/train.py'\n","rm 'RCVPose/rcvpose/util/horn.py'\n","rm 'RCVPose/rcvpose/utils.py'\n","On branch sina\n","Your branch is up to date with 'origin/sina'.\n","\n","nothing to commit, working tree clean\n","Everything up-to-date\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jtvb6CJ_VwP0"},"source":["# **Configure Wandb Settings**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2316,"status":"ok","timestamp":1747778368264,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"mI6oDV8iV4AY","outputId":"6acba1f3-5ae1-4386-d271-93c1448ebfc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n","Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"]}],"source":["#Wandb installation\n","!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9327,"status":"ok","timestamp":1747778382449,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"I5ZYYFDTV4q2","outputId":"d4507ed9-b436-43dd-c01f-7604c088b2d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msina-ghiabi\u001b[0m (\u001b[33merythm-mldl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}],"source":["!wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":6237,"status":"ok","timestamp":1747778400109,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"h-piQyhBV6le","outputId":"4b8b84ad-a284-4ae6-dc76-af4f4caddc9b"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msina-ghiabi\u001b[0m (\u001b[33merythm-mldl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250520_215956-3i6cbp0i</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">warm-pond-15</a></strong> to <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">https://wandb.ai/erythm-mldl/6D</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▇▄█▅▇</td></tr><tr><td>loss</td><td>█▆▃▄▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.84825</td></tr><tr><td>loss</td><td>0.12104</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">warm-pond-15</strong> at: <a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i</a><br> View project at: <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">https://wandb.ai/erythm-mldl/6D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20250520_215956-3i6cbp0i/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import random\n","\n","import wandb\n","\n","# Start a new wandb run to track this script.\n","run = wandb.init(\n","    # Set the wandb entity where your project will be logged (generally your team name).\n","    entity=\"erythm-mldl\",\n","    # Set the wandb project where this run will be logged.\n","    project=\"6D\",\n","    # Track hyperparameters and run metadata.\n","    config={\n","        \"learning_rate\": 0.02,\n","        \"architecture\": \"CNN\",\n","        \"dataset\": \"CIFAR-100\",\n","        \"epochs\": 10,\n","    },\n",")\n","\n","# Simulate training.\n","epochs = 10\n","offset = random.random() / 5\n","for epoch in range(2, epochs):\n","    acc = 1 - 2**-epoch - random.random() / epoch - offset\n","    loss = 2**-epoch + random.random() / epoch + offset\n","\n","    # Log metrics to wandb.\n","    run.log({\"acc\": acc, \"loss\": loss})\n","\n","# Finish the run and upload any remaining data.\n","run.finish()"]},{"cell_type":"markdown","metadata":{"id":"ja4g4pbfWAOK"},"source":["# **Configure Python**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2939,"status":"ok","timestamp":1748443270406,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"tf4W2glvWGIg","outputId":"a625020c-53a2-4a06-b6f2-436c4fc10e45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n","Collecting jedi>=0.16 (from ipython)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jedi\n","Successfully installed jedi-0.19.2\n"]}],"source":["#Install Python\n","!pip install ipython\n","\n","#Usage of the library is to display input or output images\n","from IPython.display import Image, display"]},{"cell_type":"markdown","metadata":{"id":"VpvHIshEiF5O"},"source":["# **Configure PyTorch**"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":72886,"status":"ok","timestamp":1748443352854,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"GL955jklWLg6","outputId":"be025a55-90f9-4eb2-8ce3-2acce2ba7bab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]},{"output_type":"execute_result","data":{"text/plain":["'2.6.0+cu124'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["#Install PyTorch\n","!pip install torch\n","!pip install torch torchvision opencv-python matplotlib tqdm pyyaml\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","\n","#Usage of the library is the primary neural network computation framework.\n","import torch\n","torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"eI7EMHExgdOs"},"source":["# **Configure YOLO**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3824,"status":"ok","timestamp":1748346779889,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"mkcXvJosWJG7","outputId":"334e3a3f-1efd-4684-9e96-969818a898f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.145-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.145-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.145 ultralytics-thop-2.0.14\n","/bin/bash: -c: line 2: syntax error: unexpected end of file\n","Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["#Usage of the library is To run Object Detection & Image Classification with YOLO\n","!pip install ultralytics\n","\n","#Check availability of ultralytics\n","!ultralytics.checks()\n","\n","import ultralytics\n","\n","from ultralytics import YOLO"]},{"cell_type":"markdown","metadata":{"id":"cxJlZbFTlShT"},"source":["# **Configure OpenCV**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2558,"status":"ok","timestamp":1748443362664,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"dCEtKW0eTrST","outputId":"22f81b45-b628-4ea4-945c-c13998e16ada"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"]}],"source":["#Usage of the library is for real‐time image and video processing\n","!pip install opencv-python\n","\n","import cv2"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":26,"status":"ok","timestamp":1748443373380,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"zvAaOEWyWNu4","outputId":"547a1828-e55a-4818-ab6a-61a0a83b8452"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed May 28 14:42:53 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["#Check if we have an access to nvidia\n","#If \"/bin/bash: line 1: nvidia-smi: command not found\" appeared, change Runtime to GPU\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"IiEXNi0d1krR"},"source":["# **Configure Open3D**\n","### **Open3D is used for 3D geometry processing in Pose Estimation, SciPy for scientific computations**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":31310,"status":"ok","timestamp":1748443408644,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"6V3FOZ9H0pF8","outputId":"46deb6d2-6017-4331-9e1e-bbedd63b7c86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting open3d\n","  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Collecting dash>=2.6.0 (from open3d)\n","  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n","Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n","Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n","Collecting configargparse (from open3d)\n","  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n","Collecting ipywidgets>=8.0.4 (from open3d)\n","  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n","Collecting addict (from open3d)\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.2.1)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n","Collecting pyquaternion (from open3d)\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n","Collecting flask>=3.0.0 (from open3d)\n","  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n","Collecting werkzeug>=3.0.0 (from open3d)\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.13.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n","Collecting retrying (from dash>=2.6.0->open3d)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.2.1)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n","Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n","Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n","  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.25.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.4.26)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n","Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n","Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Installing collected packages: addict, widgetsnbextension, werkzeug, retrying, pyquaternion, configargparse, comm, flask, ipywidgets, dash, open3d\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 3.1.1\n","    Uninstalling Flask-3.1.1:\n","      Successfully uninstalled Flask-3.1.1\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7.1 dash-3.0.4 flask-3.0.3 ipywidgets-8.1.7 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.14\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.0.6)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.5.21)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}],"source":["!pip install open3d scipy numpy\n","!pip install opencv-python tensorboard pyyaml scipy scikit-image\n","!pip install tensorboardX\n","\n","import open3d as o3d\n","from scipy.spatial import distance_matrix\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"hcrD4hz007gZ"},"source":["### **Manage file paths with os Library**"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"OiDNB_0YWPhe","executionInfo":{"status":"ok","timestamp":1748443416592,"user_tz":-120,"elapsed":24,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}}},"outputs":[],"source":["import os\n","import glob\n","\n","#Examples\n","#img_path = os.path.join('data', 'images', img_name)\n","#images = glob.glob('data/images/*.jpg')\n","#labels = glob.glob('data/labels/*.txt')"]},{"cell_type":"markdown","metadata":{"id":"WZxMTz5eKiig"},"source":["### **Progress Bar**"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2397,"status":"ok","timestamp":1748443420056,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"OvswuY3cKfa_","outputId":"6f95a51b-edfe-408b-a251-1d4632590c87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"]}],"source":["!pip install tqdm\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"BYVEIJsVY16K"},"source":["# **Object Detection**\n"]},{"cell_type":"markdown","metadata":{"id":"hXrj5tdH6FsL"},"source":["### **Import all the Tools**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":933,"status":"ok","timestamp":1747403978128,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"JqLkUNE_ZGLC","outputId":"9b398b85-7dc6-4f4a-c76d-b3a68973ea57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 21.5M/21.5M [00:00<00:00, 140MB/s] \n"]},{"data":{"text/plain":["YOLO(\n","  (model): DetectionModel(\n","    (model): Sequential(\n","      (0): Conv(\n","        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (1): Conv(\n","        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (2): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (3): Conv(\n","        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (4): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0-1): 2 x Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (5): Conv(\n","        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (6): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0-1): 2 x Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (7): Conv(\n","        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (8): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (9): SPPF(\n","        (cv1): Conv(\n","          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n","      )\n","      (10): Upsample(scale_factor=2.0, mode='nearest')\n","      (11): Concat()\n","      (12): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (13): Upsample(scale_factor=2.0, mode='nearest')\n","      (14): Concat()\n","      (15): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (16): Conv(\n","        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (17): Concat()\n","      (18): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (19): Conv(\n","        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","        (act): SiLU(inplace=True)\n","      )\n","      (20): Concat()\n","      (21): C2f(\n","        (cv1): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (cv2): Conv(\n","          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","          (act): SiLU(inplace=True)\n","        )\n","        (m): ModuleList(\n","          (0): Bottleneck(\n","            (cv1): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (cv2): Conv(\n","              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (22): Detect(\n","        (cv2): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (cv3): ModuleList(\n","          (0): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Sequential(\n","            (0): Conv(\n","              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (1): Conv(\n","              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n","              (act): SiLU(inplace=True)\n","            )\n","            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (dfl): DFL(\n","          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","  )\n",")"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","\n","#Choose YOLO Model\n","model = YOLO('yolov8s.pt')\n","#Choose Computing Device - CUDA: Compute Unified Device Architecture\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","#Transfer Model to Computing Device\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"00BexPmO6J9N"},"source":["### **Make Dataset Ready**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":188095,"status":"ok","timestamp":1747412607059,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"LX6Kpu2tdb9r","outputId":"5b28b097-5d56-47a3-9541-de8961c1ce99"},"outputs":[{"name":"stdout","output_type":"stream","text":["🔍 Class distribution:\n","Class  1: 1236 samples\n","Class  2: 1214 samples\n","Class  4: 1201 samples\n","Class  5: 1196 samples\n","Class  6: 1179 samples\n","Class  8: 1188 samples\n","Class  9: 1254 samples\n","Class 10: 1253 samples\n","Class 11: 1220 samples\n","Class 12: 1237 samples\n","Class 13: 1152 samples\n","Class 14: 1227 samples\n","Class 15: 1243 samples\n","\n","⚙️ Processing and splitting images...\n","    Train Count  Validation Count\n","0           988               248\n","1           971               243\n","2           960               241\n","3           956               240\n","4           943               236\n","5           950               238\n","6          1003               251\n","7          1002               251\n","8           976               244\n","9           989               248\n","10          921               231\n","11          981               246\n","12          994               249\n","✅ YOLO dataset generated, split, and labeled.\n"]}],"source":["import os\n","import yaml\n","import random\n","import shutil\n","from PIL import Image\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","\n","# ================== CONFIGURATION ======================\n","!mkdir /content/dataset/linemod/Linemod_ready\n","root_dir = '/content/dataset/linemod/Linemod_preprocessed/data'\n","output_base = '/content/dataset/linemod/Linemod_ready'  # Base output folder\n","img_out_train = os.path.join(output_base, 'images/train')\n","img_out_val = os.path.join(output_base, 'images/val')\n","label_out_train = os.path.join(output_base, 'labels/train')\n","label_out_val = os.path.join(output_base, 'labels/val')\n","\n","# Create directories if they don't exist\n","for path in [img_out_train, img_out_val, label_out_train, label_out_val]:\n","    os.makedirs(path, exist_ok=True)\n","\n","# Define valid class IDs and their YOLO-mapped indices (0-based)\n","existing_classes = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n","#Create {cls_id: idx}: {0:1} - {1:2} - {2:4} - {3:5} ...\n","class_map = {cls_id: idx for idx, cls_id in enumerate(existing_classes)}\n","\n","# ================ STEP 1: GATHER SAMPLES ===================\n","samples_by_class = defaultdict(list)\n","for cls in existing_classes:\n","    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n","    rgb_folder = os.path.join(folder_path, 'rgb')\n","    gt_file = os.path.join(folder_path, 'gt.yml')\n","\n","    if not os.path.exists(gt_file):\n","        print(f\"⚠️ Class {cls:02d} missing → skipped.\")\n","        continue\n","\n","    with open(gt_file, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    for img_id in gt_data:\n","        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n","        if os.path.exists(img_path):\n","            samples_by_class[cls].append(img_id)\n","\n","# Display class distribution\n","print(\"🔍 Class distribution:\")\n","for cls, samples in samples_by_class.items():\n","    print(f\"Class {cls:>2}: {len(samples)} samples\")\n","\n","# =============== STEP 2: LABEL AND IMAGE PROCESSING =================\n","def save_yolo_labels(sample_list, mode, cls, gt_data, rgb_folder):\n","    label_dir = label_out_train if mode == 'train' else label_out_val\n","    img_dir = img_out_train if mode == 'train' else img_out_val\n","    label_id = class_map[cls]\n","\n","    for img_id in sample_list:\n","        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n","        img = Image.open(img_path)\n","        w, h = img.size\n","\n","        bbox = gt_data[img_id][0]['obj_bb']\n","        x, y, bw, bh = bbox\n","        x_center = (x + bw / 2) / w\n","        y_center = (y + bh / 2) / h\n","        norm_bw = bw / w\n","        norm_bh = bh / h\n","\n","        # Save label\n","        label_file = f\"{cls:02d}_{int(img_id):04d}.txt\"\n","        with open(os.path.join(label_dir, label_file), 'w') as f:\n","            f.write(f\"{label_id} {x_center:.6f} {y_center:.6f} {norm_bw:.6f} {norm_bh:.6f}\\n\")\n","\n","        # Save image\n","        img_out_path = os.path.join(img_dir, f\"{cls:02d}_{int(img_id):04d}.png\")\n","        shutil.copy(img_path, img_out_path)\n","\n","# Split, save labels and copy images\n","print(\"\\n⚙️ Processing and splitting images...\")\n","for cls, samples in samples_by_class.items():\n","    random.shuffle(samples)\n","    split_idx = int(0.8 * len(samples))\n","    train_samples = samples[:split_idx]\n","    val_samples = samples[split_idx:]\n","\n","    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n","    rgb_folder = os.path.join(folder_path, 'rgb')\n","    gt_file = os.path.join(folder_path, 'gt.yml')\n","\n","    with open(gt_file, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    save_yolo_labels(train_samples, 'train', cls, gt_data, rgb_folder)\n","    save_yolo_labels(val_samples, 'val', cls, gt_data, rgb_folder)\n","\n","# ============= STEP 3: LABEL DISTRIBUTION STATS ===============\n","def count_labels(label_dir):\n","    counter = Counter()\n","    for filename in os.listdir(label_dir):\n","        if filename.endswith('.txt'):\n","            with open(os.path.join(label_dir, filename), 'r') as f:\n","                for line in f:\n","                    class_id = int(line.strip().split()[0])\n","                    counter[class_id] += 1\n","    return counter\n","\n","train_counts = count_labels(label_out_train)\n","val_counts = count_labels(label_out_val)\n","\n","import pandas as pd\n","df_stats = pd.DataFrame({\n","    \"Train Count\": pd.Series(train_counts),\n","    \"Validation Count\": pd.Series(val_counts)\n","}).fillna(0).astype(int)\n","\n","print(df_stats)\n","\n","print(\"✅ YOLO dataset generated, split, and labeled.\")"]},{"cell_type":"markdown","metadata":{"id":"Y5UQxMsZ6RT2"},"source":[]},{"cell_type":"markdown","metadata":{"id":"raqAJg_s6Y8D"},"source":["### **Train the Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWdLRQ-qyHUQ"},"outputs":[],"source":["model.train(\n","    data='/content/project/6D-pose-estimation/configs/linemod_final.yaml',\n","    epochs=15,\n","    imgsz=640,\n","    batch=8,\n","    device=0,\n","    patience=5,\n","    weight_decay=0.0005\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXhj_4zwy4Gt"},"outputs":[],"source":["!cp -r runs/detect/train ~/6d/yolo_logs/"]},{"cell_type":"markdown","metadata":{"id":"ZACuf_m66sGQ"},"source":["### **Save Trained Model Metrics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YD2UGsFky-pW"},"outputs":[],"source":["from ultralytics import YOLO\n","\n","model = YOLO(\"model.pt\")\n","metrics = model.val(data=\"linemod_final.yaml\", plots=True, save=True)"]},{"cell_type":"markdown","metadata":{"id":"Z05pObtQAvQb"},"source":["# **Pose Estimation**"]},{"cell_type":"markdown","metadata":{"id":"NLw9Lzbazk4D"},"source":["### **Prepare File & Folders for RCVPose**"]},{"cell_type":"markdown","metadata":{"id":"DingMrNW0bPZ"},"source":["### 1 - Move Objects' Models From models Folder To Class Folders & Rename Them To mesh.ply"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":49,"status":"ok","timestamp":1748443437144,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"0n9I3pMgxT0u","outputId":"56e16107-f69a-40d8-85ce-bca129fbf90d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Copying obj_10.ply → /content/dataset/linemod/Linemod_preprocessed/data/10/mesh.ply\n","Copying obj_12.ply → /content/dataset/linemod/Linemod_preprocessed/data/12/mesh.ply\n","Copying obj_06.ply → /content/dataset/linemod/Linemod_preprocessed/data/06/mesh.ply\n","Copying obj_08.ply → /content/dataset/linemod/Linemod_preprocessed/data/08/mesh.ply\n","Copying obj_14.ply → /content/dataset/linemod/Linemod_preprocessed/data/14/mesh.ply\n","Folder 07 does not exist → skipped\n","Folder 03 does not exist → skipped\n","Copying obj_11.ply → /content/dataset/linemod/Linemod_preprocessed/data/11/mesh.ply\n","Copying obj_02.ply → /content/dataset/linemod/Linemod_preprocessed/data/02/mesh.ply\n","Copying obj_04.ply → /content/dataset/linemod/Linemod_preprocessed/data/04/mesh.ply\n","Copying obj_13.ply → /content/dataset/linemod/Linemod_preprocessed/data/13/mesh.ply\n","Copying obj_09.ply → /content/dataset/linemod/Linemod_preprocessed/data/09/mesh.ply\n","Copying obj_05.ply → /content/dataset/linemod/Linemod_preprocessed/data/05/mesh.ply\n","Copying obj_01.ply → /content/dataset/linemod/Linemod_preprocessed/data/01/mesh.ply\n","Copying obj_15.ply → /content/dataset/linemod/Linemod_preprocessed/data/15/mesh.ply\n","\n","✅ Model files have been moved and renamed to mesh.ply.\n"]}],"source":["import os\n","import shutil\n","\n","# ==== CONFIGURATION ====\n","# Base directory for preprocessed model data\n","data_path = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","# Directory containing original model files\n","models_path = \"/content/dataset/linemod/Linemod_preprocessed/models\"\n","\n","for filename in os.listdir(models_path):\n","    # Only process files matching the pattern obj_<id>.ply\n","    if not filename.startswith(\"obj_\") or not filename.endswith(\".ply\"):\n","        continue\n","\n","    # Extract the model identifier (e.g., \"01\")\n","    model_id = filename.split(\"_\")[1].split(\".\")[0]  # example: \"01\"\n","    src_file = os.path.join(models_path, filename)\n","    dst_folder = os.path.join(data_path, model_id)\n","\n","    # Skip if the destination folder does not exist\n","    if not os.path.isdir(dst_folder):\n","        print(f\"Folder {model_id} does not exist → skipped\")\n","        continue\n","\n","    # Copy and rename the model file to mesh.ply\n","    dst_file = os.path.join(dst_folder, \"mesh.ply\")\n","    print(f\"Copying {filename} → {dst_file}\")\n","    shutil.copy2(src_file, dst_file)\n","\n","print(\"\\n✅ Model files have been moved and renamed to mesh.ply.\")\n"]},{"cell_type":"markdown","metadata":{"id":"vunah5M8cYW-"},"source":["### 2 - Create Outside9.npy From Objects' Models\n","A NumPy file containing a [9, 3] array of 3D keypoint coordinates sampled from the object mesh using Farthest-Point Sampling. These five points are chosen to maximally span the surface of the model. During data preprocessing, each keypoint is used to generate a per-frame radial distance map: for every pixel with valid depth, its Euclidean distance in 3D space to each keypoint is computed and stored. These distance maps serve as ground-truth supervision when training a network to predict 3D distance fields from RGB-D inputs.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":439,"status":"ok","timestamp":1748443446756,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"g0L1ky8r2svp","outputId":"f18fe3d0-a727-4c40-8d66-af563c48aabd"},"outputs":[{"output_type":"stream","name":"stdout","text":["🎯 Processing class 01\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/01/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/01/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 02\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/02/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/02/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 04\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/04/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/04/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 05\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/05/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/05/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 06\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/06/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/06/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 08\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/08/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/08/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 09\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/09/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/09/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 10\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/10/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/10/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 11\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/11/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/11/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 12\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/12/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/12/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 13\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/13/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/13/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 14\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/14/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/14/Outside9.npy loaded successfully with shape (9, 3)\n","\n","🎯 Processing class 15\n","✅ Saved /content/dataset/linemod/Linemod_preprocessed/data/15/Outside9.npy (shape (9, 3))\n","📦 Verified file: /content/dataset/linemod/Linemod_preprocessed/data/15/Outside9.npy loaded successfully with shape (9, 3)\n","\n"]}],"source":["import os\n","import numpy as np\n","import open3d as o3d\n","\n","def fps(points: np.ndarray, k: int, seed: int = 0) -> np.ndarray:\n","    \"\"\"\n","    Farthest-Point Sampling:\n","      - points: (N,3) array of XYZ samples\n","      - k: number of keypoints to pick\n","    Returns an array of shape (k,3).\n","    \"\"\"\n","    np.random.seed(seed)\n","    N = points.shape[0]\n","    centroids = np.zeros((k,), dtype=np.int32)\n","    distances = np.full((N,), np.inf)\n","    farthest = np.random.randint(0, N)\n","    for i in range(k):\n","        centroids[i] = farthest\n","        centroid = points[farthest]\n","        dist = np.sum((points - centroid)**2, axis=1)\n","        distances = np.minimum(distances, dist)\n","        farthest = np.argmax(distances)\n","    return points[centroids]\n","\n","# --- Define base path and class list ---\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","for cls in classes:\n","    seq_dir = os.path.join(base_dir, cls)\n","    mesh_path = os.path.join(seq_dir, \"mesh.ply\")\n","\n","    if not os.path.isfile(mesh_path):\n","        print(f\"⚠️  Skipping {cls}: mesh.ply not found at {mesh_path}\")\n","        continue\n","\n","    print(f\"🎯 Processing class {cls}\")\n","\n","    # Load point cloud\n","    pcd = o3d.io.read_point_cloud(mesh_path)\n","    pts = np.asarray(pcd.points)\n","\n","    if pts.shape[0] < 9:\n","        print(f\"❌ Not enough points in {mesh_path} (only {pts.shape[0]}), skipping.\")\n","        continue\n","\n","    # Perform Farthest-Point Sampling\n","    keypoints = fps(pts, k=9, seed=42)\n","\n","    # Save output\n","    out_path = os.path.join(seq_dir, \"Outside9.npy\")\n","    np.save(out_path, keypoints)\n","    print(f\"✅ Saved {out_path} (shape {keypoints.shape})\")\n","\n","    # Test load to confirm file is valid\n","    try:\n","        test = np.load(out_path)\n","        print(f\"📦 Verified file: {out_path} loaded successfully with shape {test.shape}\\n\")\n","    except Exception as e:\n","        print(f\"❌ Failed to load saved file: {e}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"AXbUbafiiMFD"},"source":["### It is only Possible to see the content of Outside9.npy using code below"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1748443459300,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"rrrof0ashbQ9","outputId":"f4aa6d0f-979f-4a94-ec2a-da63f95c1138"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📂 Class 01 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/01/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -44.2630\n"," ├─ Max  : 41.4566\n"," ├─ Mean : -3.6767\n"," ├─ Std  : 23.3938\n"," └─ Sample Data:\n","[[-20.3664  19.3442  -7.6361]\n"," [ 35.6255 -16.9712 -44.263 ]\n"," [  7.5111  -4.4303  41.4566]\n"," [ -1.604  -33.9459  -5.4867]\n"," [-26.6571 -13.9838 -43.0416]\n"," [ 10.2731  20.1762 -40.7712]\n"," [ 13.9023  17.8718   9.8713]\n"," [-20.7606  -9.8099  16.1704]\n"," [ 30.0282 -21.3168 -10.4571]]\n","\n","📂 Class 02 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/02/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -107.0920\n"," ├─ Max  : 107.3880\n"," ├─ Mean : 0.5087\n"," ├─ Std  : 54.8957\n"," └─ Sample Data:\n","[[  12.3025  -17.8971   43.9721]\n"," [  90.5719   -1.9537 -107.092 ]\n"," [ -70.8535   22.383   -92.6023]\n"," [  53.2234   58.4881  -23.509 ]\n"," [   7.5829  -29.9273  -56.5234]\n"," [ -92.0255   -4.1247   -3.2999]\n"," [  66.5464    0.135   107.388 ]\n"," [  86.4643  -24.3331  -25.008 ]\n"," [ -11.3591   30.5359   -5.3499]]\n","\n","📂 Class 04 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/04/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -67.9932\n"," ├─ Max  : 67.0034\n"," ├─ Mean : -11.1607\n"," ├─ Std  : 39.5965\n"," └─ Sample Data:\n","[[-60.7914  62.0638 -19.3466]\n"," [ 67.0034 -10.3907  -2.8697]\n"," [-54.8219 -67.9932  19.5177]\n"," [-10.7869  13.9897  49.5203]\n"," [-19.0693 -15.506  -47.6997]\n"," [ 17.4433  49.3472 -20.1936]\n"," [-66.316   -0.8159   7.7016]\n"," [  4.4803 -41.5652  16.038 ]\n"," [-56.6145 -67.7992 -45.8652]]\n","\n","📂 Class 05 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/05/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -93.7359\n"," ├─ Max  : 96.3970\n"," ├─ Mean : -3.6729\n"," ├─ Std  : 47.5687\n"," └─ Sample Data:\n","[[ -8.0273  55.7049 -79.5385]\n"," [  1.2705   6.302   96.397 ]\n"," [ -6.0933 -70.6847 -20.2746]\n"," [ 47.7824  17.2083   1.3454]\n"," [-48.5596  10.6845  13.091 ]\n"," [ 35.0964 -27.4034 -93.7359]\n"," [  2.8314  88.323   -1.3848]\n"," [-42.4456 -17.0875 -69.874 ]\n"," [ 45.2718  25.511  -60.8797]]\n","\n","📂 Class 06 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/06/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -61.5181\n"," ├─ Max  : 55.2023\n"," ├─ Mean : -5.8993\n"," ├─ Std  : 34.8858\n"," └─ Sample Data:\n","[[ -9.8392  50.4696  31.5456]\n"," [ 21.0611 -61.5181 -54.2087]\n"," [-27.4429  20.7841 -56.9346]\n"," [  1.3448 -54.5936  25.5383]\n"," [ 18.5875   3.4214  -9.0781]\n"," [-32.6254 -37.7601 -54.0536]\n"," [ 27.5001  27.813  -57.959 ]\n"," [ 12.0627   7.1267  55.2023]\n"," [-26.0997   4.744   15.631 ]]\n","\n","📂 Class 08 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/08/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -102.5330\n"," ├─ Max  : 114.7380\n"," ├─ Mean : -1.7252\n"," ├─ Std  : 57.1376\n"," └─ Sample Data:\n","[[ -85.1721   27.1747   55.3571]\n"," [  41.817   -21.577  -102.533 ]\n"," [ 114.738    -3.9728   89.5442]\n"," [  18.887    -2.2339   10.7861]\n"," [ -64.6352    9.4542  -96.3116]\n"," [  12.7168   -1.2401  101.553 ]\n"," [ -52.2799   -5.5599  -20.5459]\n"," [ -37.3041  -31.165    53.1621]\n"," [   1.973    36.0232  -95.2365]]\n","\n","📂 Class 09 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/09/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -46.6694\n"," ├─ Max  : 46.7280\n"," ├─ Mean : -4.6406\n"," ├─ Std  : 27.9066\n"," └─ Sample Data:\n","[[ -2.0994  11.7574   0.0198]\n"," [ 46.728  -13.5908 -40.1141]\n"," [-35.4824 -23.6476 -38.1239]\n"," [ 44.0857  -6.8009  20.9951]\n"," [ 21.6419  31.9062 -40.411 ]\n"," [-30.3699  26.0363 -38.8646]\n"," [  7.4866 -33.6071 -20.1031]\n"," [  1.8624  -9.9197  40.7914]\n"," [-46.6694   3.9089  -2.7115]]\n","\n","📂 Class 10 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/10/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -73.4333\n"," ├─ Max  : 63.0991\n"," ├─ Mean : -0.9485\n"," ├─ Std  : 36.2279\n"," └─ Sample Data:\n","[[ 39.1349  11.601  -23.9408]\n"," [-73.4333 -30.9207  -0.2043]\n"," [-29.2001  45.2791  29.0616]\n"," [  4.209  -42.7445  30.9551]\n"," [-56.7928  29.7865 -33.7575]\n"," [ 63.0991 -46.6746  -1.2703]\n"," [-14.9766 -26.8483 -32.3674]\n"," [ 44.4272  40.0844  33.1058]\n"," [ -5.5014  42.452  -20.1714]]\n","\n","📂 Class 11 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/11/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -84.7123\n"," ├─ Max  : 85.9639\n"," ├─ Mean : -6.8638\n"," ├─ Std  : 34.8670\n"," └─ Sample Data:\n","[[ 15.494  -15.955   -7.3843]\n"," [  0.3111   5.3904  85.9639]\n"," [-12.4399  31.7775 -84.7123]\n"," [  7.3067 -37.177  -76.2814]\n"," [-11.4441  34.2681 -24.9297]\n"," [-10.3177  14.479   31.7019]\n"," [ 16.6566   7.0834 -54.5396]\n"," [-15.0613 -32.8293 -36.4945]\n"," [-11.6849 -26.7442  22.2407]]\n","\n","📂 Class 12 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/12/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -46.0149\n"," ├─ Max  : 49.0182\n"," ├─ Mean : -1.7436\n"," ├─ Std  : 34.0428\n"," └─ Sample Data:\n","[[  1.2851 -26.6466  13.895 ]\n"," [ 44.2851  48.1457 -37.705 ]\n"," [-43.2355  29.6682 -42.005 ]\n"," [ 42.1351  36.1182  40.9671]\n"," [-42.4672 -41.2818 -39.855 ]\n"," [ 48.7106 -21.9318 -37.705 ]\n"," [-15.9149  49.0182  13.9386]\n"," [-46.0149  -4.7318  -0.097 ]\n"," [  5.5851  10.3182 -31.5558]]\n","\n","📂 Class 13 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/13/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -127.0280\n"," ├─ Max  : 127.4950\n"," ├─ Mean : -7.1318\n"," ├─ Std  : 59.5631\n"," └─ Sample Data:\n","[[ -11.3506  -37.9226  -11.2949]\n"," [ 127.495    -0.2632  -67.9833]\n"," [-127.028    44.9615  -37.3693]\n"," [-107.902    -1.4071   69.7612]\n"," [  80.0632    0.7081   38.3604]\n"," [  28.8828   49.0412  -65.602 ]\n"," [ -99.7896  -47.943   -50.6508]\n"," [ -42.0713   43.7073  -13.5953]\n"," [ -21.6835    5.7878   62.5302]]\n","\n","📂 Class 14 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/14/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -101.7510\n"," ├─ Max  : 101.2500\n"," ├─ Mean : -6.5086\n"," ├─ Std  : 62.7174\n"," └─ Sample Data:\n","[[  43.2177   52.4112   54.0458]\n"," [ -99.9506  -36.8233 -101.751 ]\n"," [  32.046    34.081   -98.0899]\n"," [ -70.1905  -24.798    76.0068]\n"," [  82.3728  -56.4652   23.6938]\n"," [  -5.7534  -11.7776  -12.261 ]\n"," [ -63.4212   52.9445  -85.1847]\n"," [ 101.25     -3.7718   92.8678]\n"," [ -11.3597  -43.5055  -95.5663]]\n","\n","📂 Class 15 — Outside9.npy\n"," ├─ Path : /content/dataset/linemod/Linemod_preprocessed/data/15/Outside9.npy\n"," ├─ Shape: (9, 3)\n"," ├─ DType: float64\n"," ├─ Min  : -87.9908\n"," ├─ Max  : 91.5867\n"," ├─ Mean : -14.2489\n"," ├─ Std  : 49.5074\n"," └─ Sample Data:\n","[[-33.3298 -51.9169 -81.7832]\n"," [-10.1223 -27.1229  91.5867]\n"," [ -3.8379  72.9849 -24.3855]\n"," [ 46.3641   7.2564 -79.3277]\n"," [ 10.0339 -60.7272   3.6516]\n"," [-37.3104  31.5644 -87.9908]\n"," [-39.8922   0.1733 -13.9607]\n"," [ 36.755  -63.5682 -71.5871]\n"," [ 19.6582  64.2833 -82.1688]]\n","\n","✅ All classes processed.\n"]}],"source":["import numpy as np\n","import os\n","\n","# Ensure NumPy will print the full array\n","np.set_printoptions(precision=4, suppress=True, threshold=np.inf)\n","\n","base_path = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","\n","# List of classes (01–15, skipping 03 and 07)\n","classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","for cls in classes:\n","    file_path = os.path.join(base_path, cls, \"Outside9.npy\")\n","    if not os.path.isfile(file_path):\n","        print(f\"⚠️  File not found for class {cls}: {file_path}\")\n","        continue\n","\n","    data = np.load(file_path)\n","    flat = data.flatten()\n","    # Compute some summary statistics\n","    shape   = data.shape\n","    dtype   = data.dtype\n","    minimum = flat.min()\n","    maximum = flat.max()\n","    mean    = flat.mean()\n","    std     = flat.std()\n","\n","    # Nicely formatted output\n","    print(f\"\\n📂 Class {cls} — Outside9.npy\")\n","    print(f\" ├─ Path : {file_path}\")\n","    print(f\" ├─ Shape: {shape}\")\n","    print(f\" ├─ DType: {dtype}\")\n","    print(f\" ├─ Min  : {minimum:.4f}\")\n","    print(f\" ├─ Max  : {maximum:.4f}\")\n","    print(f\" ├─ Mean : {mean:.4f}\")\n","    print(f\" ├─ Std  : {std:.4f}\")\n","    # Print the full matrix\n","    print(\" └─ Sample Data:\")\n","    print(data)\n","\n","print(\"\\n✅ All classes processed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"PDddF5Sy-XbT"},"source":["### **3 - Generate Pose For Each Picture Using gt.yml**\n","\n","Purpose: This script generates pose files (Rotation & Translation matrices) for each image in the dataset.\n","\n","How it works:\n","1. Reads the depth image and removes the background using the mask.\n","2. Converts the depth into a point cloud (scene point cloud).\n","3. Loads the 3D model (`mesh.ply`) and samples it into a point cloud.\n","4. Uses ICP to align the model point cloud with the scene point cloud.\n","5. Saves the resulting [R|t] matrix as a `.npy` file named `poseXXXXXX.npy`.\n","\n","Output:\n","Each frame will have a file like `pose000123.npy` in the `pose/` folder.\n","It contains a 3×4 RT matrix with:\n","- R: rotation (3×3)\n","- t: translation (3×1)\n","\n","Use in RCVPose:\n","These pose files are used to:\n","- Project 3D keypoints onto 2D image space\n","- Generate 3D radius maps\n","- Serve as ground truth during training\n","\n","Requirements:\n","- `.dpt` depth images\n","- `.png` binary masks\n","- 3D model in `.ply` format\n","- Camera intrinsics `K`\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26047,"status":"ok","timestamp":1748443491068,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"Pj1gZdE7-9j4","outputId":"67b1594c-fe0c-44d4-9d5e-40556192513c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 01...\n"]},{"output_type":"stream","name":"stderr","text":["Class 01: 100%|██████████| 1236/1236 [00:00<00:00, 10963.11image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 02...\n"]},{"output_type":"stream","name":"stderr","text":["Class 02: 100%|██████████| 1214/1214 [00:00<00:00, 9540.37image/s]\n"]},{"output_type":"stream","name":"stdout","text":["⚠️ Multiple objects in image 000000, using object 2.\n","⚠️ Multiple objects in image 000001, using object 2.\n","⚠️ Multiple objects in image 000002, using object 2.\n","⚠️ Multiple objects in image 000003, using object 2.\n","⚠️ Multiple objects in image 000004, using object 2.\n","⚠️ Multiple objects in image 000005, using object 2.\n","⚠️ Multiple objects in image 000006, using object 2.\n","⚠️ Multiple objects in image 000007, using object 2.\n","⚠️ Multiple objects in image 000008, using object 2.\n","⚠️ Multiple objects in image 000009, using object 2.\n","⚠️ Multiple objects in image 000010, using object 2.\n","⚠️ Multiple objects in image 000011, using object 2.\n","⚠️ Multiple objects in image 000012, using object 2.\n","⚠️ Multiple objects in image 000013, using object 2.\n","⚠️ Multiple objects in image 000014, using object 2.\n","⚠️ Multiple objects in image 000015, using object 2.\n","⚠️ Multiple objects in image 000016, using object 2.\n","⚠️ Multiple objects in image 000017, using object 2.\n","⚠️ Multiple objects in image 000018, using object 2.\n","⚠️ Multiple objects in image 000019, using object 2.\n","⚠️ Multiple objects in image 000020, using object 2.\n","⚠️ Multiple objects in image 000021, using object 2.\n","⚠️ Multiple objects in image 000022, using object 2.\n","⚠️ Multiple objects in image 000023, using object 2.\n","⚠️ Multiple objects in image 000024, using object 2.\n","⚠️ Multiple objects in image 000025, using object 2.\n","⚠️ Multiple objects in image 000026, using object 2.\n","⚠️ Multiple objects in image 000027, using object 2.\n","⚠️ Multiple objects in image 000028, using object 2.\n","⚠️ Multiple objects in image 000029, using object 2.\n","⚠️ Multiple objects in image 000030, using object 2.\n","⚠️ Multiple objects in image 000031, using object 2.\n","⚠️ Multiple objects in image 000032, using object 2.\n","⚠️ Multiple objects in image 000033, using object 2.\n","⚠️ Multiple objects in image 000034, using object 2.\n","⚠️ Multiple objects in image 000035, using object 2.\n","⚠️ Multiple objects in image 000036, using object 2.\n","⚠️ Multiple objects in image 000037, using object 2.\n","⚠️ Multiple objects in image 000038, using object 2.\n","⚠️ Multiple objects in image 000039, using object 2.\n","⚠️ Multiple objects in image 000040, using object 2.\n","⚠️ Multiple objects in image 000041, using object 2.\n","⚠️ Multiple objects in image 000042, using object 2.\n","⚠️ Multiple objects in image 000043, using object 2.\n","⚠️ Multiple objects in image 000044, using object 2.\n","⚠️ Multiple objects in image 000045, using object 2.\n","⚠️ Multiple objects in image 000046, using object 2.\n","⚠️ Multiple objects in image 000047, using object 2.\n","⚠️ Multiple objects in image 000048, using object 2.\n","⚠️ Multiple objects in image 000049, using object 2.\n","⚠️ Multiple objects in image 000050, using object 2.\n","⚠️ Multiple objects in image 000051, using object 2.\n","⚠️ Multiple objects in image 000052, using object 2.\n","⚠️ Multiple objects in image 000053, using object 2.\n","⚠️ Multiple objects in image 000054, using object 2.\n","⚠️ Multiple objects in image 000055, using object 2.\n","⚠️ Multiple objects in image 000056, using object 2.\n","⚠️ Multiple objects in image 000057, using object 2.\n","⚠️ Multiple objects in image 000058, using object 2.\n","⚠️ Multiple objects in image 000059, using object 2.\n","⚠️ Multiple objects in image 000060, using object 2.\n","⚠️ Multiple objects in image 000061, using object 2.\n","⚠️ Multiple objects in image 000062, using object 2.\n","⚠️ Multiple objects in image 000063, using object 2.\n","⚠️ Multiple objects in image 000064, using object 2.\n","⚠️ Multiple objects in image 000065, using object 2.\n","⚠️ Multiple objects in image 000066, using object 2.\n","⚠️ Multiple objects in image 000067, using object 2.\n","⚠️ Multiple objects in image 000068, using object 2.\n","⚠️ Multiple objects in image 000069, using object 2.\n","⚠️ Multiple objects in image 000070, using object 2.\n","⚠️ Multiple objects in image 000071, using object 2.\n","⚠️ Multiple objects in image 000072, using object 2.\n","⚠️ Multiple objects in image 000073, using object 2.\n","⚠️ Multiple objects in image 000074, using object 2.\n","⚠️ Multiple objects in image 000075, using object 2.\n","⚠️ Multiple objects in image 000076, using object 2.\n","⚠️ Multiple objects in image 000077, using object 2.\n","⚠️ Multiple objects in image 000078, using object 2.\n","⚠️ Multiple objects in image 000079, using object 2.\n","⚠️ Multiple objects in image 000080, using object 2.\n","⚠️ Multiple objects in image 000081, using object 2.\n","⚠️ Multiple objects in image 000082, using object 2.\n","⚠️ Multiple objects in image 000083, using object 2.\n","⚠️ Multiple objects in image 000084, using object 2.\n","⚠️ Multiple objects in image 000085, using object 2.\n","⚠️ Multiple objects in image 000086, using object 2.\n","⚠️ Multiple objects in image 000087, using object 2.\n","⚠️ Multiple objects in image 000088, using object 2.\n","⚠️ Multiple objects in image 000089, using object 2.\n","⚠️ Multiple objects in image 000090, using object 2.\n","⚠️ Multiple objects in image 000091, using object 2.\n","⚠️ Multiple objects in image 000092, using object 2.\n","⚠️ Multiple objects in image 000093, using object 2.\n","⚠️ Multiple objects in image 000094, using object 2.\n","⚠️ Multiple objects in image 000095, using object 2.\n","⚠️ Multiple objects in image 000096, using object 2.\n","⚠️ Multiple objects in image 000097, using object 2.\n","⚠️ Multiple objects in image 000098, using object 2.\n","⚠️ Multiple objects in image 000099, using object 2.\n","⚠️ Multiple objects in image 000100, using object 2.\n","⚠️ Multiple objects in image 000101, using object 2.\n","⚠️ Multiple objects in image 000102, using object 2.\n","⚠️ Multiple objects in image 000103, using object 2.\n","⚠️ Multiple objects in image 000104, using object 2.\n","⚠️ Multiple objects in image 000105, using object 2.\n","⚠️ Multiple objects in image 000106, using object 2.\n","⚠️ Multiple objects in image 000107, using object 2.\n","⚠️ Multiple objects in image 000108, using object 2.\n","⚠️ Multiple objects in image 000109, using object 2.\n","⚠️ Multiple objects in image 000110, using object 2.\n","⚠️ Multiple objects in image 000111, using object 2.\n","⚠️ Multiple objects in image 000112, using object 2.\n","⚠️ Multiple objects in image 000113, using object 2.\n","⚠️ Multiple objects in image 000114, using object 2.\n","⚠️ Multiple objects in image 000115, using object 2.\n","⚠️ Multiple objects in image 000116, using object 2.\n","⚠️ Multiple objects in image 000117, using object 2.\n","⚠️ Multiple objects in image 000118, using object 2.\n","⚠️ Multiple objects in image 000119, using object 2.\n","⚠️ Multiple objects in image 000120, using object 2.\n","⚠️ Multiple objects in image 000121, using object 2.\n","⚠️ Multiple objects in image 000122, using object 2.\n","⚠️ Multiple objects in image 000123, using object 2.\n","⚠️ Multiple objects in image 000124, using object 2.\n","⚠️ Multiple objects in image 000125, using object 2.\n","⚠️ Multiple objects in image 000126, using object 2.\n","⚠️ Multiple objects in image 000127, using object 2.\n","⚠️ Multiple objects in image 000128, using object 2.\n","⚠️ Multiple objects in image 000129, using object 2.\n","⚠️ Multiple objects in image 000130, using object 2.\n","⚠️ Multiple objects in image 000131, using object 2.\n","⚠️ Multiple objects in image 000132, using object 2.\n","⚠️ Multiple objects in image 000133, using object 2.\n","⚠️ Multiple objects in image 000134, using object 2.\n","⚠️ Multiple objects in image 000135, using object 2.\n","⚠️ Multiple objects in image 000136, using object 2.\n","⚠️ Multiple objects in image 000137, using object 2.\n","⚠️ Multiple objects in image 000138, using object 2.\n","⚠️ Multiple objects in image 000139, using object 2.\n","⚠️ Multiple objects in image 000140, using object 2.\n","⚠️ Multiple objects in image 000141, using object 2.\n","⚠️ Multiple objects in image 000142, using object 2.\n","⚠️ Multiple objects in image 000143, using object 2.\n","⚠️ Multiple objects in image 000144, using object 2.\n","⚠️ Multiple objects in image 000145, using object 2.\n","⚠️ Multiple objects in image 000146, using object 2.\n","⚠️ Multiple objects in image 000147, using object 2.\n","⚠️ Multiple objects in image 000148, using object 2.\n","⚠️ Multiple objects in image 000149, using object 2.\n","⚠️ Multiple objects in image 000150, using object 2.\n","⚠️ Multiple objects in image 000151, using object 2.\n","⚠️ Multiple objects in image 000152, using object 2.\n","⚠️ Multiple objects in image 000153, using object 2.\n","⚠️ Multiple objects in image 000154, using object 2.\n","⚠️ Multiple objects in image 000155, using object 2.\n","⚠️ Multiple objects in image 000156, using object 2.\n","⚠️ Multiple objects in image 000157, using object 2.\n","⚠️ Multiple objects in image 000158, using object 2.\n","⚠️ Multiple objects in image 000159, using object 2.\n","⚠️ Multiple objects in image 000160, using object 2.\n","⚠️ Multiple objects in image 000161, using object 2.\n","⚠️ Multiple objects in image 000162, using object 2.\n","⚠️ Multiple objects in image 000163, using object 2.\n","⚠️ Multiple objects in image 000164, using object 2.\n","⚠️ Multiple objects in image 000165, using object 2.\n","⚠️ Multiple objects in image 000166, using object 2.\n","⚠️ Multiple objects in image 000167, using object 2.\n","⚠️ Multiple objects in image 000168, using object 2.\n","⚠️ Multiple objects in image 000169, using object 2.\n","⚠️ Multiple objects in image 000170, using object 2.\n","⚠️ Multiple objects in image 000171, using object 2.\n","⚠️ Multiple objects in image 000172, using object 2.\n","⚠️ Multiple objects in image 000173, using object 2.\n","⚠️ Multiple objects in image 000174, using object 2.\n","⚠️ Multiple objects in image 000175, using object 2.\n","⚠️ Multiple objects in image 000176, using object 2.\n","⚠️ Multiple objects in image 000177, using object 2.\n","⚠️ Multiple objects in image 000178, using object 2.\n","⚠️ Multiple objects in image 000179, using object 2.\n","⚠️ Multiple objects in image 000180, using object 2.\n","⚠️ Multiple objects in image 000181, using object 2.\n","⚠️ Multiple objects in image 000182, using object 2.\n","⚠️ Multiple objects in image 000183, using object 2.\n","⚠️ Multiple objects in image 000184, using object 2.\n","⚠️ Multiple objects in image 000185, using object 2.\n","⚠️ Multiple objects in image 000186, using object 2.\n","⚠️ Multiple objects in image 000187, using object 2.\n","⚠️ Multiple objects in image 000188, using object 2.\n","⚠️ Multiple objects in image 000189, using object 2.\n","⚠️ Multiple objects in image 000190, using object 2.\n","⚠️ Multiple objects in image 000191, using object 2.\n","⚠️ Multiple objects in image 000192, using object 2.\n","⚠️ Multiple objects in image 000193, using object 2.\n","⚠️ Multiple objects in image 000194, using object 2.\n","⚠️ Multiple objects in image 000195, using object 2.\n","⚠️ Multiple objects in image 000196, using object 2.\n","⚠️ Multiple objects in image 000197, using object 2.\n","⚠️ Multiple objects in image 000198, using object 2.\n","⚠️ Multiple objects in image 000199, using object 2.\n","⚠️ Multiple objects in image 000200, using object 2.\n","⚠️ Multiple objects in image 000201, using object 2.\n","⚠️ Multiple objects in image 000202, using object 2.\n","⚠️ Multiple objects in image 000203, using object 2.\n","⚠️ Multiple objects in image 000204, using object 2.\n","⚠️ Multiple objects in image 000205, using object 2.\n","⚠️ Multiple objects in image 000206, using object 2.\n","⚠️ Multiple objects in image 000207, using object 2.\n","⚠️ Multiple objects in image 000208, using object 2.\n","⚠️ Multiple objects in image 000209, using object 2.\n","⚠️ Multiple objects in image 000210, using object 2.\n","⚠️ Multiple objects in image 000211, using object 2.\n","⚠️ Multiple objects in image 000212, using object 2.\n","⚠️ Multiple objects in image 000213, using object 2.\n","⚠️ Multiple objects in image 000214, using object 2.\n","⚠️ Multiple objects in image 000215, using object 2.\n","⚠️ Multiple objects in image 000216, using object 2.\n","⚠️ Multiple objects in image 000217, using object 2.\n","⚠️ Multiple objects in image 000218, using object 2.\n","⚠️ Multiple objects in image 000219, using object 2.\n","⚠️ Multiple objects in image 000220, using object 2.\n","⚠️ Multiple objects in image 000221, using object 2.\n","⚠️ Multiple objects in image 000222, using object 2.\n","⚠️ Multiple objects in image 000223, using object 2.\n","⚠️ Multiple objects in image 000224, using object 2.\n","⚠️ Multiple objects in image 000225, using object 2.\n","⚠️ Multiple objects in image 000226, using object 2.\n","⚠️ Multiple objects in image 000227, using object 2.\n","⚠️ Multiple objects in image 000228, using object 2.\n","⚠️ Multiple objects in image 000229, using object 2.\n","⚠️ Multiple objects in image 000230, using object 2.\n","⚠️ Multiple objects in image 000231, using object 2.\n","⚠️ Multiple objects in image 000232, using object 2.\n","⚠️ Multiple objects in image 000233, using object 2.\n","⚠️ Multiple objects in image 000234, using object 2.\n","⚠️ Multiple objects in image 000235, using object 2.\n","⚠️ Multiple objects in image 000236, using object 2.\n","⚠️ Multiple objects in image 000237, using object 2.\n","⚠️ Multiple objects in image 000238, using object 2.\n","⚠️ Multiple objects in image 000239, using object 2.\n","⚠️ Multiple objects in image 000240, using object 2.\n","⚠️ Multiple objects in image 000241, using object 2.\n","⚠️ Multiple objects in image 000242, using object 2.\n","⚠️ Multiple objects in image 000243, using object 2.\n","⚠️ Multiple objects in image 000244, using object 2.\n","⚠️ Multiple objects in image 000245, using object 2.\n","⚠️ Multiple objects in image 000246, using object 2.\n","⚠️ Multiple objects in image 000247, using object 2.\n","⚠️ Multiple objects in image 000248, using object 2.\n","⚠️ Multiple objects in image 000249, using object 2.\n","⚠️ Multiple objects in image 000250, using object 2.\n","⚠️ Multiple objects in image 000251, using object 2.\n","⚠️ Multiple objects in image 000252, using object 2.\n","⚠️ Multiple objects in image 000253, using object 2.\n","⚠️ Multiple objects in image 000254, using object 2.\n","⚠️ Multiple objects in image 000255, using object 2.\n","⚠️ Multiple objects in image 000256, using object 2.\n","⚠️ Multiple objects in image 000257, using object 2.\n","⚠️ Multiple objects in image 000258, using object 2.\n","⚠️ Multiple objects in image 000259, using object 2.\n","⚠️ Multiple objects in image 000260, using object 2.\n","⚠️ Multiple objects in image 000261, using object 2.\n","⚠️ Multiple objects in image 000262, using object 2.\n","⚠️ Multiple objects in image 000263, using object 2.\n","⚠️ Multiple objects in image 000264, using object 2.\n","⚠️ Multiple objects in image 000265, using object 2.\n","⚠️ Multiple objects in image 000266, using object 2.\n","⚠️ Multiple objects in image 000267, using object 2.\n","⚠️ Multiple objects in image 000268, using object 2.\n","⚠️ Multiple objects in image 000269, using object 2.\n","⚠️ Multiple objects in image 000270, using object 2.\n","⚠️ Multiple objects in image 000271, using object 2.\n","⚠️ Multiple objects in image 000272, using object 2.\n","⚠️ Multiple objects in image 000273, using object 2.\n","⚠️ Multiple objects in image 000274, using object 2.\n","⚠️ Multiple objects in image 000275, using object 2.\n","⚠️ Multiple objects in image 000276, using object 2.\n","⚠️ Multiple objects in image 000277, using object 2.\n","⚠️ Multiple objects in image 000278, using object 2.\n","⚠️ Multiple objects in image 000279, using object 2.\n","⚠️ Multiple objects in image 000280, using object 2.\n","⚠️ Multiple objects in image 000281, using object 2.\n","⚠️ Multiple objects in image 000282, using object 2.\n","⚠️ Multiple objects in image 000283, using object 2.\n","⚠️ Multiple objects in image 000284, using object 2.\n","⚠️ Multiple objects in image 000285, using object 2.\n","⚠️ Multiple objects in image 000286, using object 2.\n","⚠️ Multiple objects in image 000287, using object 2.\n","⚠️ Multiple objects in image 000288, using object 2.\n","⚠️ Multiple objects in image 000289, using object 2.\n","⚠️ Multiple objects in image 000290, using object 2.\n","⚠️ Multiple objects in image 000291, using object 2.\n","⚠️ Multiple objects in image 000292, using object 2.\n","⚠️ Multiple objects in image 000293, using object 2.\n","⚠️ Multiple objects in image 000294, using object 2.\n","⚠️ Multiple objects in image 000295, using object 2.\n","⚠️ Multiple objects in image 000296, using object 2.\n","⚠️ Multiple objects in image 000297, using object 2.\n","⚠️ Multiple objects in image 000298, using object 2.\n","⚠️ Multiple objects in image 000299, using object 2.\n","⚠️ Multiple objects in image 000300, using object 2.\n","⚠️ Multiple objects in image 000301, using object 2.\n","⚠️ Multiple objects in image 000302, using object 2.\n","⚠️ Multiple objects in image 000303, using object 2.\n","⚠️ Multiple objects in image 000304, using object 2.\n","⚠️ Multiple objects in image 000305, using object 2.\n","⚠️ Multiple objects in image 000306, using object 2.\n","⚠️ Multiple objects in image 000307, using object 2.\n","⚠️ Multiple objects in image 000308, using object 2.\n","⚠️ Multiple objects in image 000309, using object 2.\n","⚠️ Multiple objects in image 000310, using object 2.\n","⚠️ Multiple objects in image 000311, using object 2.\n","⚠️ Multiple objects in image 000312, using object 2.\n","⚠️ Multiple objects in image 000313, using object 2.\n","⚠️ Multiple objects in image 000314, using object 2.\n","⚠️ Multiple objects in image 000315, using object 2.\n","⚠️ Multiple objects in image 000316, using object 2.\n","⚠️ Multiple objects in image 000317, using object 2.\n","⚠️ Multiple objects in image 000318, using object 2.\n","⚠️ Multiple objects in image 000319, using object 2.\n","⚠️ Multiple objects in image 000320, using object 2.\n","⚠️ Multiple objects in image 000321, using object 2.\n","⚠️ Multiple objects in image 000322, using object 2.\n","⚠️ Multiple objects in image 000323, using object 2.\n","⚠️ Multiple objects in image 000324, using object 2.\n","⚠️ Multiple objects in image 000325, using object 2.\n","⚠️ Multiple objects in image 000326, using object 2.\n","⚠️ Multiple objects in image 000327, using object 2.\n","⚠️ Multiple objects in image 000328, using object 2.\n","⚠️ Multiple objects in image 000329, using object 2.\n","⚠️ Multiple objects in image 000330, using object 2.\n","⚠️ Multiple objects in image 000331, using object 2.\n","⚠️ Multiple objects in image 000332, using object 2.\n","⚠️ Multiple objects in image 000333, using object 2.\n","⚠️ Multiple objects in image 000334, using object 2.\n","⚠️ Multiple objects in image 000335, using object 2.\n","⚠️ Multiple objects in image 000336, using object 2.\n","⚠️ Multiple objects in image 000337, using object 2.\n","⚠️ Multiple objects in image 000338, using object 2.\n","⚠️ Multiple objects in image 000339, using object 2.\n","⚠️ Multiple objects in image 000340, using object 2.\n","⚠️ Multiple objects in image 000341, using object 2.\n","⚠️ Multiple objects in image 000342, using object 2.\n","⚠️ Multiple objects in image 000343, using object 2.\n","⚠️ Multiple objects in image 000344, using object 2.\n","⚠️ Multiple objects in image 000345, using object 2.\n","⚠️ Multiple objects in image 000346, using object 2.\n","⚠️ Multiple objects in image 000347, using object 2.\n","⚠️ Multiple objects in image 000348, using object 2.\n","⚠️ Multiple objects in image 000349, using object 2.\n","⚠️ Multiple objects in image 000350, using object 2.\n","⚠️ Multiple objects in image 000351, using object 2.\n","⚠️ Multiple objects in image 000352, using object 2.\n","⚠️ Multiple objects in image 000353, using object 2.\n","⚠️ Multiple objects in image 000354, using object 2.\n","⚠️ Multiple objects in image 000355, using object 2.\n","⚠️ Multiple objects in image 000356, using object 2.\n","⚠️ Multiple objects in image 000357, using object 2.\n","⚠️ Multiple objects in image 000358, using object 2.\n","⚠️ Multiple objects in image 000359, using object 2.\n","⚠️ Multiple objects in image 000360, using object 2.\n","⚠️ Multiple objects in image 000361, using object 2.\n","⚠️ Multiple objects in image 000362, using object 2.\n","⚠️ Multiple objects in image 000363, using object 2.\n","⚠️ Multiple objects in image 000364, using object 2.\n","⚠️ Multiple objects in image 000365, using object 2.\n","⚠️ Multiple objects in image 000366, using object 2.\n","⚠️ Multiple objects in image 000367, using object 2.\n","⚠️ Multiple objects in image 000368, using object 2.\n","⚠️ Multiple objects in image 000369, using object 2.\n","⚠️ Multiple objects in image 000370, using object 2.\n","⚠️ Multiple objects in image 000371, using object 2.\n","⚠️ Multiple objects in image 000372, using object 2.\n","⚠️ Multiple objects in image 000373, using object 2.\n","⚠️ Multiple objects in image 000374, using object 2.\n","⚠️ Multiple objects in image 000375, using object 2.\n","⚠️ Multiple objects in image 000376, using object 2.\n","⚠️ Multiple objects in image 000377, using object 2.\n","⚠️ Multiple objects in image 000378, using object 2.\n","⚠️ Multiple objects in image 000379, using object 2.\n","⚠️ Multiple objects in image 000380, using object 2.\n","⚠️ Multiple objects in image 000381, using object 2.\n","⚠️ Multiple objects in image 000382, using object 2.\n","⚠️ Multiple objects in image 000383, using object 2.\n","⚠️ Multiple objects in image 000384, using object 2.\n","⚠️ Multiple objects in image 000385, using object 2.\n","⚠️ Multiple objects in image 000386, using object 2.\n","⚠️ Multiple objects in image 000387, using object 2.\n","⚠️ Multiple objects in image 000388, using object 2.\n","⚠️ Multiple objects in image 000389, using object 2.\n","⚠️ Multiple objects in image 000390, using object 2.\n","⚠️ Multiple objects in image 000391, using object 2.\n","⚠️ Multiple objects in image 000392, using object 2.\n","⚠️ Multiple objects in image 000393, using object 2.\n","⚠️ Multiple objects in image 000394, using object 2.\n","⚠️ Multiple objects in image 000395, using object 2.\n","⚠️ Multiple objects in image 000396, using object 2.\n","⚠️ Multiple objects in image 000397, using object 2.\n","⚠️ Multiple objects in image 000398, using object 2.\n","⚠️ Multiple objects in image 000399, using object 2.\n","⚠️ Multiple objects in image 000400, using object 2.\n","⚠️ Multiple objects in image 000401, using object 2.\n","⚠️ Multiple objects in image 000402, using object 2.\n","⚠️ Multiple objects in image 000403, using object 2.\n","⚠️ Multiple objects in image 000404, using object 2.\n","⚠️ Multiple objects in image 000405, using object 2.\n","⚠️ Multiple objects in image 000406, using object 2.\n","⚠️ Multiple objects in image 000407, using object 2.\n","⚠️ Multiple objects in image 000408, using object 2.\n","⚠️ Multiple objects in image 000409, using object 2.\n","⚠️ Multiple objects in image 000410, using object 2.\n","⚠️ Multiple objects in image 000411, using object 2.\n","⚠️ Multiple objects in image 000412, using object 2.\n","⚠️ Multiple objects in image 000413, using object 2.\n","⚠️ Multiple objects in image 000414, using object 2.\n","⚠️ Multiple objects in image 000415, using object 2.\n","⚠️ Multiple objects in image 000416, using object 2.\n","⚠️ Multiple objects in image 000417, using object 2.\n","⚠️ Multiple objects in image 000418, using object 2.\n","⚠️ Multiple objects in image 000419, using object 2.\n","⚠️ Multiple objects in image 000420, using object 2.\n","⚠️ Multiple objects in image 000421, using object 2.\n","⚠️ Multiple objects in image 000422, using object 2.\n","⚠️ Multiple objects in image 000423, using object 2.\n","⚠️ Multiple objects in image 000424, using object 2.\n","⚠️ Multiple objects in image 000425, using object 2.\n","⚠️ Multiple objects in image 000426, using object 2.\n","⚠️ Multiple objects in image 000427, using object 2.\n","⚠️ Multiple objects in image 000428, using object 2.\n","⚠️ Multiple objects in image 000429, using object 2.\n","⚠️ Multiple objects in image 000430, using object 2.\n","⚠️ Multiple objects in image 000431, using object 2.\n","⚠️ Multiple objects in image 000432, using object 2.\n","⚠️ Multiple objects in image 000433, using object 2.\n","⚠️ Multiple objects in image 000434, using object 2.\n","⚠️ Multiple objects in image 000435, using object 2.\n","⚠️ Multiple objects in image 000436, using object 2.\n","⚠️ Multiple objects in image 000437, using object 2.\n","⚠️ Multiple objects in image 000438, using object 2.\n","⚠️ Multiple objects in image 000439, using object 2.\n","⚠️ Multiple objects in image 000440, using object 2.\n","⚠️ Multiple objects in image 000441, using object 2.\n","⚠️ Multiple objects in image 000442, using object 2.\n","⚠️ Multiple objects in image 000443, using object 2.\n","⚠️ Multiple objects in image 000444, using object 2.\n","⚠️ Multiple objects in image 000445, using object 2.\n","⚠️ Multiple objects in image 000446, using object 2.\n","⚠️ Multiple objects in image 000447, using object 2.\n","⚠️ Multiple objects in image 000448, using object 2.\n","⚠️ Multiple objects in image 000449, using object 2.\n","⚠️ Multiple objects in image 000450, using object 2.\n","⚠️ Multiple objects in image 000451, using object 2.\n","⚠️ Multiple objects in image 000452, using object 2.\n","⚠️ Multiple objects in image 000453, using object 2.\n","⚠️ Multiple objects in image 000454, using object 2.\n","⚠️ Multiple objects in image 000455, using object 2.\n","⚠️ Multiple objects in image 000456, using object 2.\n","⚠️ Multiple objects in image 000457, using object 2.\n","⚠️ Multiple objects in image 000458, using object 2.\n","⚠️ Multiple objects in image 000459, using object 2.\n","⚠️ Multiple objects in image 000460, using object 2.\n","⚠️ Multiple objects in image 000461, using object 2.\n","⚠️ Multiple objects in image 000462, using object 2.\n","⚠️ Multiple objects in image 000463, using object 2.\n","⚠️ Multiple objects in image 000464, using object 2.\n","⚠️ Multiple objects in image 000465, using object 2.\n","⚠️ Multiple objects in image 000466, using object 2.\n","⚠️ Multiple objects in image 000467, using object 2.\n","⚠️ Multiple objects in image 000468, using object 2.\n","⚠️ Multiple objects in image 000469, using object 2.\n","⚠️ Multiple objects in image 000470, using object 2.\n","⚠️ Multiple objects in image 000471, using object 2.\n","⚠️ Multiple objects in image 000472, using object 2.\n","⚠️ Multiple objects in image 000473, using object 2.\n","⚠️ Multiple objects in image 000474, using object 2.\n","⚠️ Multiple objects in image 000475, using object 2.\n","⚠️ Multiple objects in image 000476, using object 2.\n","⚠️ Multiple objects in image 000477, using object 2.\n","⚠️ Multiple objects in image 000478, using object 2.\n","⚠️ Multiple objects in image 000479, using object 2.\n","⚠️ Multiple objects in image 000480, using object 2.\n","⚠️ Multiple objects in image 000481, using object 2.\n","⚠️ Multiple objects in image 000482, using object 2.\n","⚠️ Multiple objects in image 000483, using object 2.\n","⚠️ Multiple objects in image 000484, using object 2.\n","⚠️ Multiple objects in image 000485, using object 2.\n","⚠️ Multiple objects in image 000486, using object 2.\n","⚠️ Multiple objects in image 000487, using object 2.\n","⚠️ Multiple objects in image 000488, using object 2.\n","⚠️ Multiple objects in image 000489, using object 2.\n","⚠️ Multiple objects in image 000490, using object 2.\n","⚠️ Multiple objects in image 000491, using object 2.\n","⚠️ Multiple objects in image 000492, using object 2.\n","⚠️ Multiple objects in image 000493, using object 2.\n","⚠️ Multiple objects in image 000494, using object 2.\n","⚠️ Multiple objects in image 000495, using object 2.\n","⚠️ Multiple objects in image 000496, using object 2.\n","⚠️ Multiple objects in image 000497, using object 2.\n","⚠️ Multiple objects in image 000498, using object 2.\n","⚠️ Multiple objects in image 000499, using object 2.\n","⚠️ Multiple objects in image 000500, using object 2.\n","⚠️ Multiple objects in image 000501, using object 2.\n","⚠️ Multiple objects in image 000502, using object 2.\n","⚠️ Multiple objects in image 000503, using object 2.\n","⚠️ Multiple objects in image 000504, using object 2.\n","⚠️ Multiple objects in image 000505, using object 2.\n","⚠️ Multiple objects in image 000506, using object 2.\n","⚠️ Multiple objects in image 000507, using object 2.\n","⚠️ Multiple objects in image 000508, using object 2.\n","⚠️ Multiple objects in image 000509, using object 2.\n","⚠️ Multiple objects in image 000510, using object 2.\n","⚠️ Multiple objects in image 000511, using object 2.\n","⚠️ Multiple objects in image 000512, using object 2.\n","⚠️ Multiple objects in image 000513, using object 2.\n","⚠️ Multiple objects in image 000514, using object 2.\n","⚠️ Multiple objects in image 000515, using object 2.\n","⚠️ Multiple objects in image 000516, using object 2.\n","⚠️ Multiple objects in image 000517, using object 2.\n","⚠️ Multiple objects in image 000518, using object 2.\n","⚠️ Multiple objects in image 000519, using object 2.\n","⚠️ Multiple objects in image 000520, using object 2.\n","⚠️ Multiple objects in image 000521, using object 2.\n","⚠️ Multiple objects in image 000522, using object 2.\n","⚠️ Multiple objects in image 000523, using object 2.\n","⚠️ Multiple objects in image 000524, using object 2.\n","⚠️ Multiple objects in image 000525, using object 2.\n","⚠️ Multiple objects in image 000526, using object 2.\n","⚠️ Multiple objects in image 000527, using object 2.\n","⚠️ Multiple objects in image 000528, using object 2.\n","⚠️ Multiple objects in image 000529, using object 2.\n","⚠️ Multiple objects in image 000530, using object 2.\n","⚠️ Multiple objects in image 000531, using object 2.\n","⚠️ Multiple objects in image 000532, using object 2.\n","⚠️ Multiple objects in image 000533, using object 2.\n","⚠️ Multiple objects in image 000534, using object 2.\n","⚠️ Multiple objects in image 000535, using object 2.\n","⚠️ Multiple objects in image 000536, using object 2.\n","⚠️ Multiple objects in image 000537, using object 2.\n","⚠️ Multiple objects in image 000538, using object 2.\n","⚠️ Multiple objects in image 000539, using object 2.\n","⚠️ Multiple objects in image 000540, using object 2.\n","⚠️ Multiple objects in image 000541, using object 2.\n","⚠️ Multiple objects in image 000542, using object 2.\n","⚠️ Multiple objects in image 000543, using object 2.\n","⚠️ Multiple objects in image 000544, using object 2.\n","⚠️ Multiple objects in image 000545, using object 2.\n","⚠️ Multiple objects in image 000546, using object 2.\n","⚠️ Multiple objects in image 000547, using object 2.\n","⚠️ Multiple objects in image 000548, using object 2.\n","⚠️ Multiple objects in image 000549, using object 2.\n","⚠️ Multiple objects in image 000550, using object 2.\n","⚠️ Multiple objects in image 000551, using object 2.\n","⚠️ Multiple objects in image 000552, using object 2.\n","⚠️ Multiple objects in image 000553, using object 2.\n","⚠️ Multiple objects in image 000554, using object 2.\n","⚠️ Multiple objects in image 000555, using object 2.\n","⚠️ Multiple objects in image 000556, using object 2.\n","⚠️ Multiple objects in image 000557, using object 2.\n","⚠️ Multiple objects in image 000558, using object 2.\n","⚠️ Multiple objects in image 000559, using object 2.\n","⚠️ Multiple objects in image 000560, using object 2.\n","⚠️ Multiple objects in image 000561, using object 2.\n","⚠️ Multiple objects in image 000562, using object 2.\n","⚠️ Multiple objects in image 000563, using object 2.\n","⚠️ Multiple objects in image 000564, using object 2.\n","⚠️ Multiple objects in image 000565, using object 2.\n","⚠️ Multiple objects in image 000566, using object 2.\n","⚠️ Multiple objects in image 000567, using object 2.\n","⚠️ Multiple objects in image 000568, using object 2.\n","⚠️ Multiple objects in image 000569, using object 2.\n","⚠️ Multiple objects in image 000570, using object 2.\n","⚠️ Multiple objects in image 000571, using object 2.\n","⚠️ Multiple objects in image 000572, using object 2.\n","⚠️ Multiple objects in image 000573, using object 2.\n","⚠️ Multiple objects in image 000574, using object 2.\n","⚠️ Multiple objects in image 000575, using object 2.\n","⚠️ Multiple objects in image 000576, using object 2.\n","⚠️ Multiple objects in image 000577, using object 2.\n","⚠️ Multiple objects in image 000578, using object 2.\n","⚠️ Multiple objects in image 000579, using object 2.\n","⚠️ Multiple objects in image 000580, using object 2.\n","⚠️ Multiple objects in image 000581, using object 2.\n","⚠️ Multiple objects in image 000582, using object 2.\n","⚠️ Multiple objects in image 000583, using object 2.\n","⚠️ Multiple objects in image 000584, using object 2.\n","⚠️ Multiple objects in image 000585, using object 2.\n","⚠️ Multiple objects in image 000586, using object 2.\n","⚠️ Multiple objects in image 000587, using object 2.\n","⚠️ Multiple objects in image 000588, using object 2.\n","⚠️ Multiple objects in image 000589, using object 2.\n","⚠️ Multiple objects in image 000590, using object 2.\n","⚠️ Multiple objects in image 000591, using object 2.\n","⚠️ Multiple objects in image 000592, using object 2.\n","⚠️ Multiple objects in image 000593, using object 2.\n","⚠️ Multiple objects in image 000594, using object 2.\n","⚠️ Multiple objects in image 000595, using object 2.\n","⚠️ Multiple objects in image 000596, using object 2.\n","⚠️ Multiple objects in image 000597, using object 2.\n","⚠️ Multiple objects in image 000598, using object 2.\n","⚠️ Multiple objects in image 000599, using object 2.\n","⚠️ Multiple objects in image 000600, using object 2.\n","⚠️ Multiple objects in image 000601, using object 2.\n","⚠️ Multiple objects in image 000602, using object 2.\n","⚠️ Multiple objects in image 000603, using object 2.\n","⚠️ Multiple objects in image 000604, using object 2.\n","⚠️ Multiple objects in image 000605, using object 2.\n","⚠️ Multiple objects in image 000606, using object 2.\n","⚠️ Multiple objects in image 000607, using object 2.\n","⚠️ Multiple objects in image 000608, using object 2.\n","⚠️ Multiple objects in image 000609, using object 2.\n","⚠️ Multiple objects in image 000610, using object 2.\n","⚠️ Multiple objects in image 000611, using object 2.\n","⚠️ Multiple objects in image 000612, using object 2.\n","⚠️ Multiple objects in image 000613, using object 2.\n","⚠️ Multiple objects in image 000614, using object 2.\n","⚠️ Multiple objects in image 000615, using object 2.\n","⚠️ Multiple objects in image 000616, using object 2.\n","⚠️ Multiple objects in image 000617, using object 2.\n","⚠️ Multiple objects in image 000618, using object 2.\n","⚠️ Multiple objects in image 000619, using object 2.\n","⚠️ Multiple objects in image 000620, using object 2.\n","⚠️ Multiple objects in image 000621, using object 2.\n","⚠️ Multiple objects in image 000622, using object 2.\n","⚠️ Multiple objects in image 000623, using object 2.\n","⚠️ Multiple objects in image 000624, using object 2.\n","⚠️ Multiple objects in image 000625, using object 2.\n","⚠️ Multiple objects in image 000626, using object 2.\n","⚠️ Multiple objects in image 000627, using object 2.\n","⚠️ Multiple objects in image 000628, using object 2.\n","⚠️ Multiple objects in image 000629, using object 2.\n","⚠️ Multiple objects in image 000630, using object 2.\n","⚠️ Multiple objects in image 000631, using object 2.\n","⚠️ Multiple objects in image 000632, using object 2.\n","⚠️ Multiple objects in image 000633, using object 2.\n","⚠️ Multiple objects in image 000634, using object 2.\n","⚠️ Multiple objects in image 000635, using object 2.\n","⚠️ Multiple objects in image 000636, using object 2.\n","⚠️ Multiple objects in image 000637, using object 2.\n","⚠️ Multiple objects in image 000638, using object 2.\n","⚠️ Multiple objects in image 000639, using object 2.\n","⚠️ Multiple objects in image 000640, using object 2.\n","⚠️ Multiple objects in image 000641, using object 2.\n","⚠️ Multiple objects in image 000642, using object 2.\n","⚠️ Multiple objects in image 000643, using object 2.\n","⚠️ Multiple objects in image 000644, using object 2.\n","⚠️ Multiple objects in image 000645, using object 2.\n","⚠️ Multiple objects in image 000646, using object 2.\n","⚠️ Multiple objects in image 000647, using object 2.\n","⚠️ Multiple objects in image 000648, using object 2.\n","⚠️ Multiple objects in image 000649, using object 2.\n","⚠️ Multiple objects in image 000650, using object 2.\n","⚠️ Multiple objects in image 000651, using object 2.\n","⚠️ Multiple objects in image 000652, using object 2.\n","⚠️ Multiple objects in image 000653, using object 2.\n","⚠️ Multiple objects in image 000654, using object 2.\n","⚠️ Multiple objects in image 000655, using object 2.\n","⚠️ Multiple objects in image 000656, using object 2.\n","⚠️ Multiple objects in image 000657, using object 2.\n","⚠️ Multiple objects in image 000658, using object 2.\n","⚠️ Multiple objects in image 000659, using object 2.\n","⚠️ Multiple objects in image 000660, using object 2.\n","⚠️ Multiple objects in image 000661, using object 2.\n","⚠️ Multiple objects in image 000662, using object 2.\n","⚠️ Multiple objects in image 000663, using object 2.\n","⚠️ Multiple objects in image 000664, using object 2.\n","⚠️ Multiple objects in image 000665, using object 2.\n","⚠️ Multiple objects in image 000666, using object 2.\n","⚠️ Multiple objects in image 000667, using object 2.\n","⚠️ Multiple objects in image 000668, using object 2.\n","⚠️ Multiple objects in image 000669, using object 2.\n","⚠️ Multiple objects in image 000670, using object 2.\n","⚠️ Multiple objects in image 000671, using object 2.\n","⚠️ Multiple objects in image 000672, using object 2.\n","⚠️ Multiple objects in image 000673, using object 2.\n","⚠️ Multiple objects in image 000674, using object 2.\n","⚠️ Multiple objects in image 000675, using object 2.\n","⚠️ Multiple objects in image 000676, using object 2.\n","⚠️ Multiple objects in image 000677, using object 2.\n","⚠️ Multiple objects in image 000678, using object 2.\n","⚠️ Multiple objects in image 000679, using object 2.\n","⚠️ Multiple objects in image 000680, using object 2.\n","⚠️ Multiple objects in image 000681, using object 2.\n","⚠️ Multiple objects in image 000682, using object 2.\n","⚠️ Multiple objects in image 000683, using object 2.\n","⚠️ Multiple objects in image 000684, using object 2.\n","⚠️ Multiple objects in image 000685, using object 2.\n","⚠️ Multiple objects in image 000686, using object 2.\n","⚠️ Multiple objects in image 000687, using object 2.\n","⚠️ Multiple objects in image 000688, using object 2.\n","⚠️ Multiple objects in image 000689, using object 2.\n","⚠️ Multiple objects in image 000690, using object 2.\n","⚠️ Multiple objects in image 000691, using object 2.\n","⚠️ Multiple objects in image 000692, using object 2.\n","⚠️ Multiple objects in image 000693, using object 2.\n","⚠️ Multiple objects in image 000694, using object 2.\n","⚠️ Multiple objects in image 000695, using object 2.\n","⚠️ Multiple objects in image 000696, using object 2.\n","⚠️ Multiple objects in image 000697, using object 2.\n","⚠️ Multiple objects in image 000698, using object 2.\n","⚠️ Multiple objects in image 000699, using object 2.\n","⚠️ Multiple objects in image 000700, using object 2.\n","⚠️ Multiple objects in image 000701, using object 2.\n","⚠️ Multiple objects in image 000702, using object 2.\n","⚠️ Multiple objects in image 000703, using object 2.\n","⚠️ Multiple objects in image 000704, using object 2.\n","⚠️ Multiple objects in image 000705, using object 2.\n","⚠️ Multiple objects in image 000706, using object 2.\n","⚠️ Multiple objects in image 000707, using object 2.\n","⚠️ Multiple objects in image 000708, using object 2.\n","⚠️ Multiple objects in image 000709, using object 2.\n","⚠️ Multiple objects in image 000710, using object 2.\n","⚠️ Multiple objects in image 000711, using object 2.\n","⚠️ Multiple objects in image 000712, using object 2.\n","⚠️ Multiple objects in image 000713, using object 2.\n","⚠️ Multiple objects in image 000714, using object 2.\n","⚠️ Multiple objects in image 000715, using object 2.\n","⚠️ Multiple objects in image 000716, using object 2.\n","⚠️ Multiple objects in image 000717, using object 2.\n","⚠️ Multiple objects in image 000718, using object 2.\n","⚠️ Multiple objects in image 000719, using object 2.\n","⚠️ Multiple objects in image 000720, using object 2.\n","⚠️ Multiple objects in image 000721, using object 2.\n","⚠️ Multiple objects in image 000722, using object 2.\n","⚠️ Multiple objects in image 000723, using object 2.\n","⚠️ Multiple objects in image 000724, using object 2.\n","⚠️ Multiple objects in image 000725, using object 2.\n","⚠️ Multiple objects in image 000726, using object 2.\n","⚠️ Multiple objects in image 000727, using object 2.\n","⚠️ Multiple objects in image 000728, using object 2.\n","⚠️ Multiple objects in image 000729, using object 2.\n","⚠️ Multiple objects in image 000730, using object 2.\n","⚠️ Multiple objects in image 000731, using object 2.\n","⚠️ Multiple objects in image 000732, using object 2.\n","⚠️ Multiple objects in image 000733, using object 2.\n","⚠️ Multiple objects in image 000734, using object 2.\n","⚠️ Multiple objects in image 000735, using object 2.\n","⚠️ Multiple objects in image 000736, using object 2.\n","⚠️ Multiple objects in image 000737, using object 2.\n","⚠️ Multiple objects in image 000738, using object 2.\n","⚠️ Multiple objects in image 000739, using object 2.\n","⚠️ Multiple objects in image 000740, using object 2.\n","⚠️ Multiple objects in image 000741, using object 2.\n","⚠️ Multiple objects in image 000742, using object 2.\n","⚠️ Multiple objects in image 000743, using object 2.\n","⚠️ Multiple objects in image 000744, using object 2.\n","⚠️ Multiple objects in image 000745, using object 2.\n","⚠️ Multiple objects in image 000746, using object 2.\n","⚠️ Multiple objects in image 000747, using object 2.\n","⚠️ Multiple objects in image 000748, using object 2.\n","⚠️ Multiple objects in image 000749, using object 2.\n","⚠️ Multiple objects in image 000750, using object 2.\n","⚠️ Multiple objects in image 000751, using object 2.\n","⚠️ Multiple objects in image 000752, using object 2.\n","⚠️ Multiple objects in image 000753, using object 2.\n","⚠️ Multiple objects in image 000754, using object 2.\n","⚠️ Multiple objects in image 000755, using object 2.\n","⚠️ Multiple objects in image 000756, using object 2.\n","⚠️ Multiple objects in image 000757, using object 2.\n","⚠️ Multiple objects in image 000758, using object 2.\n","⚠️ Multiple objects in image 000759, using object 2.\n","⚠️ Multiple objects in image 000760, using object 2.\n","⚠️ Multiple objects in image 000761, using object 2.\n","⚠️ Multiple objects in image 000762, using object 2.\n","⚠️ Multiple objects in image 000763, using object 2.\n","⚠️ Multiple objects in image 000764, using object 2.\n","⚠️ Multiple objects in image 000765, using object 2.\n","⚠️ Multiple objects in image 000766, using object 2.\n","⚠️ Multiple objects in image 000767, using object 2.\n","⚠️ Multiple objects in image 000768, using object 2.\n","⚠️ Multiple objects in image 000769, using object 2.\n","⚠️ Multiple objects in image 000770, using object 2.\n","⚠️ Multiple objects in image 000771, using object 2.\n","⚠️ Multiple objects in image 000772, using object 2.\n","⚠️ Multiple objects in image 000773, using object 2.\n","⚠️ Multiple objects in image 000774, using object 2.\n","⚠️ Multiple objects in image 000775, using object 2.\n","⚠️ Multiple objects in image 000776, using object 2.\n","⚠️ Multiple objects in image 000777, using object 2.\n","⚠️ Multiple objects in image 000778, using object 2.\n","⚠️ Multiple objects in image 000779, using object 2.\n","⚠️ Multiple objects in image 000780, using object 2.\n","⚠️ Multiple objects in image 000781, using object 2.\n","⚠️ Multiple objects in image 000782, using object 2.\n","⚠️ Multiple objects in image 000783, using object 2.\n","⚠️ Multiple objects in image 000784, using object 2.\n","⚠️ Multiple objects in image 000785, using object 2.\n","⚠️ Multiple objects in image 000786, using object 2.\n","⚠️ Multiple objects in image 000787, using object 2.\n","⚠️ Multiple objects in image 000788, using object 2.\n","⚠️ Multiple objects in image 000789, using object 2.\n","⚠️ Multiple objects in image 000790, using object 2.\n","⚠️ Multiple objects in image 000791, using object 2.\n","⚠️ Multiple objects in image 000792, using object 2.\n","⚠️ Multiple objects in image 000793, using object 2.\n","⚠️ Multiple objects in image 000794, using object 2.\n","⚠️ Multiple objects in image 000795, using object 2.\n","⚠️ Multiple objects in image 000796, using object 2.\n","⚠️ Multiple objects in image 000797, using object 2.\n","⚠️ Multiple objects in image 000798, using object 2.\n","⚠️ Multiple objects in image 000799, using object 2.\n","⚠️ Multiple objects in image 000800, using object 2.\n","⚠️ Multiple objects in image 000801, using object 2.\n","⚠️ Multiple objects in image 000802, using object 2.\n","⚠️ Multiple objects in image 000803, using object 2.\n","⚠️ Multiple objects in image 000804, using object 2.\n","⚠️ Multiple objects in image 000805, using object 2.\n","⚠️ Multiple objects in image 000806, using object 2.\n","⚠️ Multiple objects in image 000807, using object 2.\n","⚠️ Multiple objects in image 000808, using object 2.\n","⚠️ Multiple objects in image 000809, using object 2.\n","⚠️ Multiple objects in image 000810, using object 2.\n","⚠️ Multiple objects in image 000811, using object 2.\n","⚠️ Multiple objects in image 000812, using object 2.\n","⚠️ Multiple objects in image 000813, using object 2.\n","⚠️ Multiple objects in image 000814, using object 2.\n","⚠️ Multiple objects in image 000815, using object 2.\n","⚠️ Multiple objects in image 000816, using object 2.\n","⚠️ Multiple objects in image 000817, using object 2.\n","⚠️ Multiple objects in image 000818, using object 2.\n","⚠️ Multiple objects in image 000819, using object 2.\n","⚠️ Multiple objects in image 000820, using object 2.\n","⚠️ Multiple objects in image 000821, using object 2.\n","⚠️ Multiple objects in image 000822, using object 2.\n","⚠️ Multiple objects in image 000823, using object 2.\n","⚠️ Multiple objects in image 000824, using object 2.\n","⚠️ Multiple objects in image 000825, using object 2.\n","⚠️ Multiple objects in image 000826, using object 2.\n","⚠️ Multiple objects in image 000827, using object 2.\n","⚠️ Multiple objects in image 000828, using object 2.\n","⚠️ Multiple objects in image 000829, using object 2.\n","⚠️ Multiple objects in image 000830, using object 2.\n","⚠️ Multiple objects in image 000831, using object 2.\n","⚠️ Multiple objects in image 000832, using object 2.\n","⚠️ Multiple objects in image 000833, using object 2.\n","⚠️ Multiple objects in image 000834, using object 2.\n","⚠️ Multiple objects in image 000835, using object 2.\n","⚠️ Multiple objects in image 000836, using object 2.\n","⚠️ Multiple objects in image 000837, using object 2.\n","⚠️ Multiple objects in image 000838, using object 2.\n","⚠️ Multiple objects in image 000839, using object 2.\n","⚠️ Multiple objects in image 000840, using object 2.\n","⚠️ Multiple objects in image 000841, using object 2.\n","⚠️ Multiple objects in image 000842, using object 2.\n","⚠️ Multiple objects in image 000843, using object 2.\n","⚠️ Multiple objects in image 000844, using object 2.\n","⚠️ Multiple objects in image 000845, using object 2.\n","⚠️ Multiple objects in image 000846, using object 2.\n","⚠️ Multiple objects in image 000847, using object 2.\n","⚠️ Multiple objects in image 000848, using object 2.\n","⚠️ Multiple objects in image 000849, using object 2.\n","⚠️ Multiple objects in image 000850, using object 2.\n","⚠️ Multiple objects in image 000851, using object 2.\n","⚠️ Multiple objects in image 000852, using object 2.\n","⚠️ Multiple objects in image 000853, using object 2.\n","⚠️ Multiple objects in image 000854, using object 2.\n","⚠️ Multiple objects in image 000855, using object 2.\n","⚠️ Multiple objects in image 000856, using object 2.\n","⚠️ Multiple objects in image 000857, using object 2.\n","⚠️ Multiple objects in image 000858, using object 2.\n","⚠️ Multiple objects in image 000859, using object 2.\n","⚠️ Multiple objects in image 000860, using object 2.\n","⚠️ Multiple objects in image 000861, using object 2.\n","⚠️ Multiple objects in image 000862, using object 2.\n","⚠️ Multiple objects in image 000863, using object 2.\n","⚠️ Multiple objects in image 000864, using object 2.\n","⚠️ Multiple objects in image 000865, using object 2.\n","⚠️ Multiple objects in image 000866, using object 2.\n","⚠️ Multiple objects in image 000867, using object 2.\n","⚠️ Multiple objects in image 000868, using object 2.\n","⚠️ Multiple objects in image 000869, using object 2.\n","⚠️ Multiple objects in image 000870, using object 2.\n","⚠️ Multiple objects in image 000871, using object 2.\n","⚠️ Multiple objects in image 000872, using object 2.\n","⚠️ Multiple objects in image 000873, using object 2.\n","⚠️ Multiple objects in image 000874, using object 2.\n","⚠️ Multiple objects in image 000875, using object 2.\n","⚠️ Multiple objects in image 000876, using object 2.\n","⚠️ Multiple objects in image 000877, using object 2.\n","⚠️ Multiple objects in image 000878, using object 2.\n","⚠️ Multiple objects in image 000879, using object 2.\n","⚠️ Multiple objects in image 000880, using object 2.\n","⚠️ Multiple objects in image 000881, using object 2.\n","⚠️ Multiple objects in image 000882, using object 2.\n","⚠️ Multiple objects in image 000883, using object 2.\n","⚠️ Multiple objects in image 000884, using object 2.\n","⚠️ Multiple objects in image 000885, using object 2.\n","⚠️ Multiple objects in image 000886, using object 2.\n","⚠️ Multiple objects in image 000887, using object 2.\n","⚠️ Multiple objects in image 000888, using object 2.\n","⚠️ Multiple objects in image 000889, using object 2.\n","⚠️ Multiple objects in image 000890, using object 2.\n","⚠️ Multiple objects in image 000891, using object 2.\n","⚠️ Multiple objects in image 000892, using object 2.\n","⚠️ Multiple objects in image 000893, using object 2.\n","⚠️ Multiple objects in image 000894, using object 2.\n","⚠️ Multiple objects in image 000895, using object 2.\n","⚠️ Multiple objects in image 000896, using object 2.\n","⚠️ Multiple objects in image 000897, using object 2.\n","⚠️ Multiple objects in image 000898, using object 2.\n","⚠️ Multiple objects in image 000899, using object 2.\n","⚠️ Multiple objects in image 000900, using object 2.\n","⚠️ Multiple objects in image 000901, using object 2.\n","⚠️ Multiple objects in image 000902, using object 2.\n","⚠️ Multiple objects in image 000903, using object 2.\n","⚠️ Multiple objects in image 000904, using object 2.\n","⚠️ Multiple objects in image 000905, using object 2.\n","⚠️ Multiple objects in image 000906, using object 2.\n","⚠️ Multiple objects in image 000907, using object 2.\n","⚠️ Multiple objects in image 000908, using object 2.\n","⚠️ Multiple objects in image 000909, using object 2.\n","⚠️ Multiple objects in image 000910, using object 2.\n","⚠️ Multiple objects in image 000911, using object 2.\n","⚠️ Multiple objects in image 000912, using object 2.\n","⚠️ Multiple objects in image 000913, using object 2.\n","⚠️ Multiple objects in image 000914, using object 2.\n","⚠️ Multiple objects in image 000915, using object 2.\n","⚠️ Multiple objects in image 000916, using object 2.\n","⚠️ Multiple objects in image 000917, using object 2.\n","⚠️ Multiple objects in image 000918, using object 2.\n","⚠️ Multiple objects in image 000919, using object 2.\n","⚠️ Multiple objects in image 000920, using object 2.\n","⚠️ Multiple objects in image 000921, using object 2.\n","⚠️ Multiple objects in image 000922, using object 2.\n","⚠️ Multiple objects in image 000923, using object 2.\n","⚠️ Multiple objects in image 000924, using object 2.\n","⚠️ Multiple objects in image 000925, using object 2.\n","⚠️ Multiple objects in image 000926, using object 2.\n","⚠️ Multiple objects in image 000927, using object 2.\n","⚠️ Multiple objects in image 000928, using object 2.\n","⚠️ Multiple objects in image 000929, using object 2.\n","⚠️ Multiple objects in image 000930, using object 2.\n","⚠️ Multiple objects in image 000931, using object 2.\n","⚠️ Multiple objects in image 000932, using object 2.\n","⚠️ Multiple objects in image 000933, using object 2.\n","⚠️ Multiple objects in image 000934, using object 2.\n","⚠️ Multiple objects in image 000935, using object 2.\n","⚠️ Multiple objects in image 000936, using object 2.\n","⚠️ Multiple objects in image 000937, using object 2.\n","⚠️ Multiple objects in image 000938, using object 2.\n","⚠️ Multiple objects in image 000939, using object 2.\n","⚠️ Multiple objects in image 000940, using object 2.\n","⚠️ Multiple objects in image 000941, using object 2.\n","⚠️ Multiple objects in image 000942, using object 2.\n","⚠️ Multiple objects in image 000943, using object 2.\n","⚠️ Multiple objects in image 000944, using object 2.\n","⚠️ Multiple objects in image 000945, using object 2.\n","⚠️ Multiple objects in image 000946, using object 2.\n","⚠️ Multiple objects in image 000947, using object 2.\n","⚠️ Multiple objects in image 000948, using object 2.\n","⚠️ Multiple objects in image 000949, using object 2.\n","⚠️ Multiple objects in image 000950, using object 2.\n","⚠️ Multiple objects in image 000951, using object 2.\n","⚠️ Multiple objects in image 000952, using object 2.\n","⚠️ Multiple objects in image 000953, using object 2.\n","⚠️ Multiple objects in image 000954, using object 2.\n","⚠️ Multiple objects in image 000955, using object 2.\n","⚠️ Multiple objects in image 000956, using object 2.\n","⚠️ Multiple objects in image 000957, using object 2.\n","⚠️ Multiple objects in image 000958, using object 2.\n","⚠️ Multiple objects in image 000959, using object 2.\n","⚠️ Multiple objects in image 000960, using object 2.\n","⚠️ Multiple objects in image 000961, using object 2.\n","⚠️ Multiple objects in image 000962, using object 2.\n","⚠️ Multiple objects in image 000963, using object 2.\n","⚠️ Multiple objects in image 000964, using object 2.\n","⚠️ Multiple objects in image 000965, using object 2.\n","⚠️ Multiple objects in image 000966, using object 2.\n","⚠️ Multiple objects in image 000967, using object 2.\n","⚠️ Multiple objects in image 000968, using object 2.\n","⚠️ Multiple objects in image 000969, using object 2.\n","⚠️ Multiple objects in image 000970, using object 2.\n","⚠️ Multiple objects in image 000971, using object 2.\n","⚠️ Multiple objects in image 000972, using object 2.\n","⚠️ Multiple objects in image 000973, using object 2.\n","⚠️ Multiple objects in image 000974, using object 2.\n","⚠️ Multiple objects in image 000975, using object 2.\n","⚠️ Multiple objects in image 000976, using object 2.\n","⚠️ Multiple objects in image 000977, using object 2.\n","⚠️ Multiple objects in image 000978, using object 2.\n","⚠️ Multiple objects in image 000979, using object 2.\n","⚠️ Multiple objects in image 000980, using object 2.\n","⚠️ Multiple objects in image 000981, using object 2.\n","⚠️ Multiple objects in image 000982, using object 2.\n","⚠️ Multiple objects in image 000983, using object 2.\n","⚠️ Multiple objects in image 000984, using object 2.\n","⚠️ Multiple objects in image 000985, using object 2.\n","⚠️ Multiple objects in image 000986, using object 2.\n","⚠️ Multiple objects in image 000987, using object 2.\n","⚠️ Multiple objects in image 000988, using object 2.\n","⚠️ Multiple objects in image 000989, using object 2.\n","⚠️ Multiple objects in image 000990, using object 2.\n","⚠️ Multiple objects in image 000991, using object 2.\n","⚠️ Multiple objects in image 000992, using object 2.\n","⚠️ Multiple objects in image 000993, using object 2.\n","⚠️ Multiple objects in image 000994, using object 2.\n","⚠️ Multiple objects in image 000995, using object 2.\n","⚠️ Multiple objects in image 000996, using object 2.\n","⚠️ Multiple objects in image 000997, using object 2.\n","⚠️ Multiple objects in image 000998, using object 2.\n","⚠️ Multiple objects in image 000999, using object 2.\n","⚠️ Multiple objects in image 001000, using object 2.\n","⚠️ Multiple objects in image 001001, using object 2.\n","⚠️ Multiple objects in image 001002, using object 2.\n","⚠️ Multiple objects in image 001003, using object 2.\n","⚠️ Multiple objects in image 001004, using object 2.\n","⚠️ Multiple objects in image 001005, using object 2.\n","⚠️ Multiple objects in image 001006, using object 2.\n","⚠️ Multiple objects in image 001007, using object 2.\n","⚠️ Multiple objects in image 001008, using object 2.\n","⚠️ Multiple objects in image 001009, using object 2.\n","⚠️ Multiple objects in image 001010, using object 2.\n","⚠️ Multiple objects in image 001011, using object 2.\n","⚠️ Multiple objects in image 001012, using object 2.\n","⚠️ Multiple objects in image 001013, using object 2.\n","⚠️ Multiple objects in image 001014, using object 2.\n","⚠️ Multiple objects in image 001015, using object 2.\n","⚠️ Multiple objects in image 001016, using object 2.\n","⚠️ Multiple objects in image 001017, using object 2.\n","⚠️ Multiple objects in image 001018, using object 2.\n","⚠️ Multiple objects in image 001019, using object 2.\n","⚠️ Multiple objects in image 001020, using object 2.\n","⚠️ Multiple objects in image 001021, using object 2.\n","⚠️ Multiple objects in image 001022, using object 2.\n","⚠️ Multiple objects in image 001023, using object 2.\n","⚠️ Multiple objects in image 001024, using object 2.\n","⚠️ Multiple objects in image 001025, using object 2.\n","⚠️ Multiple objects in image 001026, using object 2.\n","⚠️ Multiple objects in image 001027, using object 2.\n","⚠️ Multiple objects in image 001028, using object 2.\n","⚠️ Multiple objects in image 001029, using object 2.\n","⚠️ Multiple objects in image 001030, using object 2.\n","⚠️ Multiple objects in image 001031, using object 2.\n","⚠️ Multiple objects in image 001032, using object 2.\n","⚠️ Multiple objects in image 001033, using object 2.\n","⚠️ Multiple objects in image 001034, using object 2.\n","⚠️ Multiple objects in image 001035, using object 2.\n","⚠️ Multiple objects in image 001036, using object 2.\n","⚠️ Multiple objects in image 001037, using object 2.\n","⚠️ Multiple objects in image 001038, using object 2.\n","⚠️ Multiple objects in image 001039, using object 2.\n","⚠️ Multiple objects in image 001040, using object 2.\n","⚠️ Multiple objects in image 001041, using object 2.\n","⚠️ Multiple objects in image 001042, using object 2.\n","⚠️ Multiple objects in image 001043, using object 2.\n","⚠️ Multiple objects in image 001044, using object 2.\n","⚠️ Multiple objects in image 001045, using object 2.\n","⚠️ Multiple objects in image 001046, using object 2.\n","⚠️ Multiple objects in image 001047, using object 2.\n","⚠️ Multiple objects in image 001048, using object 2.\n","⚠️ Multiple objects in image 001049, using object 2.\n","⚠️ Multiple objects in image 001050, using object 2.\n","⚠️ Multiple objects in image 001051, using object 2.\n","⚠️ Multiple objects in image 001052, using object 2.\n","⚠️ Multiple objects in image 001053, using object 2.\n","⚠️ Multiple objects in image 001054, using object 2.\n","⚠️ Multiple objects in image 001055, using object 2.\n","⚠️ Multiple objects in image 001056, using object 2.\n","⚠️ Multiple objects in image 001057, using object 2.\n","⚠️ Multiple objects in image 001058, using object 2.\n","⚠️ Multiple objects in image 001059, using object 2.\n","⚠️ Multiple objects in image 001060, using object 2.\n","⚠️ Multiple objects in image 001061, using object 2.\n","⚠️ Multiple objects in image 001062, using object 2.\n","⚠️ Multiple objects in image 001063, using object 2.\n","⚠️ Multiple objects in image 001064, using object 2.\n","⚠️ Multiple objects in image 001065, using object 2.\n","⚠️ Multiple objects in image 001066, using object 2.\n","⚠️ Multiple objects in image 001067, using object 2.\n","⚠️ Multiple objects in image 001068, using object 2.\n","⚠️ Multiple objects in image 001069, using object 2.\n","⚠️ Multiple objects in image 001070, using object 2.\n","⚠️ Multiple objects in image 001071, using object 2.\n","⚠️ Multiple objects in image 001072, using object 2.\n","⚠️ Multiple objects in image 001073, using object 2.\n","⚠️ Multiple objects in image 001074, using object 2.\n","⚠️ Multiple objects in image 001075, using object 2.\n","⚠️ Multiple objects in image 001076, using object 2.\n","⚠️ Multiple objects in image 001077, using object 2.\n","⚠️ Multiple objects in image 001078, using object 2.\n","⚠️ Multiple objects in image 001079, using object 2.\n","⚠️ Multiple objects in image 001080, using object 2.\n","⚠️ Multiple objects in image 001081, using object 2.\n","⚠️ Multiple objects in image 001082, using object 2.\n","⚠️ Multiple objects in image 001083, using object 2.\n","⚠️ Multiple objects in image 001084, using object 2.\n","⚠️ Multiple objects in image 001085, using object 2.\n","⚠️ Multiple objects in image 001086, using object 2.\n","⚠️ Multiple objects in image 001087, using object 2.\n","⚠️ Multiple objects in image 001088, using object 2.\n","⚠️ Multiple objects in image 001089, using object 2.\n","⚠️ Multiple objects in image 001090, using object 2.\n","⚠️ Multiple objects in image 001091, using object 2.\n","⚠️ Multiple objects in image 001092, using object 2.\n","⚠️ Multiple objects in image 001093, using object 2.\n","⚠️ Multiple objects in image 001094, using object 2.\n","⚠️ Multiple objects in image 001095, using object 2.\n","⚠️ Multiple objects in image 001096, using object 2.\n","⚠️ Multiple objects in image 001097, using object 2.\n","⚠️ Multiple objects in image 001098, using object 2.\n","⚠️ Multiple objects in image 001099, using object 2.\n","⚠️ Multiple objects in image 001100, using object 2.\n","⚠️ Multiple objects in image 001101, using object 2.\n","⚠️ Multiple objects in image 001102, using object 2.\n","⚠️ Multiple objects in image 001103, using object 2.\n","⚠️ Multiple objects in image 001104, using object 2.\n","⚠️ Multiple objects in image 001105, using object 2.\n","⚠️ Multiple objects in image 001106, using object 2.\n","⚠️ Multiple objects in image 001107, using object 2.\n","⚠️ Multiple objects in image 001108, using object 2.\n","⚠️ Multiple objects in image 001109, using object 2.\n","⚠️ Multiple objects in image 001110, using object 2.\n","⚠️ Multiple objects in image 001111, using object 2.\n","⚠️ Multiple objects in image 001112, using object 2.\n","⚠️ Multiple objects in image 001113, using object 2.\n","⚠️ Multiple objects in image 001114, using object 2.\n","⚠️ Multiple objects in image 001115, using object 2.\n","⚠️ Multiple objects in image 001116, using object 2.\n","⚠️ Multiple objects in image 001117, using object 2.\n","⚠️ Multiple objects in image 001118, using object 2.\n","⚠️ Multiple objects in image 001119, using object 2.\n","⚠️ Multiple objects in image 001120, using object 2.\n","⚠️ Multiple objects in image 001121, using object 2.\n","⚠️ Multiple objects in image 001122, using object 2.\n","⚠️ Multiple objects in image 001123, using object 2.\n","⚠️ Multiple objects in image 001124, using object 2.\n","⚠️ Multiple objects in image 001125, using object 2.\n","⚠️ Multiple objects in image 001126, using object 2.\n","⚠️ Multiple objects in image 001127, using object 2.\n","⚠️ Multiple objects in image 001128, using object 2.\n","⚠️ Multiple objects in image 001129, using object 2.\n","⚠️ Multiple objects in image 001130, using object 2.\n","⚠️ Multiple objects in image 001131, using object 2.\n","⚠️ Multiple objects in image 001132, using object 2.\n","⚠️ Multiple objects in image 001133, using object 2.\n","⚠️ Multiple objects in image 001134, using object 2.\n","⚠️ Multiple objects in image 001135, using object 2.\n","⚠️ Multiple objects in image 001136, using object 2.\n","⚠️ Multiple objects in image 001137, using object 2.\n","⚠️ Multiple objects in image 001138, using object 2.\n","⚠️ Multiple objects in image 001139, using object 2.\n","⚠️ Multiple objects in image 001140, using object 2.\n","⚠️ Multiple objects in image 001141, using object 2.\n","⚠️ Multiple objects in image 001142, using object 2.\n","⚠️ Multiple objects in image 001143, using object 2.\n","⚠️ Multiple objects in image 001144, using object 2.\n","⚠️ Multiple objects in image 001145, using object 2.\n","⚠️ Multiple objects in image 001146, using object 2.\n","⚠️ Multiple objects in image 001147, using object 2.\n","⚠️ Multiple objects in image 001148, using object 2.\n","⚠️ Multiple objects in image 001149, using object 2.\n","⚠️ Multiple objects in image 001150, using object 2.\n","⚠️ Multiple objects in image 001151, using object 2.\n","⚠️ Multiple objects in image 001152, using object 2.\n","⚠️ Multiple objects in image 001153, using object 2.\n","⚠️ Multiple objects in image 001154, using object 2.\n","⚠️ Multiple objects in image 001155, using object 2.\n","⚠️ Multiple objects in image 001156, using object 2.\n","⚠️ Multiple objects in image 001157, using object 2.\n","⚠️ Multiple objects in image 001158, using object 2.\n","⚠️ Multiple objects in image 001159, using object 2.\n","⚠️ Multiple objects in image 001160, using object 2.\n","⚠️ Multiple objects in image 001161, using object 2.\n","⚠️ Multiple objects in image 001162, using object 2.\n","⚠️ Multiple objects in image 001163, using object 2.\n","⚠️ Multiple objects in image 001164, using object 2.\n","⚠️ Multiple objects in image 001165, using object 2.\n","⚠️ Multiple objects in image 001166, using object 2.\n","⚠️ Multiple objects in image 001167, using object 2.\n","⚠️ Multiple objects in image 001168, using object 2.\n","⚠️ Multiple objects in image 001169, using object 2.\n","⚠️ Multiple objects in image 001170, using object 2.\n","⚠️ Multiple objects in image 001171, using object 2.\n","⚠️ Multiple objects in image 001172, using object 2.\n","⚠️ Multiple objects in image 001173, using object 2.\n","⚠️ Multiple objects in image 001174, using object 2.\n","⚠️ Multiple objects in image 001175, using object 2.\n","⚠️ Multiple objects in image 001176, using object 2.\n","⚠️ Multiple objects in image 001177, using object 2.\n","⚠️ Multiple objects in image 001178, using object 2.\n","⚠️ Multiple objects in image 001179, using object 2.\n","⚠️ Multiple objects in image 001180, using object 2.\n","⚠️ Multiple objects in image 001181, using object 2.\n","⚠️ Multiple objects in image 001182, using object 2.\n","⚠️ Multiple objects in image 001183, using object 2.\n","⚠️ Multiple objects in image 001184, using object 2.\n","⚠️ Multiple objects in image 001185, using object 2.\n","⚠️ Multiple objects in image 001186, using object 2.\n","⚠️ Multiple objects in image 001187, using object 2.\n","⚠️ Multiple objects in image 001188, using object 2.\n","⚠️ Multiple objects in image 001189, using object 2.\n","⚠️ Multiple objects in image 001190, using object 2.\n","⚠️ Multiple objects in image 001191, using object 2.\n","⚠️ Multiple objects in image 001192, using object 2.\n","⚠️ Multiple objects in image 001193, using object 2.\n","⚠️ Multiple objects in image 001194, using object 2.\n","⚠️ Multiple objects in image 001195, using object 2.\n","⚠️ Multiple objects in image 001196, using object 2.\n","⚠️ Multiple objects in image 001197, using object 2.\n","⚠️ Multiple objects in image 001198, using object 2.\n","⚠️ Multiple objects in image 001199, using object 2.\n","⚠️ Multiple objects in image 001200, using object 2.\n","⚠️ Multiple objects in image 001201, using object 2.\n","⚠️ Multiple objects in image 001202, using object 2.\n","⚠️ Multiple objects in image 001203, using object 2.\n","⚠️ Multiple objects in image 001204, using object 2.\n","⚠️ Multiple objects in image 001205, using object 2.\n","⚠️ Multiple objects in image 001206, using object 2.\n","⚠️ Multiple objects in image 001207, using object 2.\n","⚠️ Multiple objects in image 001208, using object 2.\n","⚠️ Multiple objects in image 001209, using object 2.\n","⚠️ Multiple objects in image 001210, using object 2.\n","⚠️ Multiple objects in image 001211, using object 2.\n","⚠️ Multiple objects in image 001212, using object 2.\n","⚠️ Multiple objects in image 001213, using object 2.\n","\n","📂 Processing class 04...\n"]},{"output_type":"stream","name":"stderr","text":["Class 04: 100%|██████████| 1201/1201 [00:00<00:00, 8928.21image/s] \n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 05...\n"]},{"output_type":"stream","name":"stderr","text":["Class 05: 100%|██████████| 1196/1196 [00:00<00:00, 10348.34image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 06...\n"]},{"output_type":"stream","name":"stderr","text":["Class 06: 100%|██████████| 1179/1179 [00:00<00:00, 10219.35image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 08...\n"]},{"output_type":"stream","name":"stderr","text":["Class 08: 100%|██████████| 1188/1188 [00:00<00:00, 10287.78image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 09...\n"]},{"output_type":"stream","name":"stderr","text":["Class 09: 100%|██████████| 1254/1254 [00:00<00:00, 10729.46image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 10...\n"]},{"output_type":"stream","name":"stderr","text":["Class 10: 100%|██████████| 1253/1253 [00:00<00:00, 10630.22image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 11...\n"]},{"output_type":"stream","name":"stderr","text":["Class 11: 100%|██████████| 1220/1220 [00:00<00:00, 10767.66image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 12...\n"]},{"output_type":"stream","name":"stderr","text":["Class 12: 100%|██████████| 1237/1237 [00:00<00:00, 10541.11image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 13...\n"]},{"output_type":"stream","name":"stderr","text":["Class 13: 100%|██████████| 1152/1152 [00:00<00:00, 10634.41image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 14...\n"]},{"output_type":"stream","name":"stderr","text":["Class 14: 100%|██████████| 1227/1227 [00:00<00:00, 10233.28image/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","📂 Processing class 15...\n"]},{"output_type":"stream","name":"stderr","text":["Class 15: 100%|██████████| 1243/1243 [00:00<00:00, 10698.22image/s]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ All poses extracted and saved from gt.yml files.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import numpy as np\n","import yaml\n","from tqdm import tqdm\n","\n","# Base dataset path\n","base_root = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","class_ids = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n","\n","for class_id in class_ids:\n","    print(f\"\\n📂 Processing class {class_id}...\")\n","    class_path = os.path.join(base_root, class_id)\n","    gt_path    = os.path.join(class_path, \"gt.yml\")\n","    pose_dir   = os.path.join(class_path, \"pose\")\n","\n","    if not os.path.exists(gt_path):\n","        print(f\"⚠️ gt.yml not found for class {class_id} — skipped.\")\n","        continue\n","    os.makedirs(pose_dir, exist_ok=True)\n","\n","    with open(gt_path, 'r') as f:\n","        gt_data = yaml.safe_load(f)\n","\n","    for img_id, pose_list in tqdm(gt_data.items(), desc=f\"Class {class_id}\", unit=\"image\"):\n","        idx = int(img_id)\n","\n","        # If multiple objects, pick the second one (index 1)\n","        if len(pose_list) > 1:\n","            print(f\"⚠️ Multiple objects in image {idx:06d}, using object 2.\")\n","            selected_pose = pose_list[1]\n","        else:\n","            selected_pose = pose_list[0]\n","\n","        # Extract R, t and build 3x4 matrix\n","        R = np.array(selected_pose['cam_R_m2c'], dtype=np.float32).reshape(3, 3)\n","        t = np.array(selected_pose['cam_t_m2c'], dtype=np.float32).reshape(3, 1)  # mm\n","        RT = np.hstack([R, t])\n","\n","        save_path = os.path.join(pose_dir, f\"pose{idx:06d}.npy\")\n","        np.save(save_path, RT)\n","\n","print(\"\\n✅ All poses extracted and saved from gt.yml files.\")"]},{"cell_type":"markdown","metadata":{"id":"yim04eDar2j1"},"source":["### 4 - Modify RGB, Mask, Depth Files Format From 4 Digits to 6 Digits - Radius Map Needs Files in 6-Digit Format"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":958,"status":"ok","timestamp":1748443512521,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"yV8C9kelWjbe","outputId":"b0293c7a-0991-4969-dd31-2d7bccdfb173"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","====================🔧 Renaming class 01=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1236 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 02=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1214 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1215 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1214 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 04=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1201 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 05=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1196 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 06=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1179 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 08=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1188 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 09=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1254 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 10=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1253 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 11=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1220 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 12=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1237 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 13=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1152 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 14=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":["                                        "]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1227 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["============================================================\n","\n","====================🔧 Renaming class 15=====================\n","Folder     │  Count\n","------------------------------------------------------------\n","🌈 rgb      │   1243 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🛡️  mask     │   1225 files\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["🌊  depth    │   1243 files\n"]},{"output_type":"stream","name":"stderr","text":["                                        "]},{"output_type":"stream","name":"stdout","text":["============================================================\n","            \n","🏁  All files renamed successfully.             \n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["from tqdm import tqdm\n","import os\n","\n","def rename_files_to_6_digit(base_dir, classes):\n","    line_width = 60\n","    for cls in classes:\n","        header = f\"🔧 Renaming class {cls}\"\n","        print(\"\\n\" + header.center(line_width, \"=\"))\n","        print(f\"{'Folder':<10} │ {'Count':>6}\")\n","        print(\"-\" * line_width)\n","\n","        for folder_name in ['rgb', 'mask', 'depth']:\n","            emoji = {\n","                'rgb':   '🌈',\n","                'mask':  '🛡️ ',\n","                'depth': '🌊 '\n","            }[folder_name]\n","\n","            folder_path = os.path.join(base_dir, cls, folder_name)\n","            if not os.path.isdir(folder_path):\n","                print(f\"{emoji} {folder_name:<8} │  {'—':>6}  (not found)\")\n","                continue\n","\n","            file_list = [\n","                f for f in os.listdir(folder_path)\n","                if os.path.splitext(f)[0].isdigit()\n","            ]\n","            print(f\"{emoji} {folder_name:<8} │ {len(file_list):>6} files\")\n","            for fname in tqdm(\n","                file_list,\n","                desc=f\"{cls}/{folder_name}\",\n","                unit=\"file\",\n","                leave=False,\n","                ncols=40\n","            ):\n","                name, ext = os.path.splitext(fname)\n","                idx = int(name)\n","                new_name = f\"{idx:06d}{ext}\"\n","                os.rename(\n","                    os.path.join(folder_path, fname),\n","                    os.path.join(folder_path, new_name)\n","                )\n","\n","        print(\"=\" * line_width)\n","    print(\"\\n🏁  All files renamed successfully.\".center(line_width))\n","\n","# === Set base path and class list ===\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","classes  = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","rename_files_to_6_digit(base_dir, classes)\n"]},{"cell_type":"markdown","metadata":{"id":"WxdBZpOTodXW"},"source":["###5 -  Convert Depth files from .png to .dpt - Radius Map Needs Depth Files With .dpt Format"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66771,"status":"ok","timestamp":1748443585725,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"ZPZWinGpongz","outputId":"4c16dfb7-cf1c-4cbc-b278-a76b98aff2c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 01...\n"]},{"output_type":"stream","name":"stderr","text":["Class 01: 100%|██████████| 1236/1236 [00:04<00:00, 290.65file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 02...\n"]},{"output_type":"stream","name":"stderr","text":["Class 02: 100%|██████████| 1214/1214 [00:06<00:00, 201.98file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 04...\n"]},{"output_type":"stream","name":"stderr","text":["Class 04: 100%|██████████| 1201/1201 [00:04<00:00, 286.59file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 05...\n"]},{"output_type":"stream","name":"stderr","text":["Class 05: 100%|██████████| 1196/1196 [00:05<00:00, 216.34file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 06...\n"]},{"output_type":"stream","name":"stderr","text":["Class 06: 100%|██████████| 1179/1179 [00:06<00:00, 173.27file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 08...\n"]},{"output_type":"stream","name":"stderr","text":["Class 08: 100%|██████████| 1188/1188 [00:05<00:00, 217.87file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 09...\n"]},{"output_type":"stream","name":"stderr","text":["Class 09: 100%|██████████| 1254/1254 [00:04<00:00, 285.78file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 10...\n"]},{"output_type":"stream","name":"stderr","text":["Class 10: 100%|██████████| 1253/1253 [00:04<00:00, 285.55file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 11...\n"]},{"output_type":"stream","name":"stderr","text":["Class 11: 100%|██████████| 1220/1220 [00:05<00:00, 209.22file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 12...\n"]},{"output_type":"stream","name":"stderr","text":["Class 12: 100%|██████████| 1237/1237 [00:05<00:00, 213.25file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 13...\n"]},{"output_type":"stream","name":"stderr","text":["Class 13: 100%|██████████| 1152/1152 [00:04<00:00, 282.55file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 14...\n"]},{"output_type":"stream","name":"stderr","text":["Class 14: 100%|██████████| 1227/1227 [00:04<00:00, 277.63file/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔍 Processing class 15...\n"]},{"output_type":"stream","name":"stderr","text":["Class 15: 100%|██████████| 1243/1243 [00:05<00:00, 221.18file/s]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ All PNG files have been converted to .dpt format.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import numpy as np\n","from PIL import Image\n","import struct\n","\n","def save_dpt_file(png_path, dpt_path, scale=1.0):\n","    \"\"\"\n","    Converts a depth .png file to a .dpt file with proper binary format:\n","    [uint32 height][uint32 width][uint16 depth values]\n","    Then deletes the original .png file.\n","    \"\"\"\n","    depth_img = np.array(Image.open(png_path)).astype(np.float32)\n","    depth_img = (depth_img * scale).astype(np.uint16)\n","\n","    h, w = depth_img.shape\n","\n","    with open(dpt_path, 'wb') as f:\n","        f.write(struct.pack('I', h))\n","        f.write(struct.pack('I', w))\n","        f.write(depth_img.tobytes())\n","\n","    # Delete the original .png file after successful conversion\n","    os.remove(png_path)\n","\n","def convert_all_png_to_dpt_in_classes(base_dir, classes, scale=1.0):\n","    \"\"\"\n","    Processes all PNG files in the 'depth' folder of each class and creates corresponding .dpt files.\n","    \"\"\"\n","    for cls in classes:\n","        depth_dir = os.path.join(base_dir, cls, \"depth\")\n","        if not os.path.exists(depth_dir):\n","            print(f\"⚠️ Skipping class {cls}: no depth folder found.\")\n","            continue\n","\n","        print(f\"\\n🔍 Processing class {cls}...\")\n","\n","        png_files = sorted([f for f in os.listdir(depth_dir) if f.endswith(\".png\")])\n","        for fname in tqdm(png_files, desc=f\"Class {cls}\", unit=\"file\"):\n","            png_path = os.path.join(depth_dir, fname)\n","            dpt_path = os.path.join(depth_dir, os.path.splitext(fname)[0] + \".dpt\")\n","            save_dpt_file(png_path, dpt_path, scale)\n","\n","    print(\"\\n✅ All PNG files have been converted to .dpt format.\")\n","\n","# === Base dataset path and class list ===\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n","\n","# PNG values are already in mm → no scaling needed\n","scale = 1.0\n","\n","convert_all_png_to_dpt_in_classes(base_dir, classes, scale)\n"]},{"cell_type":"markdown","metadata":{"id":"za-C--7N2egf"},"source":["### 3 - Generate Radius Map\n","\n","We compute the mesh and keypoints to:\n","\n","* Mesh, accurately represent an object’s 3D shape.\n","* Reduce complexity by sampling a few representative points.\n","* Enable fast registration and matching across views.\n","* Support precise pose estimation via 3D–2D point correspondences.\n","* Improve efficiency in real-time or learning-based applications.\n"]},{"cell_type":"markdown","metadata":{"id":"9ppcG2yKdz_s"},"source":["### **Generate Radius Map**"]},{"cell_type":"code","source":["# If running in Colab, install these once:\n","# !pip install --quiet open3d numba tqdm\n","\n","import numpy as np\n","from PIL import Image\n","import open3d as o3d\n","import os\n","from numba import jit, prange\n","from tqdm import tqdm\n","\n","# List of Linemod classes to process\n","linemod_cls_names     = ['01']\n","\n","# Camera intrinsics\n","linemod_K = np.array([\n","    [572.4114,   0.0,     325.2611],\n","    [  0.0,     573.57043, 242.04899],\n","    [  0.0,       0.0,       1.0   ]\n","], dtype=np.float32)\n","\n","# Paths (adjust if needed)\n","linemod_path          = \"/content/dataset/linemod/Linemod_preprocessed/data/\"\n","original_linemod_path = linemod_path\n","\n","def project(xyz, K, RT):\n","    \"\"\"Project 3D points into the image plane.\"\"\"\n","    xyz_cam = xyz @ RT[:, :3].T + RT[:, 3:].T\n","    proj    = xyz_cam @ K.T\n","    xy      = proj[:, :2] / proj[:, 2:3]\n","    return xy, xyz_cam\n","\n","def rgbd_to_point_cloud(K, depth):\n","    \"\"\"Convert a depth map to a point cloud.\"\"\"\n","    vs, us = depth.nonzero()\n","    zs     = depth[vs, us]\n","    xs     = (us - K[0,2]) * zs / K[0,0]\n","    ys     = (vs - K[1,2]) * zs / K[1,1]\n","    pts    = np.stack((xs, ys, zs), axis=1)\n","    return pts, vs, us\n","\n","@jit(nopython=True, parallel=True)\n","def fast_for_map(vs, us, pts, dist_list, radius_map):\n","    \"\"\"Fill radius_map at (v,u) with dist_list values.\"\"\"\n","    for i in prange(len(us)):\n","        radius_map[vs[i], us[i]] = dist_list[i]\n","    return radius_map\n","\n","def read_depth(path):\n","    \"\"\"Read .dpt or fallback PNG depth.\"\"\"\n","    if path.endswith(\".dpt\"):\n","        with open(path, \"rb\") as f:\n","            h, w  = np.fromfile(f, dtype=np.uint32, count=2)\n","            data  = np.fromfile(f, dtype=np.uint16, count=h*w)\n","        return data.reshape((h, w)).astype(np.float32)\n","    else:\n","        return np.asarray(Image.open(path)).astype(np.float32)\n","\n","# Track last folder per class\n","last_folder = {}\n","\n","for cls in linemod_cls_names:\n","    depth_dir = os.path.join(linemod_path, cls, \"depth\")\n","    if not os.path.isdir(depth_dir):\n","        continue\n","    dpt_files = [f for f in os.listdir(depth_dir) if f.endswith(\".dpt\")]\n","    if not dpt_files:\n","        continue\n","    keypts = np.load(os.path.join(linemod_path, cls, \"Outside9.npy\"))\n","\n","    # Print header\n","    print(\"\\n\" + f\"📂 Class {cls}\".center(80, \"─\"))\n","\n","    # One progress bar for this entire class\n","    total_steps = len(keypts) * len(dpt_files)\n","    pbar = tqdm(\n","        total=total_steps,\n","        ncols=80,\n","        bar_format=\"{desc} │{bar}| {n_fmt}/{total_fmt}\"\n","    )\n","\n","    # Preload mesh (optional)\n","    _ = o3d.io.read_point_cloud(os.path.join(linemod_path, cls, \"mesh.ply\"))\n","\n","    # Process each keypoint folder\n","    for idx_pt, kp in enumerate(keypts, start=1):\n","        folder_name = f\"Out_pt{idx_pt}_dm\"\n","        out_folder  = os.path.join(original_linemod_path, cls, folder_name)\n","        os.makedirs(out_folder, exist_ok=True)\n","        last_folder[cls] = folder_name\n","\n","        for fname in dpt_files:\n","            # Update description to show current folder\n","            pbar.set_description(f\"🔄 Class {cls} | 🔑 {folder_name}\")\n","\n","            # Read & mask depth\n","            depth = read_depth(os.path.join(depth_dir, fname))\n","            mask  = np.asarray(\n","                Image.open(os.path.join(linemod_path, cls, \"mask\",\n","                                        fname.replace(\".dpt\", \".png\")))\n","                     .convert(\"L\")\n","            )\n","            depth[mask == 0] = 0.0\n","\n","            # Load pose matrix\n","            base = os.path.splitext(fname)[0]\n","            RT   = np.load(os.path.join(linemod_path, cls, \"pose\",\n","                                        f\"pose{base}.npy\"))\n","\n","            # Backproject and compute distances\n","            pts, vs, us = rgbd_to_point_cloud(linemod_K, depth)\n","            _, kp_cam_xyz = project(np.array([kp]), linemod_K, RT)\n","            kp_cam = kp_cam_xyz[0]\n","            dist_list = np.linalg.norm(pts - kp_cam, axis=1)\n","\n","            # Build & save radius map\n","            radius_map = np.zeros_like(mask, dtype=np.float32)\n","            radius_map = fast_for_map(vs, us, pts, dist_list, radius_map)\n","            np.save(os.path.join(out_folder, base + \".npy\"), radius_map)\n","\n","            pbar.update(1)\n","\n","    # Close this class's bar\n","    pbar.close()\n","\n","# Final summary\n","print(\"\\n\" + \"🔔 Summary\".center(80, \"─\"))\n","for cls in linemod_cls_names:\n","    folder = last_folder.get(cls, \"—\")\n","    print(f\"Class {cls}  →  {folder}\")\n","print(\"✅ All processing complete\".center(80, \"─\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APk0T30mQ5NN","executionInfo":{"status":"ok","timestamp":1748443690683,"user_tz":-120,"elapsed":100484,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"d4c1fd6f-06ba-4839-dab8-d51706e70e11"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","───────────────────────────────────📂 Class 01───────────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Class 01 | 🔑 Out_pt9_dm:  │████████████████████████████████████| 11124/11124"]},{"output_type":"stream","name":"stdout","text":["\n","───────────────────────────────────🔔 Summary────────────────────────────────────\n","Class 01  →  Out_pt9_dm\n","───────────────────────────✅ All processing complete────────────────────────────\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"qx11kUkndrmA"},"source":["### **Train Model**"]},{"cell_type":"markdown","metadata":{"id":"bT_d0-U-gkpW"},"source":["### Split Data: %70: Train - %20 Validation - %10 Test"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1159,"status":"ok","timestamp":1748443698083,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"},"user_tz":-120},"id":"_MLHwzbAvO4m","outputId":"8e62ac5a-477b-4c43-b325-c036482d0268"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Splitting RGB frames for class 01 ---\n","✅ Class 01: train=865, val=247, test=124\n","\n","All classes processed successfully.\n"]}],"source":["#Split RGB Files to Train Validate Test\n","import os\n","import glob\n","import yaml\n","import random\n","\n","# ==== CONFIGURATION ====\n","# Base directory containing class subfolders\n","base_dir = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","# List of class identifiers to process\n","classes = [\"01\"]\n","# Fixed seed for reproducibility in splitting\n","random.seed(42)\n","\n","for cls in classes:\n","    print(f\"\\n--- Splitting RGB frames for class {cls} ---\")\n","\n","    # 1) Gather all RGB image indices (filenames without extension)\n","    rgb_dir = os.path.join(base_dir, cls, \"rgb\")\n","    rgb_paths = sorted(glob.glob(os.path.join(rgb_dir, \"*.png\")))\n","    rgb_idx = [os.path.splitext(os.path.basename(p))[0] for p in rgb_paths]\n","\n","    # 2) Load GT keys from gt.yml, zero-pad them to six digits\n","    gt_path = os.path.join(base_dir, cls, \"gt.yml\")\n","    if not os.path.isfile(gt_path):\n","        print(f\"WARNING: gt.yml not found for class {cls}, skipping.\")\n","        continue\n","\n","    with open(gt_path, \"r\") as f:\n","        gt = yaml.safe_load(f)\n","    gt_keys = {f\"{int(k):06d}\" for k in gt.keys()}\n","\n","    # 3) Keep only indices present in both RGB and GT\n","    valid_idx = [i for i in rgb_idx if i in gt_keys]\n","    if not valid_idx:\n","        print(f\"WARNING: No valid frames for class {cls}, skipping.\")\n","        continue\n","\n","    # 4) Shuffle and split 70% train / 20% val / 10% test\n","    random.shuffle(valid_idx)\n","    n = len(valid_idx)\n","    cut1 = int(0.7 * n)\n","    cut2 = int(0.9 * n)\n","    train_idx = valid_idx[:cut1]\n","    val_idx   = valid_idx[cut1:cut2]\n","    test_idx  = valid_idx[cut2:]\n","\n","    # 5) Write splits to text files under a new \"Split\" folder\n","    split_dir = os.path.join(base_dir, cls, \"Split\")\n","    os.makedirs(split_dir, exist_ok=True)\n","    with open(os.path.join(split_dir, \"train.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(train_idx))\n","    with open(os.path.join(split_dir, \"val.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(val_idx))\n","    with open(os.path.join(split_dir, \"test.txt\"), \"w\") as f:\n","        f.write(\"\\n\".join(test_idx))\n","\n","    print(f\"✅ Class {cls}: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n","\n","print(\"\\nAll classes processed successfully.\")\n"]},{"cell_type":"markdown","source":["### Normalization over Dataset"],"metadata":{"id":"Dptivg10sXLy"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","\n","ROOT      = \"/content/dataset/linemod/Linemod_preprocessed/data\"\n","CLASSES   = ['01']\n","OUT_PTS   = [f\"Out_pt{i}_dm\" for i in range(1,10)]\n","MEAN_RGB  = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","STD_RGB   = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","DEPTH_MAX = 10.0\n","\n","def read_depth_dpt(path):\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","    return data.reshape(h, w).astype(np.float32)\n","\n","def sep():\n","    print(\"─\" * 70)\n","\n","for cls in CLASSES:\n","    cls_dir = os.path.join(ROOT, cls)\n","    if not os.path.isdir(cls_dir):\n","        sep()\n","        print(f\"🚫 Class folder missing: {cls_dir}\")\n","        sep()\n","        continue\n","\n","    sep()\n","    print(f\"📂  Class {cls}  \".center(70, \"─\"))\n","    sep()\n","\n","    # 1) فقط بررسی Out_pt*_dm/*.npy (بدون تغییر)\n","    print(f\"{'Folder':<20} │ {'Count':>5}\")\n","    sep()\n","    for out_dir in OUT_PTS:\n","        dir_path = os.path.join(cls_dir, out_dir)\n","        if not os.path.isdir(dir_path):\n","            continue\n","        files = [f for f in os.listdir(dir_path) if f.endswith(\".npy\")]\n","        print(f\"📦  {out_dir:<16} │ {len(files):>5}\")\n","        for fn in files:\n","            path = os.path.join(dir_path, fn)\n","            try:\n","                if os.path.getsize(path) == 0:\n","                    raise ValueError(\"empty file\")\n","                arr = np.load(path).astype(np.float32)\n","                m   = arr.max() if arr.max() > 0 else 1.0\n","                np.save(path, arr / m)\n","            except Exception as e:\n","                print(f\"   ⚠️  [SKIP] {fn:<16} │ {str(e)}\")\n","    sep()\n","\n","    # 2) فقط بررسی rgb/depth/mask/pose برای TRAIN, VAL و TEST\n","    split_dir = os.path.join(cls_dir, \"Split\")\n","    for split in (\"train.txt\", \"val.txt\", \"test.txt\"):\n","        split_path = os.path.join(split_dir, split)\n","        if not os.path.isfile(split_path):\n","            print(f\"ℹ️  No {split:<12} │ skipping\")\n","            continue\n","\n","        ids = []\n","        with open(split_path) as f:\n","            for line in f:\n","                s = line.strip()\n","                if s.isdigit():\n","                    ids.append(int(s))\n","        print()\n","        sep()\n","        print(f\"🗂️  Split: {split:<10} │ {len(ids):>5} images\")\n","        sep()\n","\n","        # RGB (فقط بررسی، ذخیره نمی‌شود)\n","        print(f\"🎨  {'RGB':<16} │ checking ...\")\n","        for idx in ids:\n","            try:\n","                p = os.path.join(cls_dir, \"rgb\", f\"{idx:06d}.png\")\n","                im = Image.open(p).convert(\"RGB\")\n","                arr = (np.asarray(im, np.float32) / 255.0 - MEAN_RGB) / STD_RGB\n","                # فقط بررسی، ذخیره نمی‌شود\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP rgb {idx:06d}] │ {str(e)}\")\n","\n","        # Depth (فقط بررسی، ذخیره نمی‌شود)\n","        print(f\"🌊  {'Depth':<16} │ checking ...\")\n","        for idx in ids:\n","            try:\n","                p  = os.path.join(cls_dir, \"depth\", f\"{idx:06d}.dpt\")\n","                dp = read_depth_dpt(p)\n","                dp = np.clip(dp, 0, DEPTH_MAX) / DEPTH_MAX\n","                # فقط بررسی، ذخیره نمی‌شود\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP depth {idx:06d}] │ {str(e)}\")\n","\n","        # Mask (فقط بررسی، ذخیره نمی‌شود)\n","        print(f\"🛡️  {'Mask':<16} │ checking ...\")\n","        for idx in ids:\n","            try:\n","                p  = os.path.join(cls_dir, \"mask\", f\"{idx:06d}.png\")\n","                m  = np.asarray(Image.open(p).convert(\"L\"), np.uint8)\n","                # فقط بررسی، ذخیره نمی‌شود\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP mask {idx:06d}] │ {str(e)}\")\n","\n","        # Pose (در صورت نیاز نرمالایز و بازنویسی می‌شود)\n","        print(f\"🤖  {'Pose':<16} │ normalizing ...\")\n","        for idx in ids:\n","            try:\n","                p    = os.path.join(cls_dir, \"pose\", f\"pose{idx:06d}.npy\")\n","                pose = np.load(p).astype(np.float32)\n","                pose[:3,3] /= 1000.0\n","                np.save(p, pose)\n","            except Exception as e:\n","                print(f\"   🚧 [SKIP pose {idx:06d}] │ {str(e)}\")\n","        sep()\n","\n","    print(f\"✅ Done class {cls}\".center(70))\n","    sep()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FCouNlsqbwpJ","executionInfo":{"status":"ok","timestamp":1748443765335,"user_tz":-120,"elapsed":64868,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"f2d0a8e0-184f-49f0-bfc0-3b759fdc58a3"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["──────────────────────────────────────────────────────────────────────\n","────────────────────────────📂  Class 01  ─────────────────────────────\n","──────────────────────────────────────────────────────────────────────\n","Folder               │ Count\n","──────────────────────────────────────────────────────────────────────\n","📦  Out_pt1_dm       │  1236\n","📦  Out_pt2_dm       │  1236\n","📦  Out_pt3_dm       │  1236\n","📦  Out_pt4_dm       │  1236\n","📦  Out_pt5_dm       │  1236\n","📦  Out_pt6_dm       │  1236\n","📦  Out_pt7_dm       │  1236\n","📦  Out_pt8_dm       │  1236\n","📦  Out_pt9_dm       │  1236\n","──────────────────────────────────────────────────────────────────────\n","\n","──────────────────────────────────────────────────────────────────────\n","🗂️  Split: train.txt  │   865 images\n","──────────────────────────────────────────────────────────────────────\n","🎨  RGB              │ checking ...\n","🌊  Depth            │ checking ...\n","🛡️  Mask             │ checking ...\n","🤖  Pose             │ normalizing ...\n","──────────────────────────────────────────────────────────────────────\n","\n","──────────────────────────────────────────────────────────────────────\n","🗂️  Split: val.txt    │   247 images\n","──────────────────────────────────────────────────────────────────────\n","🎨  RGB              │ checking ...\n","🌊  Depth            │ checking ...\n","🛡️  Mask             │ checking ...\n","🤖  Pose             │ normalizing ...\n","──────────────────────────────────────────────────────────────────────\n","\n","──────────────────────────────────────────────────────────────────────\n","🗂️  Split: test.txt   │   124 images\n","──────────────────────────────────────────────────────────────────────\n","🎨  RGB              │ checking ...\n","🌊  Depth            │ checking ...\n","🛡️  Mask             │ checking ...\n","🤖  Pose             │ normalizing ...\n","──────────────────────────────────────────────────────────────────────\n","                           ✅ Done class 01                            \n","──────────────────────────────────────────────────────────────────────\n"]}]},{"cell_type":"markdown","source":["# **Train**"],"metadata":{"id":"2KsgE4ZxQxhJ"}},{"cell_type":"code","source":["!pip install colorama"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-sn2AhNkYBw","executionInfo":{"status":"ok","timestamp":1748443790328,"user_tz":-120,"elapsed":2425,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"3ff2d4df-0c05-480c-fc40-9c659c9fb3e3"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.6\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Enhanced RCVPose: Deep Learning Model for 6D Pose Estimation\n","\n","This script is designed to train a 6D pose estimation model using RGB images, depth maps, masks, pose data, and radius maps (Outside9 points) for multiple objects. The code is fully documented in English and all data paths are explicitly set according to the provided folder structure.\n","\n","Folder Structure for Each Object (e.g., 01, 02, ...):\n","- Out_pt1_dm to Out_pt9_dm: Folders containing .npy files for each radius map\n","- depth: Contains .dpt files (depth images)\n","- mask: Contains .png files (object masks)\n","- pose: Contains pose files named as pose00000.npy, pose00001.npy, ...\n","- rgb: Contains .png files (RGB images)\n","- Split: Contains train.txt, val.txt, test.txt (lists of sample names)\n","- Outside9.npy: Reference points for the object\n","- gt.yml, info.yml: Metadata files\n","- mesh.ply: 3D mesh model\n","- test.txt, train.txt: Additional text files (if needed)\n","\n","All measurements are in millimeters.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import cv2\n","import os\n","from tqdm import tqdm\n","import logging\n","from datetime import datetime\n","from google.colab import drive\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.nn import functional as F\n","import time\n","from colorama import init, Fore, Style\n","from torch.cuda.amp import autocast, GradScaler\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","\n","# Initialize colorama for colored terminal output\n","init()\n","\n","# =========================\n","# Configuration Section\n","# =========================\n","CONFIG = {\n","    # Base directory containing all object folders (01, 02, ...)\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',\n","    # List of object IDs to train (edit this list to select which objects to train)\n","    'OBJECT_IDS': ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15'],\n","    # Subdirectory names inside each object folder\n","    'RGB_DIR': 'rgb',              # RGB images (.png)\n","    'DEPTH_DIR': 'depth',          # Depth maps (.dpt)\n","    'MASK_DIR': 'mask',            # Object masks (.png)\n","    'POSE_DIR': 'pose',            # Pose data (pose00000.npy, ...)\n","    'RADIUS_BASE_DIR': '.',        # Base dir for Out_pt*_dm folders (relative to object folder)\n","    'RADIUS_PREFIX': 'Out_pt',     # Prefix for radius map folders\n","    'RADIUS_SUFFIX': '_dm',        # Suffix for radius map folders\n","    'NUM_RADIUS_POINTS': 9,        # Number of radius map points\n","    'SPLIT_DIR': 'Split',          # Directory containing split files\n","    'TRAIN_SPLIT': 'train.txt',    # Training set filenames\n","    'VAL_SPLIT': 'val.txt',        # Validation set filenames\n","    'TEST_SPLIT': 'test.txt',      # Test set filenames\n","    # Training parameters\n","    'BATCH_SIZE': 8,               # Reduced batch size for memory efficiency\n","    'NUM_WORKERS': 2,              # Reduced number of workers\n","    'NUM_EPOCHS': 100,             # Number of training epochs\n","    'LEARNING_RATE': 0.001,        # Initial learning rate\n","    'ACCUMULATION_STEPS': 4,       # Number of mini-batches to accumulate gradients over\n","    # Model saving\n","    'MODELS_DIR': '/content/models',  # Directory to save trained models\n","}\n","\n","# =========================\n","# Logger for Training Output\n","# =========================\n","class TrainingLogger:\n","    \"\"\"\n","    Logger class for professional, colored, and well-separated output during training.\n","    \"\"\"\n","    def __init__(self):\n","        self.start_time = time.time()\n","        self.epoch_times = []\n","        self.separator = \"=\" * 100\n","        self.sub_separator = \"-\" * 100\n","    def log_section(self, message):\n","        print(f\"\\n{Fore.CYAN}{self.separator}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}🚀 {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{self.separator}{Style.RESET_ALL}\\n\")\n","    def log_subsection(self, message, no_lines=False):\n","        if no_lines:\n","            print(f\"{Fore.YELLOW}{message}{Style.RESET_ALL}\")\n","        else:\n","            print(f\"\\n{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\")\n","            print(f\"{Fore.YELLOW}📌 {message}{Style.RESET_ALL}\")\n","            print(f\"{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\\n\")\n","    def log_info(self, message):\n","        print(f\"{Fore.GREEN}ℹ️  {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.GREEN}{'.' * 50}{Style.RESET_ALL}\")\n","    def log_warning(self, message):\n","        print(f\"\\n{Fore.YELLOW}⚠️  {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.YELLOW}{'!' * 50}{Style.RESET_ALL}\\n\")\n","    def log_error(self, message):\n","        print(f\"\\n{Fore.RED}❌ {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.RED}{'#' * 50}{Style.RESET_ALL}\\n\")\n","    def log_success(self, message):\n","        print(f\"\\n{Fore.GREEN}✅ {message}{Style.RESET_ALL}\")\n","        print(f\"{Fore.GREEN}{'*' * 50}{Style.RESET_ALL}\\n\")\n","    def log_metrics(self, metrics, phase=\"Training\", no_lines=False):\n","        if no_lines:\n","            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}📊 {phase} Metrics:{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'-' * 50}{Style.RESET_ALL}\")\n","            for key, value in metrics.items():\n","                print(f\"{Fore.MAGENTA}   {key}: {value:.4f}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\\n\")\n","        else:\n","            print(f\"\\n{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}📊 {phase} Metrics:{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'-' * 50}{Style.RESET_ALL}\")\n","            for key, value in metrics.items():\n","                print(f\"{Fore.MAGENTA}   {key}: {value:.4f}{Style.RESET_ALL}\")\n","            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\\n\")\n","    def log_model_save(self, path):\n","        print(f\"\\n{Fore.BLUE}{'=' * 50}{Style.RESET_ALL}\")\n","        print(f\"{Fore.BLUE}💾 Model saved at: {path}{Style.RESET_ALL}\")\n","        print(f\"{Fore.BLUE}{'=' * 50}{Style.RESET_ALL}\\n\")\n","    def log_time(self, epoch, total_epochs):\n","        elapsed = time.time() - self.start_time\n","        avg_epoch_time = elapsed / (epoch + 1)\n","        remaining_time = avg_epoch_time * (total_epochs - epoch - 1)\n","        print(f\"\\n{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}⏱️  Time Information:{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{'-' * 50}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Elapsed Time: {self.format_time(elapsed)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Average Epoch Time: {self.format_time(avg_epoch_time)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}   Estimated Remaining Time: {self.format_time(remaining_time)}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\\n\")\n","    def log_epoch_start(self, epoch, total_epochs):\n","        print(f\"\\n{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}🔄 Starting Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\\n\")\n","    def log_epoch_end(self, epoch, total_epochs, metrics):\n","        print(f\"\\n{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\")\n","        print(f\"{Fore.CYAN}✅ Completed Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n","        self.log_metrics(metrics, f\"Epoch {epoch + 1} Summary\")\n","        print(f\"{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\\n\")\n","    @staticmethod\n","    def format_time(seconds):\n","        hours = int(seconds // 3600)\n","        minutes = int((seconds % 3600) // 60)\n","        seconds = int(seconds % 60)\n","        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n","\n","def safe_read_depth(path):\n","    \"\"\"\n","    Safely read a depth image. If the file is .dpt, try to read as binary; otherwise, use OpenCV.\n","    Returns a numpy array or raises a FileNotFoundError if not found or unreadable.\n","    \"\"\"\n","    if not os.path.exists(path):\n","        raise FileNotFoundError(f\"Depth image not found: {path}\")\n","    if path.endswith('.dpt'):\n","        try:\n","            with open(path, 'rb') as f:\n","                h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","                data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","                if data.size != h*w:\n","                    raise ValueError(f\"Depth data size mismatch in file: {path}\")\n","                return data.reshape(h, w).astype(np.float32)\n","        except Exception as e:\n","            raise IOError(f\"Failed to read .dpt file: {path}. Error: {e}\")\n","    # For other formats, use OpenCV\n","    depth_img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n","    if depth_img is None:\n","        raise IOError(f\"Failed to read depth image: {path}\")\n","    return depth_img.astype(np.float32)\n","\n","# =========================\n","# Custom Dataset Definition\n","# =========================\n","class CustomDataset(Dataset):\n","    \"\"\"\n","    Custom PyTorch Dataset for loading RGB, depth, mask, pose, and radius map data for a single object.\n","    Each sample consists of:\n","      - RGB image (.png)\n","      - Depth map (.dpt)\n","      - Mask (.png)\n","      - Pose (pose{base_name}.npy)\n","      - Radius maps (9 .npy files, one from each Out_pt*_dm folder)\n","    All measurements are in millimeters.\n","    \"\"\"\n","    def __init__(self, rgb_dir, depth_dir, mask_dir, pose_dir, radius_base_dir, transform_rgb=None, transform_depth=None, transform_mask=None):\n","        self.rgb_dir = rgb_dir\n","        self.depth_dir = depth_dir\n","        self.mask_dir = mask_dir\n","        self.pose_dir = pose_dir\n","        self.radius_base_dir = radius_base_dir\n","        self.transform_rgb = transform_rgb\n","        self.transform_depth = transform_depth\n","        self.transform_mask = transform_mask\n","        self.filenames = sorted([f for f in os.listdir(rgb_dir) if f.endswith('.png')])\n","    def __len__(self):\n","        return len(self.filenames)\n","    def __getitem__(self, idx):\n","        filename = self.filenames[idx]\n","        base_name = filename.split('.')[0]  # e.g., '000000'\n","        # Load RGB image (H, W, 3) in uint8\n","        rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","        rgb_img = cv2.imread(rgb_path)\n","        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","        rgb_img = Image.fromarray(rgb_img)  # Convert to PIL.Image\n","        # Load depth map safely (handles .dpt and other formats)\n","        depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","        depth_img = safe_read_depth(depth_path)  # (H, W) in float32\n","        depth_img = np.expand_dims(depth_img / 10000.0, axis=2)  # Normalize depth to [0, 1] and add channel\n","        depth_img = (depth_img * 255).astype(np.uint8)  # Convert to uint8 for PIL compatibility\n","        depth_img = Image.fromarray(depth_img.squeeze(), mode='L')  # Convert to PIL.Image\n","        # Load mask (H, W) in uint8\n","        mask_path = os.path.join(self.mask_dir, f'{base_name}.png')\n","        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","        mask = np.expand_dims(mask.astype(np.float32) / 255.0, axis=2)  # Normalize mask to [0, 1] and add channel\n","        mask = (mask * 255).astype(np.uint8)\n","        mask = Image.fromarray(mask.squeeze(), mode='L')\n","        # Load pose (with 'pose' prefix)\n","        pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","        pose = np.load(pose_path)\n","        # Convert pose to (7,) vector if needed\n","        if pose.shape == (3, 4):\n","            rot = R.from_matrix(pose[:, :3]).as_quat()  # (x, y, z, w)\n","            trans = pose[:, 3]\n","            pose_vec = np.concatenate([trans, rot])  # (3,) + (4,) = (7,)\n","            pose = pose_vec\n","        elif pose.shape == (4, 4):\n","            rot = R.from_matrix(pose[:3, :3]).as_quat()\n","            trans = pose[:3, 3]\n","            pose_vec = np.concatenate([trans, rot])\n","            pose = pose_vec\n","        # If pose is already (7,), do nothing\n","        # Load radius maps for all 9 points\n","        radius_maps = []\n","        for pt_idx in range(1, CONFIG['NUM_RADIUS_POINTS'] + 1):\n","            radius_folder = f\"Out_pt{pt_idx}_dm\"\n","            radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","            radius_map = np.load(radius_path)\n","            # --- FULL RESOLUTION: Do NOT resize radius maps ---\n","            # radius_map = cv2.resize(radius_map, (256, 256), interpolation=cv2.INTER_LINEAR)  # <-- REMOVE or COMMENT OUT\n","            radius_maps.append(radius_map)\n","        radius_maps = np.array(radius_maps)\n","        # Apply separate transforms for each type\n","        if self.transform_rgb:\n","            rgb_img = self.transform_rgb(rgb_img)\n","        if self.transform_depth:\n","            depth_img = self.transform_depth(depth_img)\n","        if self.transform_mask:\n","            mask = self.transform_mask(mask)\n","        return {\n","            'rgb': rgb_img.float(),\n","            'depth': depth_img.float(),\n","            'mask': mask.float(),\n","            'pose': torch.FloatTensor(pose),\n","            'radius_maps': torch.FloatTensor(radius_maps)\n","        }\n","\n","class AdvancedAugmentation:\n","    \"\"\"\n","    Applies various transformations to input images to improve model robustness.\n","    This helps the model learn better by seeing different variations of the same image.\n","    \"\"\"\n","    def __init__(self):\n","        # Set up color transformations\n","        self.color_jitter = transforms.ColorJitter(\n","            brightness=0.2,  # Random brightness adjustment\n","            contrast=0.2,    # Random contrast adjustment\n","            saturation=0.2,  # Random saturation adjustment\n","            hue=0.1         # Random hue adjustment\n","        )\n","\n","    def __call__(self, rgb, depth, mask):\n","        # Apply color transformations to RGB image\n","        rgb = self.color_jitter(rgb)\n","\n","        # Random rotation between -10 and 10 degrees\n","        angle = np.random.uniform(-10, 10)\n","        rgb = transforms.functional.rotate(rgb, angle)\n","        depth = transforms.functional.rotate(depth, angle)\n","        mask = transforms.functional.rotate(mask, angle)\n","\n","        # Random scaling between 0.9 and 1.1\n","        scale = np.random.uniform(0.9, 1.1)\n","        rgb = transforms.functional.resize(rgb, (int(rgb.shape[1] * scale), int(rgb.shape[2] * scale)))\n","        depth = transforms.functional.resize(depth, (int(depth.shape[1] * scale), int(depth.shape[2] * scale)))\n","        mask = transforms.functional.resize(mask, (int(mask.shape[1] * scale), int(mask.shape[2] * scale)))\n","\n","        # Add small random noise to depth image\n","        depth_noise = torch.randn_like(depth) * 0.01\n","        depth = depth + depth_noise\n","\n","        return rgb, depth, mask\n","\n","class AttentionModule(nn.Module):\n","    \"\"\"\n","    Attention mechanism to focus on important parts of the image.\n","    This helps the model pay more attention to relevant features.\n","    \"\"\"\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        # Create query, key, and value projections\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))  # Learnable parameter\n","\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","\n","        # Generate attention map\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","\n","        # Calculate attention scores\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","\n","        # Apply attention to values\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    \"\"\"\n","    Feature Pyramid Network for multi-scale feature extraction.\n","    Accepts a list of input channels for each feature map.\n","    \"\"\"\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        # Create lateral connections for each input channel\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        # Create output convolutions\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","    def forward(self, features):\n","        # features should be a list of feature maps from different layers\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        # Top-down pathway\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        # Final convolutions\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    \"\"\"\n","    Main model for 6D pose estimation.\n","    Combines RGB and depth information to predict object pose.\n","    Uses FPN with correct feature extraction from ResNet.\n","    Now uses ResNet50 (original, higher memory usage).\n","    \"\"\"\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        # Use ResNet50 as backbone (original)\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])   # Output: 256\n","        self.rgb_layer2 = list(resnet.children())[5]                    # Output: 512\n","        self.rgb_layer3 = list(resnet.children())[6]                    # Output: 1024\n","        self.rgb_layer4 = list(resnet.children())[7]                    # Output: 2048\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","        # Fusion block\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        # Pose head (global pooling + FC)\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","        # Outside9 head: output 9 full-resolution maps\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)  # 9 channel output\n","        )\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)  # [B, 256, H, W]\n","        # Pose: global pooling + FC\n","        pooled = self.global_pool(fused)  # [B, 256, 1, 1]\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","        # Outside9: full maps\n","        outside9 = self.outside9_head(fused)  # [B, 9, h, w]\n","        # --- ALWAYS UPSAMPLE to input image size (full resolution) ---\n","        target_size = (rgb.shape[2], rgb.shape[3])  # (H, W)\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","        return pose, outside9\n","\n","class GeometricLoss(nn.Module):\n","    \"\"\"\n","    Loss function that combines translation, rotation, and point correspondence errors.\n","    All measurements are in millimeters.\n","    \"\"\"\n","    def __init__(self):\n","        super(GeometricLoss, self).__init__()\n","\n","    def forward(self, pred_pose, target_pose, pred_outside9, target_outside9):\n","        # --- LOSS IS COMPUTED AT FULL RESOLUTION ---\n","        trans_loss = F.mse_loss(pred_pose[:, :3], target_pose[:, :3])\n","        rot_loss = 1 - torch.abs(torch.sum(pred_pose[:, 3:] * target_pose[:, 3:], dim=1))\n","        rot_loss = rot_loss.mean()\n","        points_loss = F.mse_loss(pred_outside9, target_outside9)\n","        total_loss = trans_loss + 0.1 * rot_loss + 0.5 * points_loss\n","        return total_loss, trans_loss, rot_loss, points_loss\n","\n","def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n","    \"\"\"\n","    Train the model with advanced features and professional logging.\n","    Uses gradient accumulation and mixed precision training for memory efficiency.\n","    \"\"\"\n","    logger = TrainingLogger()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    logger.log_section(\"Initializing Training\")\n","    logger.log_info(f\"Using device: {device}\")\n","    model = model.to(device)\n","    os.makedirs(CONFIG['MODELS_DIR'], exist_ok=True)\n","    logger.log_success(f\"Created models directory at: {CONFIG['MODELS_DIR']}\")\n","\n","    criterion = GeometricLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n","    scheduler = optim.lr_scheduler.OneCycleLR(\n","        optimizer,\n","        max_lr=learning_rate,\n","        epochs=num_epochs,\n","        steps_per_epoch=len(train_loader)\n","    )\n","\n","    # Initialize gradient scaler for mixed precision training\n","    scaler = GradScaler()\n","\n","    best_val_loss = float('inf')\n","    accumulation_steps = CONFIG.get('ACCUMULATION_STEPS', 4)\n","    logger.log_section(\"Starting Training Loop\")\n","\n","    for epoch in range(num_epochs):\n","        torch.cuda.empty_cache()  # Free up GPU memory at the start of each epoch\n","        epoch_start_time = time.time()\n","        # #### EPOCH HEADER ####\n","        print(f\"\\n{'#'*12} Epoch {epoch+1}/{num_epochs} {'#'*12}\")\n","        logger.log_epoch_start(epoch, num_epochs)\n","        model.train()\n","        train_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n","        # Training Phase (no line separators)\n","        logger.log_subsection(\"Training Phase\", no_lines=True)\n","        optimizer.zero_grad()\n","        for i, batch in enumerate(tqdm(train_loader, desc=f'🔄 Training')):\n","            rgb = batch['rgb'].to(device)\n","            depth = batch['depth'].to(device)\n","            pose_target = batch['pose'].to(device)\n","            outside9_target = batch['radius_maps'].to(device)\n","            with autocast():\n","                pose_pred, outside9_pred = model(rgb, depth)\n","                total_loss, trans_loss, rot_loss, points_loss = criterion(\n","                    pose_pred, pose_target, outside9_pred, outside9_target\n","                )\n","            scaler.scale(total_loss / accumulation_steps).backward()\n","            if (i + 1) % accumulation_steps == 0:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","            train_losses['total'] += total_loss.item()\n","            train_losses['trans'] += trans_loss.item()\n","            train_losses['rot'] += rot_loss.item()\n","            train_losses['points'] += points_loss.item()\n","        if (i + 1) % accumulation_steps != 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","        for k in train_losses:\n","            train_losses[k] /= len(train_loader)\n","        logger.log_metrics(train_losses, \"Training\", no_lines=True)\n","        # ----\n","        print(\"----\")\n","        model.eval()\n","        val_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n","        # Validation Phase (no line separators)\n","        logger.log_subsection(\"Validation Phase\", no_lines=True)\n","        with torch.no_grad():\n","            for batch in tqdm(val_loader, desc=f'🔍 Validation'):\n","                rgb = batch['rgb'].to(device)\n","                depth = batch['depth'].to(device)\n","                pose_target = batch['pose'].to(device)\n","                outside9_target = batch['radius_maps'].to(device)\n","                with autocast():\n","                    pose_pred, outside9_pred = model(rgb, depth)\n","                    total_loss, trans_loss, rot_loss, points_loss = criterion(\n","                        pose_pred, pose_target, outside9_pred, outside9_target\n","                    )\n","                val_losses['total'] += total_loss.item()\n","                val_losses['trans'] += trans_loss.item()\n","                val_losses['rot'] += rot_loss.item()\n","                val_losses['points'] += points_loss.item()\n","        for k in val_losses:\n","            val_losses[k] /= len(val_loader)\n","        logger.log_metrics(val_losses, \"Validation\", no_lines=True)\n","        # #### EPOCH FOOTER ####\n","        print(f\"{'#'*12} End Epoch {epoch+1}/{num_epochs} {'#'*12}\\n\")\n","\n","        if val_losses['total'] < best_val_loss:\n","            best_val_loss = val_losses['total']\n","            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","            save_path = os.path.join(CONFIG['MODELS_DIR'], f'best_model_{timestamp}.pth')\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': val_losses['total'],\n","                'val_metrics': val_losses,\n","                'config': CONFIG\n","            }, save_path)\n","            logger.log_success(f\"New best model saved with validation loss: {val_losses['total']:.4f}\")\n","            logger.log_model_save(save_path)\n","\n","            # Save backup to Google Drive\n","            drive_save_path = os.path.join('/content/drive/MyDrive/models', f'best_model_{timestamp}.pth')\n","            os.makedirs(os.path.dirname(drive_save_path), exist_ok=True)\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': val_losses['total'],\n","                'val_metrics': val_losses,\n","                'config': CONFIG\n","            }, drive_save_path)\n","            logger.log_success(f\"Backup copy saved to Google Drive\")\n","            logger.log_model_save(drive_save_path)\n","\n","        logger.log_time(epoch, num_epochs)\n","        logger.log_epoch_end(epoch, num_epochs, {\n","            'train_loss': train_losses['total'],\n","            'val_loss': val_losses['total']\n","        })\n","\n","    logger.log_section(\"Training Completed\")\n","    logger.log_success(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n","\n","def load_split_file(split_file):\n","    \"\"\"\n","    Load filenames from a split file.\n","\n","    Args:\n","        split_file (str): Path to the split file (Train.txt, Test.txt, or Validation.txt)\n","\n","    Returns:\n","        list: List of filenames (without extension)\n","    \"\"\"\n","    with open(split_file, 'r') as f:\n","        filenames = [line.strip() for line in f.readlines()]\n","    return filenames\n","\n","def main():\n","    logger = TrainingLogger()\n","    logger.log_section(\"Starting RCVPose Training for Selected Objects\")\n","\n","    # Use the list of object IDs specified in CONFIG['OBJECT_IDS']\n","    object_ids = CONFIG['OBJECT_IDS']\n","    base_data_dir = CONFIG['BASE_DIR']\n","\n","    for object_id in object_ids:\n","        logger.log_section(f\"Training for Object {object_id}\")\n","        object_dir = os.path.join(base_data_dir, object_id)\n","\n","        rgb_dir = os.path.join(object_dir, CONFIG['RGB_DIR'])\n","        depth_dir = os.path.join(object_dir, CONFIG['DEPTH_DIR'])\n","        mask_dir = os.path.join(object_dir, CONFIG['MASK_DIR'])\n","        pose_dir = os.path.join(object_dir, CONFIG['POSE_DIR'])\n","        radius_dir = os.path.join(object_dir, CONFIG['RADIUS_BASE_DIR'])\n","        split_dir = os.path.join(object_dir, CONFIG['SPLIT_DIR'])\n","\n","        logger.log_info(\"Loading split files\")\n","        # Load split files\n","        train_filenames = load_split_file(os.path.join(split_dir, CONFIG['TRAIN_SPLIT']))\n","        val_filenames = load_split_file(os.path.join(split_dir, CONFIG['VAL_SPLIT']))\n","        test_filenames = load_split_file(os.path.join(split_dir, CONFIG['TEST_SPLIT']))\n","\n","        logger.log_success(f\"Loaded {len(train_filenames)} training samples for Object {object_id}\")\n","        logger.log_success(f\"Loaded {len(val_filenames)} validation samples for Object {object_id}\")\n","        logger.log_success(f\"Loaded {len(test_filenames)} test samples for Object {object_id}\")\n","\n","        logger.log_info(\"Setting up data transforms and augmentation\")\n","        # Define transforms and augmentation WITHOUT resizing (full resolution)\n","        transform_rgb = transforms.Compose([\n","            # No resizing! Use full resolution\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        transform_depth = transforms.Compose([\n","            # No resizing! Use full resolution\n","            transforms.ToTensor()\n","        ])\n","        transform_mask = transforms.Compose([\n","            # No resizing! Use full resolution\n","            transforms.ToTensor()\n","        ])\n","        augmentation = AdvancedAugmentation()\n","\n","        logger.log_info(\"Creating datasets\")\n","        # Create datasets\n","        full_dataset = CustomDataset(rgb_dir, depth_dir, mask_dir, pose_dir, radius_dir,\n","                                   transform_rgb=transform_rgb,\n","                                   transform_depth=transform_depth,\n","                                   transform_mask=transform_mask)\n","\n","        # Restore original train/val split\n","        train_dataset = torch.utils.data.Subset(full_dataset,\n","            [i for i, f in enumerate(full_dataset.filenames) if f.split('.')[0] in train_filenames])\n","        val_dataset = torch.utils.data.Subset(full_dataset,\n","            [i for i, f in enumerate(full_dataset.filenames) if f.split('.')[0] in val_filenames])\n","\n","        logger.log_info(\"Creating data loaders\")\n","        # Create data loaders with optimized settings\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=CONFIG['BATCH_SIZE'],\n","            shuffle=True,\n","            num_workers=CONFIG['NUM_WORKERS'],\n","            pin_memory=True,\n","            persistent_workers=True,\n","            prefetch_factor=2\n","        )\n","        val_loader = DataLoader(\n","            val_dataset,\n","            batch_size=CONFIG['BATCH_SIZE'],\n","            shuffle=False,\n","            num_workers=CONFIG['NUM_WORKERS'],\n","            pin_memory=True,\n","            persistent_workers=True,\n","            prefetch_factor=2\n","        )\n","\n","        logger.log_info(\"Initializing model\")\n","        # Initialize model with ResNet18 backbone\n","        model = EnhancedRCVPose()\n","\n","        logger.log_section(f\"Starting Training Process for Object {object_id}\")\n","        # Start training\n","        train_model(model, train_loader, val_loader,\n","                    num_epochs=CONFIG['NUM_EPOCHS'],\n","                    learning_rate=CONFIG['LEARNING_RATE'])\n","\n","        logger.log_section(f\"Training Process Completed for Object {object_id}\")\n","\n","    logger.log_section(\"All Object Trainings Completed\")\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BPjhiytw3HkQ","executionInfo":{"status":"error","timestamp":1748444177243,"user_tz":-120,"elapsed":375781,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"f7098405-4792-41b9-ec3d-2a23c2fa7ea4"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","====================================================================================================\n","🚀 Starting RCVPose Training for Selected Objects\n","====================================================================================================\n","\n","\n","====================================================================================================\n","🚀 Training for Object 01\n","====================================================================================================\n","\n","ℹ️  Loading split files\n","..................................................\n","\n","✅ Loaded 865 training samples for Object 01\n","**************************************************\n","\n","\n","✅ Loaded 247 validation samples for Object 01\n","**************************************************\n","\n","\n","✅ Loaded 124 test samples for Object 01\n","**************************************************\n","\n","ℹ️  Setting up data transforms and augmentation\n","..................................................\n","ℹ️  Creating datasets\n","..................................................\n","ℹ️  Creating data loaders\n","..................................................\n","ℹ️  Initializing model\n","..................................................\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 128MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","====================================================================================================\n","🚀 Starting Training Process for Object 01\n","====================================================================================================\n","\n","\n","====================================================================================================\n","🚀 Initializing Training\n","====================================================================================================\n","\n","ℹ️  Using device: cuda\n","..................................................\n","\n","✅ Created models directory at: /content/models\n","**************************************************\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-35bef6a0fc42>:467: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["\n","====================================================================================================\n","🚀 Starting Training Loop\n","====================================================================================================\n","\n","\n","############ Epoch 1/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 1/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["\r🔄 Training:   0%|          | 0/109 [00:00<?, ?it/s]<ipython-input-22-35bef6a0fc42>:489: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.1961\n","   trans: 0.1169\n","   rot: 0.7704\n","   points: 0.0043\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["\r🔍 Validation:   0%|          | 0/31 [00:00<?, ?it/s]<ipython-input-22-35bef6a0fc42>:522: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","🔍 Validation: 100%|██████████| 31/31 [00:06<00:00,  5.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.0877\n","   trans: 0.0234\n","   rot: 0.6265\n","   points: 0.0033\n","==================================================\n","\n","############ End Epoch 1/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.0877\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250528_145118.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250528_145118.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:01:12\n","   Average Epoch Time: 00:01:12\n","   Estimated Remaining Time: 01:59:08\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 1/100\n","\n","==================================================\n","📊 Epoch 1 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.1961\n","   val_loss: 0.0877\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 2/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 2/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.0944\n","   trans: 0.0520\n","   rot: 0.4099\n","   points: 0.0028\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:06<00:00,  5.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: 0.0249\n","   trans: 0.0068\n","   rot: 0.1697\n","   points: 0.0022\n","==================================================\n","\n","############ End Epoch 2/100 ############\n","\n","\n","✅ New best model saved with validation loss: 0.0249\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250528_145230.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250528_145230.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:02:24\n","   Average Epoch Time: 00:01:12\n","   Estimated Remaining Time: 01:58:11\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 2/100\n","\n","==================================================\n","📊 Epoch 2 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.0944\n","   val_loss: 0.0249\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 3/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 3/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:02<00:00,  1.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.0667\n","   trans: 0.0612\n","   rot: 0.0444\n","   points: 0.0021\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:06<00:00,  5.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: -0.0180\n","   trans: 0.0078\n","   rot: -0.2687\n","   points: 0.0021\n","==================================================\n","\n","############ End Epoch 3/100 ############\n","\n","\n","✅ New best model saved with validation loss: -0.0180\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250528_145342.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250528_145342.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:03:36\n","   Average Epoch Time: 00:01:12\n","   Estimated Remaining Time: 01:56:42\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 3/100\n","\n","==================================================\n","📊 Epoch 3 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.0667\n","   val_loss: -0.0180\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 4/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 4/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:03<00:00,  1.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.0452\n","   trans: 0.0770\n","   rot: -0.3279\n","   points: 0.0020\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:06<00:00,  5.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: -0.0392\n","   trans: 0.0088\n","   rot: -0.4899\n","   points: 0.0020\n","==================================================\n","\n","############ End Epoch 4/100 ############\n","\n","\n","✅ New best model saved with validation loss: -0.0392\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250528_145454.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250528_145454.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:04:48\n","   Average Epoch Time: 00:01:12\n","   Estimated Remaining Time: 01:55:21\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 4/100\n","\n","==================================================\n","📊 Epoch 4 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.0452\n","   val_loss: -0.0392\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 5/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 5/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training: 100%|██████████| 109/109 [01:03<00:00,  1.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Training Metrics:\n","--------------------------------------------------\n","   total: 0.0215\n","   trans: 0.1001\n","   rot: -0.7963\n","   points: 0.0019\n","==================================================\n","\n","----\n","Validation Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔍 Validation: 100%|██████████| 31/31 [00:06<00:00,  5.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","📊 Validation Metrics:\n","--------------------------------------------------\n","   total: -0.1060\n","   trans: 0.0129\n","   rot: -1.2002\n","   points: 0.0021\n","==================================================\n","\n","############ End Epoch 5/100 ############\n","\n","\n","✅ New best model saved with validation loss: -0.1060\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/models/best_model_20250528_145606.pth\n","==================================================\n","\n","\n","✅ Backup copy saved to Google Drive\n","**************************************************\n","\n","\n","==================================================\n","💾 Model saved at: /content/drive/MyDrive/models/best_model_20250528_145606.pth\n","==================================================\n","\n","\n","==================================================\n","⏱️  Time Information:\n","--------------------------------------------------\n","   Elapsed Time: 00:06:00\n","   Average Epoch Time: 00:01:12\n","   Estimated Remaining Time: 01:54:07\n","==================================================\n","\n","\n","####################################################################################################\n","✅ Completed Epoch 5/100\n","\n","==================================================\n","📊 Epoch 5 Summary Metrics:\n","--------------------------------------------------\n","   train_loss: 0.0215\n","   val_loss: -0.1060\n","==================================================\n","\n","####################################################################################################\n","\n","\n","############ Epoch 6/100 ############\n","\n","####################################################################################################\n","🔄 Starting Epoch 6/100\n","####################################################################################################\n","\n","Training Phase\n"]},{"output_type":"stream","name":"stderr","text":["🔄 Training:  11%|█         | 12/109 [00:07<01:02,  1.55it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-35bef6a0fc42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-35bef6a0fc42>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting Training Process for Object {object_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         train_model(model, train_loader, val_loader,\n\u001b[0m\u001b[1;32m    676\u001b[0m                     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_EPOCHS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                     learning_rate=CONFIG['LEARNING_RATE'])\n","\u001b[0;32m<ipython-input-22-35bef6a0fc42>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    492\u001b[0m                     \u001b[0mpose_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutside9_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutside9_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 )\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# **Validation**"],"metadata":{"id":"BbAqYcV9HLro"}},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","import cv2\n","from tqdm import tqdm\n","import torchvision.models as models\n","\n","# =========================\n","# CONFIGURATION SECTION\n","# =========================\n","CONFIG = {\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',  # Root directory containing all object folders\n","    'OBJECT_ID': '01',  # Object ID to validate (e.g., '01')\n","    'BATCH_SIZE': 1,  # For validation, use batch size 1 for accurate metrics\n","    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n","    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'MODELS_DIR': '/content/models',  # Directory containing saved models\n","    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n","}\n","\n","# =========================\n","# DEPTH READER FOR .dpt FILES\n","# =========================\n","def read_depth_dpt(path):\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","    return data.reshape(h, w).astype(np.float32)\n","\n","# =========================\n","# MODEL ARCHITECTURE\n","# =========================\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","\n","    def forward(self, features):\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        # Use ResNet50 as backbone\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])   # Output: 256\n","        self.rgb_layer2 = list(resnet.children())[5]                    # Output: 512\n","        self.rgb_layer3 = list(resnet.children())[6]                    # Output: 1024\n","        self.rgb_layer4 = list(resnet.children())[7]                    # Output: 2048\n","\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","\n","        # Fusion block\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Pose head\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","\n","        # Outside9 head\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)\n","        )\n","\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)\n","\n","        # Pose prediction\n","        pooled = self.global_pool(fused)\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","\n","        # Outside9 prediction\n","        outside9 = self.outside9_head(fused)\n","        target_size = (rgb.shape[2], rgb.shape[3])\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","\n","        return pose, outside9\n","\n","# =========================\n","# DATASET CLASS\n","# =========================\n","class ValidationDataset(Dataset):\n","    def __init__(self, base_dir, object_id, num_radius_points=9):\n","        self.base_dir = base_dir\n","        self.object_id = object_id\n","        self.num_radius_points = num_radius_points\n","\n","        # Set up paths\n","        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n","        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n","        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n","        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n","        self.radius_base_dir = os.path.join(base_dir, object_id)\n","\n","        # Load validation split\n","        split_file = os.path.join(base_dir, object_id, 'Split', 'val.txt')\n","        with open(split_file, 'r') as f:\n","            self.filenames = [line.strip() for line in f.readlines()]\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            filename = self.filenames[idx]\n","            base_name = filename.split('.')[0]\n","            # Load RGB image\n","            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","            rgb_img = cv2.imread(rgb_path)\n","            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","            rgb_img = Image.fromarray(rgb_img)\n","            rgb_img = torch.from_numpy(np.array(rgb_img)).float() / 255.0\n","            rgb_img = rgb_img.permute(2, 0, 1)\n","            # Load depth image (.dpt files need special reader)\n","            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","            if not os.path.exists(depth_path):\n","                raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n","            depth_img = read_depth_dpt(depth_path)\n","            if depth_img is None:\n","                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n","            depth_img = torch.from_numpy(depth_img).float() / 10000.0\n","            depth_img = depth_img.unsqueeze(0)\n","            # Load pose\n","            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","            pose = np.load(pose_path)\n","            if pose.shape == (3, 4):\n","                rot = R.from_matrix(pose[:, :3]).as_quat()\n","                trans = pose[:, 3]\n","                pose = np.concatenate([trans, rot])\n","            elif pose.shape == (4, 4):\n","                rot = R.from_matrix(pose[:3, :3]).as_quat()\n","                trans = pose[:3, 3]\n","                pose = np.concatenate([trans, rot])\n","            # Load radius maps\n","            radius_maps = []\n","            for pt_idx in range(1, self.num_radius_points + 1):\n","                radius_folder = f\"Out_pt{pt_idx}_dm\"\n","                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","                radius_map = np.load(radius_path)\n","                radius_maps.append(radius_map)\n","            radius_maps = np.array(radius_maps)\n","            return {\n","                'rgb': rgb_img,\n","                'depth': depth_img,\n","                'pose': torch.FloatTensor(pose),\n","                'radius_maps': torch.FloatTensor(radius_maps)\n","            }\n","        except Exception as e:\n","            # Raise a RuntimeError with sample name for skipping in validation\n","            raise RuntimeError(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n","\n","# =========================\n","# METRIC FUNCTIONS\n","# =========================\n","def translation_rmse(pred, gt):\n","    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n","\n","def rotation_error(pred, gt):\n","    pred_quat = pred[3:]\n","    gt_quat = gt[3:]\n","    # Normalize quaternions\n","    pred_quat = pred_quat / np.linalg.norm(pred_quat)\n","    gt_quat = gt_quat / np.linalg.norm(gt_quat)\n","    # Compute quaternion difference\n","    diff = 1 - np.abs(np.dot(pred_quat, gt_quat))\n","    return diff\n","\n","def add_metric(pred_pose, gt_pose, mesh_points, threshold=0.1):\n","    if mesh_points is None:\n","        return 0.0\n","\n","    # Convert quaternions to rotation matrices\n","    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n","    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n","\n","    # Transform mesh points\n","    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n","    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n","\n","    # Compute mean distance\n","    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n","    return mean_dist\n","\n","def get_latest_model(models_dir):\n","    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n","    if not model_files:\n","        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n","    return max(model_files, key=os.path.getctime)\n","\n","# =========================\n","# VALIDATION PIPELINE\n","# =========================\n","def validate():\n","    print(\"\\n================= RCVPose Validation (Colab) =================\")\n","\n","    # Find and load latest model\n","    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n","    print(f\"Using latest model: {model_path}\")\n","\n","    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n","    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    # Load validation data\n","    dataset = ValidationDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n","\n","    # Load mesh for ADD metric\n","    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n","    mesh_points = None\n","    try:\n","        if os.path.exists(mesh_path):\n","            import open3d as o3d\n","            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n","    except ImportError:\n","        print(\"open3d is not installed. Please run: !pip install open3d\")\n","\n","    # Metrics accumulators\n","    trans_errors = []\n","    rot_errors = []\n","    points_errors = []\n","    add_errors = []\n","    add_success = 0\n","\n","    # Validation loop (skip samples with missing/corrupt files)\n","    with torch.no_grad():\n","        for i in tqdm(range(len(dataset)), desc=\"Validating\"):\n","            try:\n","                batch = dataset[i]\n","            except RuntimeError as e:\n","                print(e)\n","                continue\n","            # Prepare batch (single sample)\n","            rgb = batch['rgb'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            depth = batch['depth'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            pose_gt = batch['pose'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            radius_maps_gt = batch['radius_maps'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            # Forward pass\n","            pose_pred, radius_maps_pred = model(rgb, depth)\n","            # Move predictions to CPU for metric computation\n","            pose_pred = pose_pred.cpu().numpy()\n","            pose_gt = pose_gt.cpu().numpy()\n","            radius_maps_pred = radius_maps_pred.cpu().numpy()\n","            radius_maps_gt = radius_maps_gt.cpu().numpy()\n","            # Compute metrics\n","            for j in range(len(pose_pred)):\n","                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n","                trans_errors.append(trans_error)\n","                rot_error = rotation_error(pose_pred[j], pose_gt[j])\n","                rot_errors.append(rot_error)\n","                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n","                points_errors.append(points_error)\n","                if mesh_points is not None:\n","                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n","                    add_errors.append(add_error)\n","                    if add_error < 0.1:\n","                        add_success += 1\n","\n","    # Compute and print final metrics\n","    print(\"\\nValidation Results:\")\n","    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n","    print(f\"Rotation Error: {np.mean(rot_errors):.4f}\")\n","    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n","    if mesh_points is not None:\n","        print(f\"ADD Metric: {np.mean(add_errors):.4f}\")\n","        print(f\"ADD Success Rate: {add_success/len(trans_errors)*100:.2f}%\")\n","\n","    return {\n","        'trans_rmse': np.mean(trans_errors),\n","        'rot_error': np.mean(rot_errors),\n","        'points_mse': np.mean(points_errors),\n","        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n","        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n","    }\n","\n","if __name__ == \"__main__\":\n","    validate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QoKBaCBCHLHI","executionInfo":{"status":"ok","timestamp":1748444332022,"user_tz":-120,"elapsed":22763,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"38b6d5e8-c712-4006-e2f5-62e775ac178c"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================= RCVPose Validation (Colab) =================\n","Using latest model: /content/models/best_model_20250528_145606.pth\n"]},{"output_type":"stream","name":"stderr","text":["Validating: 100%|██████████| 247/247 [00:20<00:00, 11.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Validation Results:\n","Translation RMSE: 0.0743 mm\n","Rotation Error: 0.3829\n","Points MSE: 0.0020\n","ADD Metric: 38.1050\n","ADD Success Rate: 0.00%\n"]}]},{"cell_type":"markdown","source":["# **Test**"],"metadata":{"id":"q1BtA8hCbrYK"}},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from scipy.spatial.transform import Rotation as R\n","import cv2\n","from tqdm import tqdm\n","import torchvision.models as models\n","\n","# =========================\n","# CONFIGURATION SECTION\n","# =========================\n","CONFIG = {\n","    'BASE_DIR': '/content/dataset/linemod/Linemod_preprocessed/data',  # Root directory containing all object folders\n","    'OBJECT_ID': '01',  # Object ID to test (e.g., '01')\n","    'BATCH_SIZE': 1,  # For test, use batch size 1 for accurate metrics\n","    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n","    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'MODELS_DIR': '/content/models',  # Directory containing saved models\n","    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n","}\n","\n","# =========================\n","# DEPTH READER FOR .dpt FILES\n","# =========================\n","def read_depth_dpt(path):\n","    with open(path, \"rb\") as f:\n","        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n","        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n","    return data.reshape(h, w).astype(np.float32)\n","\n","# =========================\n","# MODEL ARCHITECTURE\n","# =========================\n","class AttentionModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionModule, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n","        self.value = nn.Conv2d(in_channels, in_channels, 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","    def forward(self, x):\n","        batch_size, C, H, W = x.size()\n","        query = self.query(x).view(batch_size, -1, H * W)\n","        key = self.key(x).view(batch_size, -1, H * W)\n","        value = self.value(x).view(batch_size, -1, H * W)\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, C, H, W)\n","        return self.gamma * out + x\n","\n","class FeaturePyramidNetwork(nn.Module):\n","    def __init__(self, in_channels_list):\n","        super(FeaturePyramidNetwork, self).__init__()\n","        self.lateral_convs = nn.ModuleList([\n","            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n","        ])\n","        self.fpn_convs = nn.ModuleList([\n","            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n","        ])\n","    def forward(self, features):\n","        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n","        for i in range(len(laterals)-1, 0, -1):\n","            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n","        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n","\n","class EnhancedRCVPose(nn.Module):\n","    def __init__(self):\n","        super(EnhancedRCVPose, self).__init__()\n","        resnet = models.resnet50(pretrained=True)\n","        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])\n","        self.rgb_layer2 = list(resnet.children())[5]\n","        self.rgb_layer3 = list(resnet.children())[6]\n","        self.rgb_layer4 = list(resnet.children())[7]\n","        resnet_depth = models.resnet50(pretrained=True)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n","        self.depth_layer2 = list(resnet_depth.children())[5]\n","        self.depth_layer3 = list(resnet_depth.children())[6]\n","        self.depth_layer4 = list(resnet_depth.children())[7]\n","        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n","        self.rgb_attention = AttentionModule(256)\n","        self.depth_attention = AttentionModule(256)\n","        self.fusion = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.pose_head = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(128, 7)\n","        )\n","        self.outside9_head = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 9, kernel_size=1)\n","        )\n","    def forward(self, rgb, depth):\n","        x1 = self.rgb_layer1(rgb)\n","        x2 = self.rgb_layer2(x1)\n","        x3 = self.rgb_layer3(x2)\n","        x4 = self.rgb_layer4(x3)\n","        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n","        d1 = self.depth_layer1(depth)\n","        d2 = self.depth_layer2(d1)\n","        d3 = self.depth_layer3(d2)\n","        d4 = self.depth_layer4(d3)\n","        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n","        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n","        depth_attended = self.depth_attention(depth_fpn_features[0])\n","        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n","        fused = self.fusion(combined)\n","        pooled = self.global_pool(fused)\n","        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n","        outside9 = self.outside9_head(fused)\n","        target_size = (rgb.shape[2], rgb.shape[3])\n","        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n","        return pose, outside9\n","\n","# =========================\n","# DATASET CLASS\n","# =========================\n","class TestDataset(Dataset):\n","    def __init__(self, base_dir, object_id, num_radius_points=9):\n","        self.base_dir = base_dir\n","        self.object_id = object_id\n","        self.num_radius_points = num_radius_points\n","        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n","        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n","        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n","        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n","        self.radius_base_dir = os.path.join(base_dir, object_id)\n","        split_file = os.path.join(base_dir, object_id, 'Split', 'test.txt')\n","        with open(split_file, 'r') as f:\n","            self.filenames = [line.strip() for line in f.readlines()]\n","    def __len__(self):\n","        return len(self.filenames)\n","    def __getitem__(self, idx):\n","        try:\n","            filename = self.filenames[idx]\n","            base_name = filename.split('.')[0]\n","            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n","            rgb_img = cv2.imread(rgb_path)\n","            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n","            rgb_img = Image.fromarray(rgb_img)\n","            rgb_img = torch.from_numpy(np.array(rgb_img)).float() / 255.0\n","            rgb_img = rgb_img.permute(2, 0, 1)\n","            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n","            if not os.path.exists(depth_path):\n","                raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n","            depth_img = read_depth_dpt(depth_path)\n","            if depth_img is None:\n","                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n","            depth_img = torch.from_numpy(depth_img).float() / 10000.0\n","            depth_img = depth_img.unsqueeze(0)\n","            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n","            pose = np.load(pose_path)\n","            if pose.shape == (3, 4):\n","                rot = R.from_matrix(pose[:, :3]).as_quat()\n","                trans = pose[:, 3]\n","                pose = np.concatenate([trans, rot])\n","            elif pose.shape == (4, 4):\n","                rot = R.from_matrix(pose[:3, :3]).as_quat()\n","                trans = pose[:3, 3]\n","                pose = np.concatenate([trans, rot])\n","            radius_maps = []\n","            for pt_idx in range(1, self.num_radius_points + 1):\n","                radius_folder = f\"Out_pt{pt_idx}_dm\"\n","                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n","                radius_map = np.load(radius_path)\n","                radius_maps.append(radius_map)\n","            radius_maps = np.array(radius_maps)\n","            return {\n","                'rgb': rgb_img,\n","                'depth': depth_img,\n","                'pose': torch.FloatTensor(pose),\n","                'radius_maps': torch.FloatTensor(radius_maps)\n","            }\n","        except Exception as e:\n","            raise RuntimeError(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n","\n","# =========================\n","# METRIC FUNCTIONS\n","# =========================\n","def translation_rmse(pred, gt):\n","    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n","\n","def rotation_error(pred, gt):\n","    pred_quat = pred[3:]\n","    gt_quat = gt[3:]\n","    pred_quat = pred_quat / np.linalg.norm(pred_quat)\n","    gt_quat = gt_quat / np.linalg.norm(gt_quat)\n","    diff = 1 - np.abs(np.dot(pred_quat, gt_quat))\n","    return diff\n","\n","def add_metric(pred_pose, gt_pose, mesh_points, threshold=0.1):\n","    if mesh_points is None:\n","        return 0.0\n","    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n","    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n","    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n","    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n","    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n","    return mean_dist\n","\n","def get_latest_model(models_dir):\n","    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n","    if not model_files:\n","        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n","    return max(model_files, key=os.path.getctime)\n","\n","# =========================\n","# TEST PIPELINE\n","# =========================\n","def test():\n","    print(\"\\n================= RCVPose Test (Colab) =================\")\n","    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n","    print(f\"Using latest model: {model_path}\")\n","    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n","    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","    dataset = TestDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n","    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n","    mesh_points = None\n","    try:\n","        if os.path.exists(mesh_path):\n","            import open3d as o3d\n","            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n","    except ImportError:\n","        print(\"open3d is not installed. Please run: !pip install open3d\")\n","    trans_errors = []\n","    rot_errors = []\n","    points_errors = []\n","    add_errors = []\n","    add_success = 0\n","    with torch.no_grad():\n","        for i in tqdm(range(len(dataset)), desc=\"Testing\"):\n","            try:\n","                batch = dataset[i]\n","            except RuntimeError as e:\n","                print(e)\n","                continue\n","            rgb = batch['rgb'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            depth = batch['depth'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            pose_gt = batch['pose'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            radius_maps_gt = batch['radius_maps'].unsqueeze(0).to(CONFIG['DEVICE'])\n","            pose_pred, radius_maps_pred = model(rgb, depth)\n","            pose_pred = pose_pred.cpu().numpy()\n","            pose_gt = pose_gt.cpu().numpy()\n","            radius_maps_pred = radius_maps_pred.cpu().numpy()\n","            radius_maps_gt = radius_maps_gt.cpu().numpy()\n","            for j in range(len(pose_pred)):\n","                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n","                trans_errors.append(trans_error)\n","                rot_error = rotation_error(pose_pred[j], pose_gt[j])\n","                rot_errors.append(rot_error)\n","                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n","                points_errors.append(points_error)\n","                if mesh_points is not None:\n","                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n","                    add_errors.append(add_error)\n","                    if add_error < 0.1:\n","                        add_success += 1\n","    print(\"\\nTest Results:\")\n","    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n","    print(f\"Rotation Error: {np.mean(rot_errors):.4f}\")\n","    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n","    if mesh_points is not None:\n","        print(f\"ADD Metric: {np.mean(add_errors):.4f}\")\n","        print(f\"ADD Success Rate: {add_success/len(trans_errors)*100:.2f}%\")\n","    return {\n","        'trans_rmse': np.mean(trans_errors),\n","        'rot_error': np.mean(rot_errors),\n","        'points_mse': np.mean(points_errors),\n","        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n","        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n","    }\n","\n","if __name__ == \"__main__\":\n","    test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4Mhrqi_bu5q","executionInfo":{"status":"ok","timestamp":1748444853519,"user_tz":-120,"elapsed":12172,"user":{"displayName":"Sina Ghiabi","userId":"15330816834987319397"}},"outputId":"efece613-1042-450b-c9ec-2db6e57e9f96"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================= RCVPose Test (Colab) =================\n","Using latest model: /content/models/best_model_20250528_145606.pth\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 124/124 [00:10<00:00, 11.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Results:\n","Translation RMSE: 0.0797 mm\n","Rotation Error: 0.3884\n","Points MSE: 0.0020\n","ADD Metric: 38.5360\n","ADD Success Rate: 0.00%\n"]}]},{"cell_type":"markdown","metadata":{"id":"8yar8tY6XPh1"},"source":["# **All data are in millimeter scale.**"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1oEZopAmzNXMtUQLbC7-Zf3IGgr_lDSYp","timestamp":1748445235052},{"file_id":"1C6CXJWlRC5Mqfws-zB9kqh4-OeMSd4YM","timestamp":1747998293619},{"file_id":"1LEXC3BfIpeRC4cTCPIhH1QZ05N01bh1k","timestamp":1747955566766},{"file_id":"1NxmXsK5njA3LFwUTTPIei7t7qWxkHZim","timestamp":1747877518212},{"file_id":"1Ed8hajlaGmwSAwDFUftKdDTn5ReDDJjw","timestamp":1747775767352}],"authorship_tag":"ABX9TyMXx7H9RrBT9CJh0Rd33lRa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}